{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"AWS Deep Learning Containers <p>One stop shop for running AI/ML on AWS</p> <p>AWS Doc \u00b7 Available Images \u00b7 Tutorials</p>"},{"location":"#whats-new","title":"\ud83d\udd25 What's New","text":""},{"location":"#release-highlights","title":"\ud83d\ude80 Release Highlights","text":"<ul> <li>[2025/12/19] Released v0.13.0 vLLM DLC</li> <li>EC2/EKS/ECS: <code>public.ecr.aws/deep-learning-containers/vllm:0.13-gpu-py312-ec2</code></li> <li>SageMaker: <code>public.ecr.aws/deep-learning-containers/vllm:0.13.0-gpu-py312</code></li> <li>[2025/11/17] Released first SGLang DLC</li> <li>SageMaker: <code>public.ecr.aws/deep-learning-containers/sglang:0.5.5-gpu-py312</code></li> </ul>"},{"location":"#hot-off-the-press","title":"\ud83c\udf89 Hot Off the Press","text":"<ul> <li>\ud83c\udf10   Master Distributed Training on Amazon EKS   - Set up and validate a distributed training environment on Amazon EKS for scalable ML model training across multiple nodes.</li> <li>\ud83d\udd04   Level Up with Amazon SageMaker AI &amp; MLflow   - Integrate AWSDLC with Amazon SageMaker AI's managed MLflow service for streamlined experiment tracking and model management.</li> <li>\ud83d\ude80   Deploy LLMs Like a Pro on Amazon EKS   - Deploy and serve Large Language Models efficiently on Amazon EKS using vLLM Deep Learning Containers.</li> <li>\ud83c\udfaf   Web Automation with Meta Llama 3.2 Vision   - Fine-tune and deploy Meta's Llama 3.2 Vision model for AI-powered web automation.</li> <li>\u26a1   Supercharge Your DL Environment   - Integrate AWSDLC with Amazon Q Developer and Model Context Protocol (MCP).</li> </ul>"},{"location":"#hands-on-workshop","title":"\ud83c\udf93 Hands-on Workshop","text":"<ul> <li>\ud83d\ude80 LLM Deployment on Amazon EKS Workshop -   Deploy and optimize LLMs on Amazon EKS using vLLM Deep Learning Containers. For more information, see   Sample Code</li> </ul>"},{"location":"#about","title":"About","text":"<p>AWS Deep Learning Containers (DLC) are a suite of Docker images that streamline the deployment of AI/ML workloads on Amazon SageMaker AI, Amazon EKS, and Amazon EC2.</p>"},{"location":"#what-we-offer","title":"\ud83c\udfaf What We Offer","text":"<ul> <li>Pre-optimized Environments - Production-ready containers with optimized deep learning frameworks</li> <li>Latest AI/ML Tools - Quick access to cutting-edge frameworks like vLLM, SGLang, and PyTorch</li> <li>Multi-Platform Support - Run seamlessly on Amazon SageMaker AI, Amazon EKS, or Amazon EC2</li> <li>Enterprise-Ready - Built with security, performance, and scalability in mind</li> </ul>"},{"location":"#key-benefits","title":"\ud83d\udcaa Key Benefits","text":"<ul> <li>Rapid Deployment - Get started in minutes with pre-configured environments</li> <li>Framework Flexibility - Support for popular frameworks like PyTorch, TensorFlow, and more</li> <li>Performance Optimized - Containers tuned for AWS infrastructure</li> <li>Regular Updates - Quick access to latest framework releases and security patches</li> <li>AWS Integration - Seamless compatibility with AWS AI/ML services</li> </ul>"},{"location":"#perfect-for","title":"\ud83c\udfae Perfect For","text":"<ul> <li>Data Scientists building and training models</li> <li>ML Engineers deploying production workloads</li> <li>DevOps teams managing ML infrastructure</li> <li>Researchers exploring cutting-edge AI capabilities</li> </ul>"},{"location":"#security-compliance","title":"\ud83d\udd12 Security &amp; Compliance","text":"<p>Our containers undergo rigorous security scanning and are regularly updated to address vulnerabilities, ensuring your ML workloads run on a secure foundation.</p> <p>For more information on our security policy, see Security.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Getting Started - Get up and running in minutes</li> <li>Tutorials - Step-by-step guides</li> <li>Available Images - Browse all container images</li> <li>Support Policy - Framework versions and timelines</li> <li>Security - Security policy</li> </ul>"},{"location":"#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Issues - Report bugs or request features</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the Apache-2.0 License.</p>"},{"location":"DEVELOPMENT/","title":"Documentation Development Guide","text":"<p>Guide for developers working with AWS Deep Learning Containers documentation.</p>"},{"location":"DEVELOPMENT/#quick-start","title":"Quick Start","text":"<pre><code>cd /path/to/deep-learning-containers\nsource .venv/bin/activate\ncd docs/src &amp;&amp; python main.py --verbose\n</code></pre>"},{"location":"DEVELOPMENT/#directory-structure","title":"Directory Structure","text":"<pre><code>docs/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 data/                                           # Per-image configuration files\n\u2502   \u2502   \u251c\u2500\u2500 template/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 image-template.yml                      # Template with all fields documented\n\u2502   \u2502   \u251c\u2500\u2500 pytorch-training/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 &lt;version&gt;-&lt;accelerator&gt;-&lt;platform&gt;.yml  # Naming is for organization only\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 legacy/                                         # Historical support data\n\u2502   \u2502   \u2514\u2500\u2500 legacy_support.yml\n\u2502   \u251c\u2500\u2500 tables/                                         # Table column configurations\n\u2502   \u2502   \u2514\u2500\u2500 &lt;repository&gt;.yml\n\u2502   \u251c\u2500\u2500 templates/\n\u2502   \u2502   \u251c\u2500\u2500 reference/                                  # Reference page templates\n\u2502   \u2502   \u2514\u2500\u2500 releasenotes/                               # Release notes templates\n\u2502   \u251c\u2500\u2500 constants.py                                    # Path constants and GLOBAL_CONFIG\n\u2502   \u251c\u2500\u2500 generate.py                                     # Generation logic\n\u2502   \u251c\u2500\u2500 global.yml                                      # Shared terminology and configuration\n\u2502   \u251c\u2500\u2500 hooks.py                                        # MkDocs hooks\n\u2502   \u251c\u2500\u2500 image_config.py                                 # ImageConfig class\n\u2502   \u251c\u2500\u2500 macros.py                                       # MkDocs macros plugin\n\u2502   \u251c\u2500\u2500 main.py                                         # CLI entry point\n\u2502   \u251c\u2500\u2500 sorter.py                                       # Sorting tiebreaker functions\n\u2502   \u2514\u2500\u2500 utils.py                                        # Utility functions\n\u251c\u2500\u2500 reference/                                          # Generated reference pages\n\u251c\u2500\u2500 releasenotes/                                       # Generated release notes\n\u2514\u2500\u2500 mkdocs.yml\n</code></pre>"},{"location":"DEVELOPMENT/#adding-a-new-image","title":"Adding a New Image","text":""},{"location":"DEVELOPMENT/#step-1-create-image-config","title":"Step 1: Create Image Config","text":"<p>Create <code>docs/src/data/&lt;repository&gt;/&lt;version&gt;-&lt;accelerator&gt;-&lt;platform&gt;.yml</code>:</p> <pre><code># Required fields\nframework: PyTorch\nversion: \"2.9\"\naccelerator: gpu              # gpu, cpu, or neuronx\nplatform: ec2                 # ec2 or sagemaker\ntags:\n  - \"2.9.0-gpu-py312-cu130-ubuntu22.04-ec2\"\n\n# Optional metadata\npython: py312\ncuda: cu130\nos: ubuntu22.04\npublic_registry: true\n</code></pre> <p>The YAML file name is for organizational purposes only. However, make sure that the image configuration file lives in the correct repository directory.</p> <p>See <code>docs/src/data/template/image-template.yml</code> for all available fields.</p>"},{"location":"DEVELOPMENT/#step-2-regenerate","title":"Step 2: Regenerate","text":"<pre><code>cd docs/src &amp;&amp; python main.py --verbose\n</code></pre>"},{"location":"DEVELOPMENT/#adding-support-policy-dates","title":"Adding Support Policy Dates","text":"<p>Add <code>ga</code> and <code>eop</code> fields to image configs for repositories that appear in support policy:</p> <pre><code>ga: \"2025-10-15\"    # General Availability date\neop: \"2026-10-15\"   # End of Patch date\n</code></pre> <p>Version Consolidation:</p> <ul> <li>Images with the same major.minor version (e.g., <code>2.6.0</code> and <code>2.6.1</code>) are consolidated into a single row displayed as <code>2.6</code> if they have identical GA/EOP dates</li> <li>If patch versions have different GA/EOP dates, each is displayed separately with full version (e.g., <code>2.6.0</code>, <code>2.6.1</code>) and a warning is logged</li> </ul> <p>Validation: All images in the same framework group with the same full version (X.Y.Z) must have identical GA/EOP dates.</p>"},{"location":"DEVELOPMENT/#adding-release-notes","title":"Adding Release Notes","text":"<p>Add these fields to an image config:</p> <pre><code>announcements:\n  - \"Introduced containers for PyTorch 2.9\"\n  - \"Added Python 3.12 support\"\n\npackages:\n  python: \"3.12\"\n  pytorch: \"2.9.0\"\n  cuda: \"13.0\"\n\n# Optional sections (rendered dynamically)\noptional:\n  known_issues:\n    - \"Description of known issue\"\n</code></pre> <p>Release notes are generated automatically for images with <code>announcements</code> and <code>packages</code> fields.</p>"},{"location":"DEVELOPMENT/#adding-new-optional-sections","title":"Adding New Optional Sections","text":"<ol> <li>Add section to image config under <code>optional</code>:</li> </ol> <pre><code>optional:\n  known_issues:\n    - \"Issue 1\"\n  deprecation_notice:\n    - \"This image will be deprecated...\"\n</code></pre> <ol> <li>Add display name to <code>global.yml</code>:</li> </ol> <pre><code>display_names:\n  deprecation_notice: \"Deprecation Notice\"\n</code></pre> <p>Sections render in YAML order as bullet lists.</p> <p>Section headers in optional sections are rendered via the section key. To format your optional section headers, add a new field in <code>docs/src/global.yml</code> under <code>display_names</code> section. Eg: deprecation_notice section will render its header as <code>## deprecation_notice</code> unless a formatted string is provided in <code>docs/src/global.yml</code>.</p>"},{"location":"DEVELOPMENT/#adding-a-new-repository","title":"Adding a New Repository","text":"<ol> <li> <p>Create directory: <code>docs/src/data/&lt;repository&gt;/</code></p> </li> <li> <p>Create table config <code>docs/src/tables/&lt;repository&gt;.yml</code>:</p> </li> </ol> <pre><code>columns:\n  - field: framework_version\n    header: \"Framework\"\n  - field: python\n    header: \"Python\"\n  - field: example_url\n    header: \"Example URL\"\n</code></pre> <ol> <li>Add to <code>docs/src/global.yml</code>:</li> </ol> <pre><code>display_names:\n  my-repo: \"My Repository\"\n\ntable_order:\n  - my-repo\n</code></pre>"},{"location":"DEVELOPMENT/#editing-table-columns","title":"Editing Table Columns","text":"<p>Edit <code>docs/src/tables/&lt;repository&gt;.yml</code>:</p> <pre><code>columns:\n  - field: framework_version\n    header: \"Framework\"\n  - field: python\n    header: \"Python\"\n  # Add/remove/reorder columns here\n</code></pre> <p>Available fields: <code>framework_version</code>, <code>python</code>, <code>cuda</code>, <code>sdk</code>, <code>accelerator</code>, <code>platform</code>, <code>os</code>, <code>example_url</code>, <code>version</code>, <code>ga</code>, <code>eop</code>, <code>framework_group</code>, <code>repository</code>, <code>release_note_link</code> To add additional fields, ensure that the image configuration YAML file contains said field of the same name. Additionally, if you require the field to be formatted, add an additional attribute in <code>ImageConfig</code> class of <code>display_&lt;field_name&gt;</code> to grab the formatted field.</p>"},{"location":"DEVELOPMENT/#legacy-support-data","title":"Legacy Support Data","text":"<p>Historical data for unsupported images in <code>docs/src/legacy/legacy_support.yml</code>:</p> <pre><code>pytorch:\n  - version: \"2.5\"\n    ga: \"2024-10-29\"\n    eop: \"2025-10-29\"\n</code></pre> <p>Generally, this is only required if an image configuration file does not already exist and the image is already past its support.</p>"},{"location":"DEVELOPMENT/#global-configuration","title":"Global Configuration","text":"<p><code>docs/src/global.yml</code> contains:</p> <ul> <li>Terminology: <code>aws</code>, <code>dlc_long</code>, <code>sagemaker</code>, etc.</li> <li>display_names: Repository and package display names</li> <li>framework_groups: Support policy consolidation groups</li> <li>table_order: Order of tables displayed within the documentations website (eg: available_images.md and support_policy.md)</li> <li>platforms/accelerators: Display mappings</li> </ul>"},{"location":"DEVELOPMENT/#running-generation","title":"Running Generation","text":"<pre><code># Help\npython main.py --help\n\n# Full generation\npython main.py --verbose\n\n# Specific outputs\npython main.py --available-images-only\npython main.py --support-policy-only\npython main.py --release-notes-only\n\n# Preview site\ncd docs &amp;&amp; mkdocs serve\n</code></pre>"},{"location":"DEVELOPMENT/#local-documentation-development","title":"Local Documentation Development","text":""},{"location":"DEVELOPMENT/#generation-only-no-server","title":"Generation Only (No Server)","text":"<p>Run <code>main.py</code> to generate documentation without serving:</p> <pre><code>cd docs/src &amp;&amp; python main.py --verbose\n</code></pre> <p>This automatically clones <code>tutorials/</code> repository and generates markdown files in <code>reference/</code> and <code>releasenotes/</code> directories without starting a web server.</p>"},{"location":"DEVELOPMENT/#serving-locally","title":"Serving Locally","text":"<p>Use <code>mkdocs serve</code> to automatically clone <code>tutorials/</code> and generate documentation in <code>reference/</code> and <code>releasenotes/</code> and serve the website:</p> <pre><code>cd docs &amp;&amp; mkdocs serve\n</code></pre> <p>The site is typically available at <code>http://127.0.0.1:8000/deep-learning-containers/</code> - check the command output for the actual URL.</p>"},{"location":"DEVELOPMENT/#live-reload","title":"Live Reload","text":"<p>Enable automatic reload on content changes:</p> <pre><code>mkdocs serve --livereload\n</code></pre> <p>Note: Live reload only detects changes to:</p> <ul> <li>Markdown file content</li> <li><code>.nav.yml</code> content</li> <li><code>mkdocs.yml</code> content</li> </ul> <p>Live reload does not detect changes requiring documentation regeneration (e.g., image config YAML files, templates). To regenerate documentation, stop the server (<code>Ctrl+C</code>) and rerun <code>mkdocs serve</code>.</p>"},{"location":"DEVELOPMENT/#troubleshooting","title":"Troubleshooting","text":"Error Solution \"Display name not found\" Add repository to <code>display_names</code> in <code>global.yml</code> \"Inconsistent dates\" Ensure all images in same framework group/version have identical GA/EOP Images not appearing Check repository is in <code>table_order</code> Release notes not generating Ensure <code>announcements</code> and <code>packages</code> fields are present"},{"location":"get_started/","title":"Getting Started","text":"<p>Get up and running with AWS Deep Learning Containers quickly.</p>"},{"location":"get_started/#prerequisites","title":"Prerequisites","text":"<ul> <li>An AWS account with appropriate permissions</li> <li>Docker installed on your local machine</li> <li>AWS CLI configured with your credentials</li> </ul>"},{"location":"get_started/#pulling-images","title":"Pulling Images","text":"<p>Learn how to authenticate and pull AWS Deep Learning Containers images.</p>"},{"location":"get_started/#authentication","title":"Authentication","text":"<pre><code>aws ecr get-login-password --region &lt;region&gt; | docker login --username AWS --password-stdin &lt;account_id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com\n</code></pre> <p>Then pull images:</p> <pre><code>docker pull &lt;account_id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/&lt;repository&gt;:&lt;tag&gt;\n</code></pre>"},{"location":"get_started/#image-url-format","title":"Image URL Format","text":"<p>To form your container image URL, use the following format:</p> <pre><code>&lt;account_id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/&lt;repository&gt;:&lt;tag&gt;\n</code></pre> <p>Where:</p> <ul> <li><code>&lt;account_id&gt;</code>: Find the account ID for your region in the Region Availability table</li> <li><code>&lt;region&gt;</code>: Your AWS region (e.g., <code>us-east-1</code>, <code>us-west-2</code>, <code>eu-west-1</code>)</li> <li><code>&lt;repository&gt;</code>: The framework repository name (e.g., <code>pytorch-training</code>, <code>tensorflow-inference</code>)</li> <li><code>&lt;tag&gt;</code>: The image tag from the Available Images tables</li> </ul>"},{"location":"get_started/#example","title":"Example","text":"<pre><code># Authenticate\naws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-west-2.amazonaws.com\n\n# Pull PyTorch training image\ndocker pull 763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.9.0-cpu-py312-ubuntu22.04-ec2\n</code></pre>"},{"location":"get_started/#next-steps","title":"Next Steps","text":"<p>After completing the getting started guides, explore our tutorials for more advanced use cases.</p>"},{"location":"get_started/using_dlcs/","title":"Using Deep Learning Containers","text":"<p>The following sections describe how to use Deep Learning Containers to run sample code from each of the frameworks on AWS infrastructure.</p>"},{"location":"get_started/using_dlcs/#use-cases","title":"Use Cases","text":"<ul> <li> <p>For information on using Deep Learning Containers with Amazon SageMaker AI, see the   Use Your Own Algorithms or Models with Amazon SageMaker AI Documentation.</p> </li> <li> <p>To learn about using Deep Learning Containers with Amazon SageMaker AI HyperPod on Amazon EKS, see   Orchestrating SageMaker HyperPod clusters with Amazon EKS and Amazon SageMaker AI.</p> </li> </ul>"},{"location":"get_started/using_dlcs/#running-on-amazon-sagemaker-ai","title":"Running on Amazon SageMaker AI","text":""},{"location":"get_started/using_dlcs/#using-sagemaker-python-sdk","title":"Using SageMaker Python SDK","text":""},{"location":"get_started/using_dlcs/#deploy-an-sglang-inference-endpoint","title":"Deploy an SGLang inference endpoint:","text":"<pre><code>from sagemaker.model import Model\n\nmodel = Model(\n    image_uri=\"763104351884.dkr.ecr.us-west-2.amazonaws.com/sglang:0.5.8-gpu-py312-cu129-ubuntu24.04-sagemaker\",\n    role=\"arn:aws:iam::&lt;account_id&gt;:role/&lt;role_name&gt;\",\n    env={\n        \"SM_SGLANG_MODEL_PATH\": \"meta-llama/Llama-3.1-8B-Instruct\",\n        \"HF_TOKEN\": \"&lt;your_hf_token&gt;\",\n    },\n)\n\npredictor = model.deploy(\n    instance_type=\"ml.g5.2xlarge\",\n    initial_instance_count=1,\n)\n</code></pre>"},{"location":"get_started/using_dlcs/#deploy-a-vllm-inference-endpoint","title":"Deploy a vLLM inference endpoint:","text":"<pre><code>from sagemaker.model import Model\n\nmodel = Model(\n    image_uri=\"763104351884.dkr.ecr.us-west-2.amazonaws.com/vllm:0.14.0-gpu-py312-cu129-ubuntu22.04-sagemaker\",\n    role=\"arn:aws:iam::&lt;account_id&gt;:role/&lt;role_name&gt;\",\n    env={\n        \"SM_VLLM_MODEL\": \"meta-llama/Llama-3.1-8B-Instruct\",\n        \"HF_TOKEN\": \"&lt;your_hf_token&gt;\",\n    },\n)\n\npredictor = model.deploy(\n    instance_type=\"ml.g5.2xlarge\",\n    initial_instance_count=1,\n)\n</code></pre>"},{"location":"get_started/using_dlcs/#using-boto3","title":"Using Boto3","text":""},{"location":"get_started/using_dlcs/#deploy-an-sglang-inference-endpoint_1","title":"Deploy an SGLang inference endpoint:","text":"<pre><code>import boto3\n\nsagemaker = boto3.client(\"sagemaker\")\n\nsagemaker.create_model(\n    ModelName=\"sglang-model\",\n    PrimaryContainer={\n        \"Image\": \"763104351884.dkr.ecr.us-west-2.amazonaws.com/sglang:0.5.8-gpu-py312-cu129-ubuntu24.04-sagemaker\",\n        \"Environment\": {\n            \"SM_SGLANG_MODEL_PATH\": \"meta-llama/Llama-3.1-8B-Instruct\",\n            \"HF_TOKEN\": \"&lt;your_hf_token&gt;\",\n        },\n    },\n    ExecutionRoleArn=\"arn:aws:iam::&lt;account_id&gt;:role/&lt;role_name&gt;\",\n)\n\nsagemaker.create_endpoint_config(\n    EndpointConfigName=\"sglang-endpoint-config\",\n    ProductionVariants=[\n        {\n            \"VariantName\": \"default\",\n            \"ModelName\": \"sglang-model\",\n            \"InstanceType\": \"ml.g5.2xlarge\",\n            \"InitialInstanceCount\": 1,\n            \"InferenceAmiVersion\": \"al2-ami-sagemaker-inference-gpu-3-1\",\n        }\n    ],\n)\n\nsagemaker.create_endpoint(\n    EndpointName=\"sglang-endpoint\",\n    EndpointConfigName=\"sglang-endpoint-config\",\n)\n</code></pre>"},{"location":"get_started/using_dlcs/#deploy-a-vllm-inference-endpoint_1","title":"Deploy a vLLM inference endpoint:","text":"<pre><code>import boto3\n\nsagemaker = boto3.client(\"sagemaker\")\n\nsagemaker.create_model(\n    ModelName=\"vllm-model\",\n    PrimaryContainer={\n        \"Image\": \"763104351884.dkr.ecr.us-west-2.amazonaws.com/vllm:0.14.0-gpu-py312-cu129-ubuntu22.04-sagemaker\",\n        \"Environment\": {\n            \"SM_VLLM_MODEL\": \"meta-llama/Llama-3.1-8B-Instruct\",\n            \"HF_TOKEN\": \"&lt;your_hf_token&gt;\",\n        },\n    },\n    ExecutionRoleArn=\"arn:aws:iam::&lt;account_id&gt;:role/&lt;role_name&gt;\",\n)\n\nsagemaker.create_endpoint_config(\n    EndpointConfigName=\"vllm-endpoint-config\",\n    ProductionVariants=[\n        {\n            \"VariantName\": \"default\",\n            \"ModelName\": \"vllm-model\",\n            \"InstanceType\": \"ml.g5.2xlarge\",\n            \"InitialInstanceCount\": 1,\n            \"InferenceAmiVersion\": \"al2-ami-sagemaker-inference-gpu-3-1\",\n        }\n    ],\n)\n\nsagemaker.create_endpoint(\n    EndpointName=\"vllm-endpoint\",\n    EndpointConfigName=\"vllm-endpoint-config\",\n)\n</code></pre>"},{"location":"get_started/using_dlcs/#running-on-amazon-ec2","title":"Running on Amazon EC2","text":""},{"location":"get_started/using_dlcs/#running-pytorch-training-container-on-an-ec2-instance","title":"Running PyTorch Training Container on an EC2 Instance","text":"<pre><code># Run interactively\ndocker run -it --gpus all &lt;account_id&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/&lt;repository&gt;:&lt;tag&gt; bash\n\n# Example: Run PyTorch container\ndocker run -it --gpus all 763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.9.0-cpu-py312-ubuntu22.04-ec2 bash\n\n# Mount local directories to persist data\ndocker run -it --gpus all -v /local/data:/data 763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.9.0-cpu-py312-ubuntu22.04-ec2 bash\n</code></pre>"},{"location":"get_started/using_dlcs/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images - Browse all container images</li> <li>Support Policy - Framework versions and timelines</li> </ul>"},{"location":"reference/","title":"Reference","text":"<p>Reference documentation for AWS Deep Learning Containers.</p> <ul> <li>Available Images - List of all available container images</li> <li>Support Policy - Image support and end-of-patch dates</li> </ul>"},{"location":"reference/available_images/","title":"Available Images","text":"<p>This page is for referencing our supported Deep Learning Containers.</p> <p>Refer to the tables below for all images that are available in Amazon ECR repositories.</p>"},{"location":"reference/available_images/#region-availability","title":"Region Availability","text":"Region Code General Neuron Example URL US East (Ohio) us-east-2 \u2705 \u2705 <code>763104351884.dkr.ecr.us-east-2.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> US East (N. Virginia) us-east-1 \u2705 \u2705 <code>763104351884.dkr.ecr.us-east-1.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> US West (N. California) us-west-1 \u2705 \u274c <code>763104351884.dkr.ecr.us-west-1.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> US West (Oregon) us-west-2 \u2705 \u2705 <code>763104351884.dkr.ecr.us-west-2.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> Africa (Cape Town) af-south-1 \u2705 \u274c <code>626614931356.dkr.ecr.af-south-1.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> Asia Pacific (Hong Kong) ap-east-1 \u2705 \u274c <code>871362719292.dkr.ecr.ap-east-1.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> Asia Pacific (Hyderabad) ap-south-2 \u2705 \u274c <code>772153158452.dkr.ecr.ap-south-2.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> Asia Pacific (Jakarta) ap-southeast-3 \u2705 \u274c <code>907027046896.dkr.ecr.ap-southeast-3.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> Asia Pacific (Malaysia) ap-southeast-5 \u2705 \u274c <code>550225433462.dkr.ecr.ap-southeast-5.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> Asia Pacific (Melbourne) ap-southeast-4 \u2705 \u274c <code>457447274322.dkr.ecr.ap-southeast-4.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> Asia Pacific (Mumbai) ap-south-1 \u2705 \u2705 <code>763104351884.dkr.ecr.ap-south-1.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> Asia Pacific (Osaka) ap-northeast-3 \u2705 \u274c <code>364406365360.dkr.ecr.ap-northeast-3.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> Asia Pacific (Seoul) ap-northeast-2 \u2705 \u274c <code>763104351884.dkr.ecr.ap-northeast-2.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> Asia Pacific (Singapore) ap-southeast-1 \u2705 \u2705 <code>763104351884.dkr.ecr.ap-southeast-1.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> Asia Pacific (Sydney) ap-southeast-2 \u2705 \u2705 <code>763104351884.dkr.ecr.ap-southeast-2.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> Asia Pacific (Taipei) ap-east-2 \u2705 \u2705 <code>763104351884.dkr.ecr.ap-east-2.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> Asia Pacific (Thailand) ap-southeast-7 \u2705 \u274c <code>590183813437.dkr.ecr.ap-southeast-7.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> Asia Pacific (Tokyo) ap-northeast-1 \u2705 \u2705 <code>763104351884.dkr.ecr.ap-northeast-1.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> Canada (Central) ca-central-1 \u2705 \u274c <code>763104351884.dkr.ecr.ca-central-1.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> Canada (Calgary) ca-west-1 \u2705 \u274c <code>204538143572.dkr.ecr.ca-west-1.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> EU (Frankfurt) eu-central-1 \u2705 \u2705 <code>763104351884.dkr.ecr.eu-central-1.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> EU (Ireland) eu-west-1 \u2705 \u2705 <code>763104351884.dkr.ecr.eu-west-1.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> EU (London) eu-west-2 \u2705 \u274c <code>763104351884.dkr.ecr.eu-west-2.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> EU (Milan) eu-south-1 \u2705 \u274c <code>692866216735.dkr.ecr.eu-south-1.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> EU (Paris) eu-west-3 \u2705 \u2705 <code>763104351884.dkr.ecr.eu-west-3.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> EU (Spain) eu-south-2 \u2705 \u274c <code>503227376785.dkr.ecr.eu-south-2.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> EU (Stockholm) eu-north-1 \u2705 \u274c <code>763104351884.dkr.ecr.eu-north-1.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> EU (Zurich) eu-central-2 \u2705 \u274c <code>380420809688.dkr.ecr.eu-central-2.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> Israel (Tel Aviv) il-central-1 \u2705 \u274c <code>780543022126.dkr.ecr.il-central-1.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> Mexico (Central) mx-central-1 \u2705 \u274c <code>637423239942.dkr.ecr.mx-central-1.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> Middle East (Bahrain) me-south-1 \u2705 \u274c <code>217643126080.dkr.ecr.me-south-1.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> Middle East (UAE) me-central-1 \u2705 \u274c <code>914824155844.dkr.ecr.me-central-1.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> South America (Sao Paulo) sa-east-1 \u2705 \u2705 <code>763104351884.dkr.ecr.sa-east-1.amazonaws.com/&lt;repository-name&gt;:&lt;image-tag&gt;</code> China (Beijing) cn-north-1 \u2705 \u274c <code>727897471807.dkr.ecr.cn-north-1.amazonaws.com.cn/&lt;repository-name&gt;:&lt;image-tag&gt;</code> China (Ningxia) cn-northwest-1 \u2705 \u274c <code>727897471807.dkr.ecr.cn-northwest-1.amazonaws.com.cn/&lt;repository-name&gt;:&lt;image-tag&gt;</code>"},{"location":"reference/available_images/#base","title":"Base","text":"<p>These images are also available in ECR Public Gallery: base</p> Framework Python CUDA Accelerator Platform Example URL Base 13.0 py312 cu130 GPU EC2, ECS, EKS <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/base:13.0.0-gpu-py312-cu130-ubuntu22.04-ec2</code> Base 12.9 py312 cu129 GPU EC2, ECS, EKS <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/base:12.9.1-gpu-py312-cu129-ubuntu22.04-ec2</code> Base 12.8.1 py312 cu128 GPU EC2, ECS, EKS <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/base:12.8.1-gpu-py312-cu128-ubuntu24.04-ec2</code> Base 12.8.0 py312 cu128 GPU EC2, ECS, EKS <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/base:12.8.0-gpu-py312-cu128-ubuntu22.04-ec2</code>"},{"location":"reference/available_images/#sglang","title":"SGLang","text":"<p>These images are also available in ECR Public Gallery: sglang</p> Framework Python CUDA Accelerator Platform Example URL SGLang 0.5.8 py312 cu129 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/sglang:0.5.8-gpu-py312-cu129-ubuntu24.04-sagemaker</code> SGLang 0.5.7 py312 cu129 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/sglang:0.5.7-gpu-py312-cu129-ubuntu24.04-sagemaker</code> SGLang 0.5.6 py312 cu129 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/sglang:0.5.6-gpu-py312-cu129-ubuntu22.04-sagemaker</code> SGLang 0.5.5 py312 cu129 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/sglang:0.5.5-gpu-py312-cu129-ubuntu22.04-sagemaker</code>"},{"location":"reference/available_images/#vllm","title":"vLLM","text":"<p>These images are also available in ECR Public Gallery: vllm</p> Framework Python CUDA Accelerator Platform Example URL vLLM 0.14.0 py312 cu129 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/vllm:0.14.0-gpu-py312-cu129-ubuntu22.04-sagemaker</code> vLLM 0.14.0 py312 cu129 GPU EC2, ECS, EKS <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/vllm:0.14.0-gpu-py312-cu129-ubuntu22.04-ec2</code> vLLM 0.13.0 py312 cu129 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/vllm:0.13.0-gpu-py312-cu129-ubuntu22.04-sagemaker</code> vLLM 0.13.0 py312 cu129 GPU EC2, ECS, EKS <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/vllm:0.13.0-gpu-py312-cu129-ubuntu22.04-ec2</code>"},{"location":"reference/available_images/#vllm-arm64","title":"vLLM ARM64","text":"<p>These images are also available in ECR Public Gallery: vllm-arm64</p> Framework Python CUDA Accelerator Platform Example URL vLLM 0.10.2 py312 cu129 GPU EC2, ECS, EKS <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/vllm-arm64:0.10.2-gpu-py312-cu129-ubuntu22.04-ec2</code>"},{"location":"reference/available_images/#pytorch-training","title":"PyTorch Training","text":"<p>These images are also available in ECR Public Gallery: pytorch-training</p> Framework Python CUDA Accelerator Platform Example URL PyTorch 2.9 py312 cu130 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/pytorch-training:2.9.0-gpu-py312-cu130-ubuntu22.04-sagemaker</code> PyTorch 2.9 py312 - CPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/pytorch-training:2.9.0-cpu-py312-ubuntu22.04-sagemaker</code> PyTorch 2.9 py312 cu130 GPU EC2, ECS, EKS <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/pytorch-training:2.9.0-gpu-py312-cu130-ubuntu22.04-ec2</code> PyTorch 2.9 py312 - CPU EC2, ECS, EKS <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/pytorch-training:2.9.0-cpu-py312-ubuntu22.04-ec2</code> PyTorch 2.8 py312 cu129 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/pytorch-training:2.8.0-gpu-py312-cu129-ubuntu22.04-sagemaker</code> PyTorch 2.8 py312 - CPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/pytorch-training:2.8.0-cpu-py312-ubuntu22.04-sagemaker</code> PyTorch 2.8 py312 cu129 GPU EC2, ECS, EKS <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/pytorch-training:2.8.0-gpu-py312-cu129-ubuntu22.04-ec2</code> PyTorch 2.8 py312 - CPU EC2, ECS, EKS <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/pytorch-training:2.8.0-cpu-py312-ubuntu22.04-ec2</code> PyTorch 2.7 py312 cu128 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/pytorch-training:2.7.1-gpu-py312-cu128-ubuntu22.04-sagemaker</code> PyTorch 2.7 py312 - CPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/pytorch-training:2.7.1-cpu-py312-ubuntu22.04-sagemaker</code> PyTorch 2.7 py312 cu128 GPU EC2, ECS, EKS <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/pytorch-training:2.7.1-gpu-py312-cu128-ubuntu22.04-ec2</code> PyTorch 2.7 py312 - CPU EC2, ECS, EKS <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/pytorch-training:2.7.1-cpu-py312-ubuntu22.04-ec2</code>"},{"location":"reference/available_images/#pytorch-training-arm64","title":"PyTorch Training ARM64","text":"<p>These images are also available in ECR Public Gallery: pytorch-training-arm64</p> Framework Python CUDA Accelerator Platform Example URL PyTorch 2.7 py312 cu128 GPU EC2, ECS, EKS <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/pytorch-training-arm64:2.7.0-gpu-py312-cu128-ubuntu22.04-ec2</code>"},{"location":"reference/available_images/#tensorflow-training","title":"TensorFlow Training","text":"<p>These images are also available in ECR Public Gallery: tensorflow-training</p> Framework Python CUDA Accelerator Platform Example URL TensorFlow 2.19 py312 cu125 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/tensorflow-training:2.19.0-gpu-py312-cu125-ubuntu22.04-sagemaker</code> TensorFlow 2.19 py312 - CPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/tensorflow-training:2.19.0-cpu-py312-ubuntu22.04-sagemaker</code>"},{"location":"reference/available_images/#tensorflow-inference","title":"TensorFlow Inference","text":"<p>These images are also available in ECR Public Gallery: tensorflow-inference</p> Framework Python CUDA Accelerator Platform Example URL TensorFlow 2.19 py312 cu122 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/tensorflow-inference:2.19.0-gpu-py312-cu122-ubuntu22.04-sagemaker</code> TensorFlow 2.19 py312 - CPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/tensorflow-inference:2.19.0-cpu-py312-ubuntu22.04-sagemaker</code>"},{"location":"reference/available_images/#tensorflow-inference-arm64","title":"TensorFlow Inference ARM64","text":"<p>These images are also available in ECR Public Gallery: tensorflow-inference-arm64</p> Framework Python CUDA Accelerator Platform Example URL TensorFlow 2.19 py312 - CPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/tensorflow-inference-arm64:2.19.0-cpu-py312-ubuntu22.04-sagemaker</code>"},{"location":"reference/available_images/#pytorch-training-neuronx","title":"PyTorch Training NeuronX","text":"Framework Python SDK Accelerator Platform Example URL PyTorch 2.9 py312 2.27.1 NeuronX SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/pytorch-training-neuronx:2.9.0-neuronx-py312-sdk2.27.1-ubuntu24.04</code> PyTorch 2.8 py311 2.26.1 NeuronX SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/pytorch-training-neuronx:2.8.0-neuronx-py311-sdk2.26.1-ubuntu22.04</code> PyTorch 2.7 py310 2.24.1 NeuronX SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/pytorch-training-neuronx:2.7.0-neuronx-py310-sdk2.24.1-ubuntu22.04</code> PyTorch 2.7 py310 2.25.0 NeuronX SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/pytorch-training-neuronx:2.7.0-neuronx-py310-sdk2.25.0-ubuntu22.04</code> PyTorch 2.6 py310 2.23.0 NeuronX SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/pytorch-training-neuronx:2.6.0-neuronx-py310-sdk2.23.0-ubuntu22.04</code>"},{"location":"reference/available_images/#pytorch-inference-neuronx","title":"PyTorch Inference NeuronX","text":"Framework Python SDK Accelerator Platform Example URL PyTorch 2.9 py312 2.27.1 NeuronX SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/pytorch-inference-neuronx:2.9.0-neuronx-py312-sdk2.27.1-ubuntu24.04</code> PyTorch 2.8 py311 2.26.1 NeuronX SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/pytorch-inference-neuronx:2.8.0-neuronx-py311-sdk2.26.1-ubuntu22.04</code> PyTorch 2.7 py310 2.24.1 NeuronX SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/pytorch-inference-neuronx:2.7.0-neuronx-py310-sdk2.24.1-ubuntu22.04</code> PyTorch 2.7 py310 2.25.0 NeuronX SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/pytorch-inference-neuronx:2.7.0-neuronx-py310-sdk2.25.0-ubuntu22.04</code> PyTorch 2.6 py310 2.23.0 NeuronX SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/pytorch-inference-neuronx:2.6.0-neuronx-py310-sdk2.23.0-ubuntu22.04</code>"},{"location":"reference/available_images/#tensorflow-inference-neuronx","title":"TensorFlow Inference NeuronX","text":"Framework Python SDK Accelerator Platform Example URL TensorFlow 2.10 py310 2.17.0 NeuronX SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/tensorflow-inference-neuronx:2.10.1-neuronx-py310-sdk2.17.0-ubuntu20.04</code>"},{"location":"reference/available_images/#huggingface-pytorch-training","title":"HuggingFace PyTorch Training","text":"Framework Python CUDA Transformers Accelerator Platform Example URL PyTorch 2.5 py311 cu124 4.49.0 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/huggingface-pytorch-training:2.5.1-transformers4.49.0-gpu-py311-cu124-ubuntu22.04</code> PyTorch 2.1 py310 cu121 4.36.0 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/huggingface-pytorch-training:2.1.0-transformers4.36.0-gpu-py310-cu121-ubuntu20.04</code> PyTorch 2.0 py310 cu118 4.28.1 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/huggingface-pytorch-training:2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04</code> PyTorch 1.13 py39 cu117 4.26.0 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/huggingface-pytorch-training:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04</code>"},{"location":"reference/available_images/#huggingface-pytorch-inference","title":"HuggingFace PyTorch Inference","text":"Framework Python CUDA Transformers Accelerator Platform Example URL PyTorch 2.6 py312 cu124 4.49.0 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/huggingface-pytorch-inference:2.6.0-transformers4.49.0-gpu-py312-cu124-ubuntu22.04</code> PyTorch 2.6 py312 - 4.49.0 CPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/huggingface-pytorch-inference:2.6.0-transformers4.49.0-cpu-py312-ubuntu22.04</code> PyTorch 2.1 py310 cu118 4.37.0 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/huggingface-pytorch-inference:2.1.0-transformers4.37.0-gpu-py310-cu118-ubuntu20.04</code> PyTorch 2.1 py310 - 4.37.0 CPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/huggingface-pytorch-inference:2.1.0-transformers4.37.0-cpu-py310-ubuntu22.04</code>"},{"location":"reference/available_images/#huggingface-pytorch-training-neuronx","title":"HuggingFace PyTorch Training NeuronX","text":"Framework Python SDK Transformers Accelerator Platform Example URL PyTorch 2.8 py310 2.26.0 4.55.4 NeuronX SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/huggingface-pytorch-training-neuronx:2.8.0-transformers4.55.4-neuronx-py310-sdk2.26.0-ubuntu22.04</code> PyTorch 2.7 py310 2.24.1 4.51.0 NeuronX SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/huggingface-pytorch-training-neuronx:2.7.0-transformers4.51.0-neuronx-py310-sdk2.24.1-ubuntu22.04</code> PyTorch 2.1 py310 2.20.0 4.48.1 NeuronX SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/huggingface-pytorch-training-neuronx:2.1.2-transformers4.48.1-neuronx-py310-sdk2.20.0-ubuntu20.04</code>"},{"location":"reference/available_images/#huggingface-pytorch-inference-neuronx","title":"HuggingFace PyTorch Inference NeuronX","text":"Framework Python SDK Transformers Accelerator Platform Example URL PyTorch 2.8 py310 2.26.0 4.55.4 NeuronX SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/huggingface-pytorch-inference-neuronx:2.8.0-transformers4.55.4-neuronx-py310-sdk2.26.0-ubuntu22.04</code> PyTorch 2.7 py310 2.24.1 4.51.3 NeuronX SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/huggingface-pytorch-inference-neuronx:2.7.1-transformers4.51.3-neuronx-py310-sdk2.24.1-ubuntu22.04</code> PyTorch 2.1 py310 2.20.0 4.43.2 NeuronX SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/huggingface-pytorch-inference-neuronx:2.1.2-transformers4.43.2-neuronx-py310-sdk2.20.0-ubuntu20.04</code>"},{"location":"reference/available_images/#huggingface-pytorch-training-compiler","title":"HuggingFace PyTorch Training Compiler","text":"Framework Python CUDA Transformers Accelerator Platform Example URL PyTorch 1.11 py38 cu113 4.21.1 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/huggingface-pytorch-trcomp-training:1.11.0-transformers4.21.1-gpu-py38-cu113-ubuntu20.04</code>"},{"location":"reference/available_images/#huggingface-tensorflow-training","title":"HuggingFace TensorFlow Training","text":"Framework Python CUDA Transformers Accelerator Platform Example URL TensorFlow 2.6 py38 cu112 4.17.0 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/huggingface-tensorflow-training:2.6.3-transformers4.17.0-gpu-py38-cu112-ubuntu20.04</code>"},{"location":"reference/available_images/#huggingface-tensorflow-inference","title":"HuggingFace TensorFlow Inference","text":"Framework Python CUDA Transformers Accelerator Platform Example URL TensorFlow 2.11 py39 cu112 4.26.0 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/huggingface-tensorflow-inference:2.11.1-transformers4.26.0-gpu-py39-cu112-ubuntu20.04</code> TensorFlow 2.11 py39 - 4.26.0 CPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/huggingface-tensorflow-inference:2.11.1-transformers4.26.0-cpu-py39-ubuntu20.04</code>"},{"location":"reference/available_images/#autogluon-training","title":"AutoGluon Training","text":"Framework Python CUDA Accelerator Platform Example URL AutoGluon 1.4 py311 cu124 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/autogluon-training:1.4.0-gpu-py311-cu124-ubuntu22.04</code> AutoGluon 1.4 py311 - CPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/autogluon-training:1.4.0-cpu-py311-ubuntu22.04</code> AutoGluon 1.3 py311 cu124 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/autogluon-training:1.3.0-gpu-py311-cu124-ubuntu22.04</code> AutoGluon 1.3 py311 - CPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/autogluon-training:1.3.0-cpu-py311-ubuntu22.04</code>"},{"location":"reference/available_images/#autogluon-inference","title":"AutoGluon Inference","text":"Framework Python CUDA Accelerator Platform Example URL AutoGluon 1.4 py311 cu124 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/autogluon-inference:1.4.0-gpu-py311-cu124-ubuntu22.04</code> AutoGluon 1.4 py311 - CPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/autogluon-inference:1.4.0-cpu-py311-ubuntu22.04</code> AutoGluon 1.3 py311 cu124 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/autogluon-inference:1.3.0-gpu-py311-cu124-ubuntu22.04</code> AutoGluon 1.3 py311 - CPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/autogluon-inference:1.3.0-cpu-py311-ubuntu22.04</code>"},{"location":"reference/available_images/#djl-inference","title":"DJL Inference","text":"Framework CUDA Engine Accelerator Platform Example URL DJLServing 0.36 cu128 LMI 19.0.0 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/djl-inference:0.36.0-lmi19.0.0-cu128</code> DJLServing 0.36 cu128 LMI 18.0.0 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/djl-inference:0.36.0-lmi18.0.0-cu128</code> DJLServing 0.36 - CPU Full CPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/djl-inference:0.36.0-cpu-full</code> DJLServing 0.35 cu128 LMI 17.0.0 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/djl-inference:0.35.0-lmi17.0.0-cu128</code> DJLServing 0.35 - CPU Full CPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/djl-inference:0.35.0-cpu-full</code> DJLServing 0.34 cu128 LMI 16.0.0 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/djl-inference:0.34.0-lmi16.0.0-cu128</code> DJLServing 0.33 cu128 LMI 15.0.0 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/djl-inference:0.33.0-lmi15.0.0-cu128</code> DJLServing 0.33 cu128 TensorRT-LLM 0.21.0 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/djl-inference:0.33.0-tensorrtllm0.21.0-cu128</code> DJLServing 0.32 cu126 LMI 14.0.0 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/djl-inference:0.32.0-lmi14.0.0-cu126</code> DJLServing 0.32 cu125 TensorRT-LLM 0.12.0 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/djl-inference:0.32.0-tensorrtllm0.12.0-cu125</code> DJLServing 0.31 cu124 LMI 13.0.0 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/djl-inference:0.31.0-lmi13.0.0-cu124</code> DJLServing 0.30 cu124 LMI 12.0.0 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/djl-inference:0.30.0-lmi12.0.0-cu124</code> DJLServing 0.30 cu125 TensorRT-LLM 0.12.0 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/djl-inference:0.30.0-tensorrtllm0.12.0-cu125</code> DJLServing 0.29 cu124 LMI 11.0.0 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124</code> DJLServing 0.29 cu124 TensorRT-LLM 0.11.0 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/djl-inference:0.29.0-tensorrtllm0.11.0-cu124</code> DJLServing 0.29 - CPU Full CPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/djl-inference:0.29.0-cpu-full</code> DJLServing 0.28 cu124 LMI 10.0.0 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/djl-inference:0.28.0-lmi10.0.0-cu124</code> DJLServing 0.28 cu122 TensorRT-LLM 0.9.0 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/djl-inference:0.28.0-tensorrtllm0.9.0-cu122</code> DJLServing 0.28 - CPU Full CPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/djl-inference:0.28.0-cpu-full</code> DJLServing 0.27 - CPU Full CPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/djl-inference:0.27.0-cpu-full</code>"},{"location":"reference/available_images/#nvidia-triton-server-for-sagemaker","title":"NVIDIA Triton Server for SageMaker","text":"Framework Python Accelerator Platform Example URL Triton 25.09 py3 GPU SageMaker <code>007439368137.dkr.ecr.&lt;region&gt;.amazonaws.com/sagemaker-tritonserver:25.09-py3</code> Triton 25.04 py3 GPU SageMaker <code>007439368137.dkr.ecr.&lt;region&gt;.amazonaws.com/sagemaker-tritonserver:25.04-py3</code> Triton 24.09 py3 GPU SageMaker <code>007439368137.dkr.ecr.&lt;region&gt;.amazonaws.com/sagemaker-tritonserver:24.09-py3</code> Triton 24.05 py3 GPU SageMaker <code>007439368137.dkr.ecr.&lt;region&gt;.amazonaws.com/sagemaker-tritonserver:24.05-py3</code> Triton 24.03 py3 GPU SageMaker <code>007439368137.dkr.ecr.&lt;region&gt;.amazonaws.com/sagemaker-tritonserver:24.03-py3</code> Triton 24.01 py3 GPU SageMaker <code>007439368137.dkr.ecr.&lt;region&gt;.amazonaws.com/sagemaker-tritonserver:24.01-py3</code> Triton 23.12 py3 GPU SageMaker <code>007439368137.dkr.ecr.&lt;region&gt;.amazonaws.com/sagemaker-tritonserver:23.12-py3</code>"},{"location":"reference/available_images/#stabilityai-pytorch-inference","title":"StabilityAI PyTorch Inference","text":"Framework Python CUDA Accelerator Platform Example URL StabilityAI 2.0 py310 cu118 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/stabilityai-pytorch-inference:2.0.1-sgm0.1.0-gpu-py310-cu118-ubuntu20.04-sagemaker</code>"},{"location":"reference/available_images/#pytorch-training-compiler","title":"PyTorch Training Compiler","text":"Framework Python CUDA Accelerator Platform Example URL PyTorch 1.13 py39 cu117 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/pytorch-trcomp-training:1.13.1-gpu-py39-cu117-ubuntu20.04-sagemaker</code> PyTorch 1.12 py38 cu113 GPU SageMaker <code>763104351884.dkr.ecr.&lt;region&gt;.amazonaws.com/pytorch-trcomp-training:1.12.0-gpu-py38-cu113-ubuntu20.04-sagemaker</code>"},{"location":"reference/support_policy/","title":"Framework Support Policy","text":"<p>AWS Deep Learning Containers simplify image configuration for deep learning workloads and are optimized with the latest frameworks, hardware, drivers, libraries, and operating systems. This page details the framework support policy for DLCs.</p>"},{"location":"reference/support_policy/#glossary","title":"Glossary","text":"<ul> <li>GA (General Availability): The date when a framework version becomes officially supported and available for production use.</li> <li>EOP (End of Patch): The date after which a framework version no longer receives security patches or bug fixes.</li> </ul>"},{"location":"reference/support_policy/#supported-frameworks","title":"Supported Frameworks","text":"Framework Version GA Date EOP Date Base 13.0 2025-10-22 2026-10-22 Base 12.9 2025-08-18 2027-08-18 Base 12.8 2025-06-05 2026-06-05 PyTorch 2.9 2025-10-15 2026-10-15 PyTorch 2.8 2025-08-06 2026-08-06 PyTorch 2.7 2025-04-23 2026-04-23 TensorFlow 2.19 2025-03-11 2026-03-11"},{"location":"reference/support_policy/#unsupported-frameworks","title":"Unsupported Frameworks","text":"Framework Version GA Date EOP Date PyTorch 2.6 2025-01-29 2026-01-29 PyTorch 2.5 2024-10-29 2025-10-29 PyTorch 2.4 2024-07-24 2025-07-24 PyTorch 2.3 2024-04-24 2025-04-24 PyTorch 2.2 2024-01-30 2025-01-30 PyTorch 2.1 2023-10-04 2024-10-04 PyTorch 2.0 2023-03-15 2024-03-15 PyTorch 1.13 2022-10-28 2024-10-28 TensorFlow 2.18 2024-10-24 2025-10-24 TensorFlow 2.16 2024-03-07 2025-03-07 TensorFlow 2.14 2023-09-26 2024-09-26"},{"location":"releasenotes/","title":"Release Notes","text":"<p>This section contains release notes for AWS Deep Learning Containers organized by framework.</p>"},{"location":"releasenotes/#frameworks","title":"Frameworks","text":"<ul> <li>Base - Release notes for Base CUDA containers</li> <li>SGLang - Release notes for SGLang inference containers</li> <li>vLLM - Release notes for vLLM inference containers</li> <li>PyTorch - Release notes for PyTorch containers</li> <li>TensorFlow - Release notes for TensorFlow containers</li> </ul>"},{"location":"releasenotes/#resources","title":"Resources","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> <li>Discussion Forum</li> </ul>"},{"location":"releasenotes/base/","title":"Base Release Notes","text":"<p>Release notes for AWS Deep Learning Containers with Base.</p>"},{"location":"releasenotes/base/#base-130","title":"Base 13.0","text":"Framework Version Accelerator Platform Link Base 13.0 GPU EC2, ECS, EKS Release Notes"},{"location":"releasenotes/base/#base-129","title":"Base 12.9","text":"Framework Version Accelerator Platform Link Base 12.9 GPU EC2, ECS, EKS Release Notes"},{"location":"releasenotes/base/#base-128","title":"Base 12.8","text":"Framework Version Accelerator Platform Link Base 12.8.1 GPU EC2, ECS, EKS Release Notes Base 12.8.0 GPU EC2, ECS, EKS Release Notes"},{"location":"releasenotes/base/base-12.8.0-gpu-ec2/","title":"AWS Deep Learning Containers for Base 12.8.0 on EC2, ECS, EKS","text":"<p>AWS Deep Learning Containers for EC2, ECS, EKS are now available with Base 12.8.0.</p>"},{"location":"releasenotes/base/base-12.8.0-gpu-ec2/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced Base containers with CUDA 12.8.0 for EC2, ECS, EKS</p> </li> <li> <p>Added Python 3.12 support</p> </li> </ul>"},{"location":"releasenotes/base/base-12.8.0-gpu-ec2/#core-packages","title":"Core Packages","text":"Package Version Python 3.12.10 CUDA 12.8.0 cuDNN 9.7.1.26 NCCL 2.26.2-1 EFA 1.40.0"},{"location":"releasenotes/base/base-12.8.0-gpu-ec2/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/base/base-12.8.0-gpu-ec2/#reference","title":"Reference","text":""},{"location":"releasenotes/base/base-12.8.0-gpu-ec2/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/base:12.8.0-gpu-py312-cu128-ubuntu22.04-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/base:12.8-gpu-py312-cu128-ubuntu22.04-ec2-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/base:12.8.0-gpu-py312-ubuntu22.04-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/base:12.8-gpu-py312-ubuntu22.04-ec2\n\npublic.ecr.aws/deep-learning-containers/base:12.8.0-gpu-py312-cu128-ubuntu22.04-ec2\n\npublic.ecr.aws/deep-learning-containers/base:12.8-gpu-py312-cu128-ubuntu22.04-ec2-v1\n\npublic.ecr.aws/deep-learning-containers/base:12.8.0-gpu-py312-ubuntu22.04-ec2\n\npublic.ecr.aws/deep-learning-containers/base:12.8-gpu-py312-ubuntu22.04-ec2\n</code></pre>"},{"location":"releasenotes/base/base-12.8.0-gpu-ec2/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/base/base-12.8.1-gpu-ec2/","title":"AWS Deep Learning Containers for Base 12.8.1 on EC2, ECS, EKS","text":"<p>AWS Deep Learning Containers for EC2, ECS, EKS are now available with Base 12.8.1.</p>"},{"location":"releasenotes/base/base-12.8.1-gpu-ec2/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced Base containers with CUDA 12.8.1 for EC2, ECS, EKS</p> </li> <li> <p>Added Python 3.12 support</p> </li> <li> <p>Ubuntu 24.04 support</p> </li> </ul>"},{"location":"releasenotes/base/base-12.8.1-gpu-ec2/#core-packages","title":"Core Packages","text":"Package Version Python 3.12.10 CUDA 12.8.1 cuDNN 9.8.0.87 NCCL 2.26.2-1 EFA 1.42.0"},{"location":"releasenotes/base/base-12.8.1-gpu-ec2/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/base/base-12.8.1-gpu-ec2/#reference","title":"Reference","text":""},{"location":"releasenotes/base/base-12.8.1-gpu-ec2/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/base:12.8.1-gpu-py312-cu128-ubuntu24.04-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/base:12.8-gpu-py312-cu128-ubuntu24.04-ec2-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/base:12.8.1-gpu-py312-ubuntu24.04-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/base:12.8-gpu-py312-ubuntu24.04-ec2\n\npublic.ecr.aws/deep-learning-containers/base:12.8.1-gpu-py312-cu128-ubuntu24.04-ec2\n\npublic.ecr.aws/deep-learning-containers/base:12.8-gpu-py312-cu128-ubuntu24.04-ec2-v1\n\npublic.ecr.aws/deep-learning-containers/base:12.8.1-gpu-py312-ubuntu24.04-ec2\n\npublic.ecr.aws/deep-learning-containers/base:12.8-gpu-py312-ubuntu24.04-ec2\n</code></pre>"},{"location":"releasenotes/base/base-12.8.1-gpu-ec2/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/base/base-12.9-gpu-ec2/","title":"AWS Deep Learning Containers for Base 12.9 on EC2, ECS, EKS","text":"<p>AWS Deep Learning Containers for EC2, ECS, EKS are now available with Base 12.9.</p>"},{"location":"releasenotes/base/base-12.9-gpu-ec2/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced Base containers with CUDA 12.9 for EC2, ECS, EKS</p> </li> <li> <p>Added Python 3.12 support</p> </li> </ul>"},{"location":"releasenotes/base/base-12.9-gpu-ec2/#core-packages","title":"Core Packages","text":"Package Version Python 3.12.10 CUDA 12.9.1 cuDNN 9.10.2.21 NCCL 2.27.3-1 EFA 1.43.1"},{"location":"releasenotes/base/base-12.9-gpu-ec2/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/base/base-12.9-gpu-ec2/#reference","title":"Reference","text":""},{"location":"releasenotes/base/base-12.9-gpu-ec2/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/base:12.9.1-gpu-py312-cu129-ubuntu22.04-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/base:12.9-gpu-py312-cu129-ubuntu22.04-ec2-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/base:12.9.1-gpu-py312-ubuntu22.04-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/base:12.9-gpu-py312-ubuntu22.04-ec2\n\npublic.ecr.aws/deep-learning-containers/base:12.9.1-gpu-py312-cu129-ubuntu22.04-ec2\n\npublic.ecr.aws/deep-learning-containers/base:12.9-gpu-py312-cu129-ubuntu22.04-ec2-v1\n\npublic.ecr.aws/deep-learning-containers/base:12.9.1-gpu-py312-ubuntu22.04-ec2\n\npublic.ecr.aws/deep-learning-containers/base:12.9-gpu-py312-ubuntu22.04-ec2\n</code></pre>"},{"location":"releasenotes/base/base-12.9-gpu-ec2/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/base/base-13.0-gpu-ec2/","title":"AWS Deep Learning Containers for Base 13.0 on EC2, ECS, EKS","text":"<p>AWS Deep Learning Containers for EC2, ECS, EKS are now available with Base 13.0.</p>"},{"location":"releasenotes/base/base-13.0-gpu-ec2/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced Base containers with CUDA 13.0 for EC2, ECS, EKS</p> </li> <li> <p>Added Python 3.12 support</p> </li> </ul>"},{"location":"releasenotes/base/base-13.0-gpu-ec2/#core-packages","title":"Core Packages","text":"Package Version Python 3.12.10 CUDA 13.0.0 cuDNN 9.13.0.50 NCCL 2.27.7-1 EFA 1.44.0"},{"location":"releasenotes/base/base-13.0-gpu-ec2/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/base/base-13.0-gpu-ec2/#reference","title":"Reference","text":""},{"location":"releasenotes/base/base-13.0-gpu-ec2/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/base:13.0.0-gpu-py312-cu130-ubuntu22.04-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/base:13.0-gpu-py312-cu130-ubuntu22.04-ec2-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/base:13.0.0-gpu-py312-ubuntu22.04-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/base:13.0-gpu-py312-ubuntu22.04-ec2\n\npublic.ecr.aws/deep-learning-containers/base:13.0.0-gpu-py312-cu130-ubuntu22.04-ec2\n\npublic.ecr.aws/deep-learning-containers/base:13.0-gpu-py312-cu130-ubuntu22.04-ec2-v1\n\npublic.ecr.aws/deep-learning-containers/base:13.0.0-gpu-py312-ubuntu22.04-ec2\n\npublic.ecr.aws/deep-learning-containers/base:13.0-gpu-py312-ubuntu22.04-ec2\n</code></pre>"},{"location":"releasenotes/base/base-13.0-gpu-ec2/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/pytorch/","title":"PyTorch Release Notes","text":"<p>Release notes for AWS Deep Learning Containers with PyTorch.</p>"},{"location":"releasenotes/pytorch/#pytorch-29","title":"PyTorch 2.9","text":"Framework Version Accelerator Platform Link PyTorch Training 2.9 GPU SageMaker Release Notes PyTorch Training 2.9 CPU SageMaker Release Notes PyTorch Training 2.9 GPU EC2, ECS, EKS Release Notes PyTorch Training 2.9 CPU EC2, ECS, EKS Release Notes"},{"location":"releasenotes/pytorch/#pytorch-28","title":"PyTorch 2.8","text":"Framework Version Accelerator Platform Link PyTorch Training 2.8 GPU SageMaker Release Notes PyTorch Training 2.8 CPU SageMaker Release Notes PyTorch Training 2.8 GPU EC2, ECS, EKS Release Notes PyTorch Training 2.8 CPU EC2, ECS, EKS Release Notes"},{"location":"releasenotes/pytorch/#pytorch-27","title":"PyTorch 2.7","text":"Framework Version Accelerator Platform Link PyTorch Training 2.7 GPU SageMaker Release Notes PyTorch Training 2.7 CPU SageMaker Release Notes PyTorch Training 2.7 GPU EC2, ECS, EKS Release Notes PyTorch Training 2.7 CPU EC2, ECS, EKS Release Notes PyTorch Training ARM64 2.7 GPU EC2, ECS, EKS Release Notes <p>Deprecated Images</p> <p>The following images are no longer supported and do not receive security patches or bug fixes.</p>"},{"location":"releasenotes/pytorch/#pytorch-26","title":"PyTorch 2.6","text":"Framework Version Accelerator Platform Link PyTorch Training 2.6 GPU SageMaker Release Notes PyTorch Training 2.6 CPU SageMaker Release Notes PyTorch Training 2.6 GPU EC2, ECS, EKS Release Notes PyTorch Training 2.6 CPU EC2, ECS, EKS Release Notes PyTorch Inference 2.6 GPU SageMaker Release Notes PyTorch Inference 2.6 CPU SageMaker Release Notes PyTorch Inference 2.6 GPU EC2, ECS, EKS Release Notes PyTorch Inference 2.6 CPU EC2, ECS, EKS Release Notes PyTorch Inference ARM64 2.6 CPU SageMaker Release Notes PyTorch Inference ARM64 2.6 GPU EC2, ECS, EKS Release Notes PyTorch Inference ARM64 2.6 CPU EC2, ECS, EKS Release Notes"},{"location":"releasenotes/pytorch/pytorch-inference-2.6-cpu-ec2/","title":"AWS Deep Learning Containers for PyTorch Inference 2.6 on EC2, ECS, EKS","text":"<p>AWS Deep Learning Containers for EC2, ECS, EKS are now available with PyTorch 2.6.</p>"},{"location":"releasenotes/pytorch/pytorch-inference-2.6-cpu-ec2/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced containers for PyTorch 2.6.0 for Inference on EC2, ECS, EKS</p> </li> <li> <p>Added Python 3.12 support</p> </li> <li> <p>Starting with PyTorch 2.6, we are removing Conda from the DLCs and installing all Python packages from PyPI</p> </li> </ul>"},{"location":"releasenotes/pytorch/pytorch-inference-2.6-cpu-ec2/#core-packages","title":"Core Packages","text":"Package Version Python 3.12.12 PyTorch 2.6.0 TorchVision 0.21.0 TorchAudio 2.6.0 TorchText 0.18.0 TorchData 0.10.1 TorchServe 0.12.0 OpenMPI 4.1.7"},{"location":"releasenotes/pytorch/pytorch-inference-2.6-cpu-ec2/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/pytorch/pytorch-inference-2.6-cpu-ec2/#reference","title":"Reference","text":""},{"location":"releasenotes/pytorch/pytorch-inference-2.6-cpu-ec2/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:2.6.0-cpu-py312-ubuntu22.04-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:2.6-cpu-py312-ubuntu22.04-ec2-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:2.6.0-cpu-py312-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:2.6-cpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-inference:2.6.0-cpu-py312-ubuntu22.04-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-inference:2.6-cpu-py312-ubuntu22.04-ec2-v1\n\npublic.ecr.aws/deep-learning-containers/pytorch-inference:2.6.0-cpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-inference:2.6-cpu-py312-ec2\n</code></pre>"},{"location":"releasenotes/pytorch/pytorch-inference-2.6-cpu-ec2/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/pytorch/pytorch-inference-2.6-cpu-sagemaker/","title":"AWS Deep Learning Containers for PyTorch Inference 2.6 on SageMaker","text":"<p>AWS Deep Learning Containers for SageMaker are now available with PyTorch 2.6.</p>"},{"location":"releasenotes/pytorch/pytorch-inference-2.6-cpu-sagemaker/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced containers for PyTorch 2.6.0 for Inference on SageMaker</p> </li> <li> <p>Added Python 3.12 support</p> </li> <li> <p>Starting with PyTorch 2.6, we are removing Conda from the DLCs and installing all Python packages from PyPI</p> </li> </ul>"},{"location":"releasenotes/pytorch/pytorch-inference-2.6-cpu-sagemaker/#core-packages","title":"Core Packages","text":"Package Version Python 3.12.12 PyTorch 2.6.0 TorchVision 0.21.0 TorchAudio 2.6.0 TorchText 0.18.0 TorchData 0.10.1 TorchServe 0.12.0 OpenMPI 4.1.7 SageMaker PyTorch Inference Toolkit 2.0.25"},{"location":"releasenotes/pytorch/pytorch-inference-2.6-cpu-sagemaker/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/pytorch/pytorch-inference-2.6-cpu-sagemaker/#reference","title":"Reference","text":""},{"location":"releasenotes/pytorch/pytorch-inference-2.6-cpu-sagemaker/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:2.6.0-cpu-py312-ubuntu22.04-sagemaker\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:2.6-cpu-py312-ubuntu22.04-sagemaker-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:2.6.0-cpu-py312\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:2.6-cpu-py312\n\npublic.ecr.aws/deep-learning-containers/pytorch-inference:2.6.0-cpu-py312-ubuntu22.04-sagemaker\n\npublic.ecr.aws/deep-learning-containers/pytorch-inference:2.6-cpu-py312-ubuntu22.04-sagemaker-v1\n\npublic.ecr.aws/deep-learning-containers/pytorch-inference:2.6.0-cpu-py312\n\npublic.ecr.aws/deep-learning-containers/pytorch-inference:2.6-cpu-py312\n</code></pre>"},{"location":"releasenotes/pytorch/pytorch-inference-2.6-cpu-sagemaker/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/pytorch/pytorch-inference-2.6-gpu-ec2/","title":"AWS Deep Learning Containers for PyTorch Inference 2.6 on EC2, ECS, EKS","text":"<p>AWS Deep Learning Containers for EC2, ECS, EKS are now available with PyTorch 2.6.</p>"},{"location":"releasenotes/pytorch/pytorch-inference-2.6-gpu-ec2/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced containers for PyTorch 2.6.0 for Inference on EC2, ECS, EKS</p> </li> <li> <p>Added Python 3.12 support</p> </li> <li> <p>Starting with PyTorch 2.6, we are removing Conda from the DLCs and installing all Python packages from PyPI</p> </li> </ul>"},{"location":"releasenotes/pytorch/pytorch-inference-2.6-gpu-ec2/#core-packages","title":"Core Packages","text":"Package Version Python 3.12.12 PyTorch 2.6.0 TorchVision 0.21.0 TorchAudio 2.6.0 TorchText 0.18.0 TorchData 0.10.1 TorchServe 0.12.0 CUDA 12.4.1 cuDNN 9.1.0.70 NCCL 2.23.4 OpenMPI 4.1.7"},{"location":"releasenotes/pytorch/pytorch-inference-2.6-gpu-ec2/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/pytorch/pytorch-inference-2.6-gpu-ec2/#reference","title":"Reference","text":""},{"location":"releasenotes/pytorch/pytorch-inference-2.6-gpu-ec2/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:2.6.0-gpu-py312-cu124-ubuntu22.04-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:2.6-gpu-py312-cu124-ubuntu22.04-ec2-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:2.6.0-gpu-py312-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:2.6-gpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-inference:2.6.0-gpu-py312-cu124-ubuntu22.04-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-inference:2.6-gpu-py312-cu124-ubuntu22.04-ec2-v1\n\npublic.ecr.aws/deep-learning-containers/pytorch-inference:2.6.0-gpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-inference:2.6-gpu-py312-ec2\n</code></pre>"},{"location":"releasenotes/pytorch/pytorch-inference-2.6-gpu-ec2/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/pytorch/pytorch-inference-2.6-gpu-sagemaker/","title":"AWS Deep Learning Containers for PyTorch Inference 2.6 on SageMaker","text":"<p>AWS Deep Learning Containers for SageMaker are now available with PyTorch 2.6.</p>"},{"location":"releasenotes/pytorch/pytorch-inference-2.6-gpu-sagemaker/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced containers for PyTorch 2.6.0 for Inference on SageMaker</p> </li> <li> <p>Added Python 3.12 support</p> </li> <li> <p>Starting with PyTorch 2.6, we are removing Conda from the DLCs and installing all Python packages from PyPI</p> </li> </ul>"},{"location":"releasenotes/pytorch/pytorch-inference-2.6-gpu-sagemaker/#core-packages","title":"Core Packages","text":"Package Version Python 3.12.12 PyTorch 2.6.0 TorchVision 0.21.0 TorchAudio 2.6.0 TorchText 0.18.0 TorchData 0.10.1 TorchServe 0.12.0 CUDA 12.4.1 cuDNN 9.1.0.70 NCCL 2.23.4 OpenMPI 4.1.7 SageMaker PyTorch Inference Toolkit 2.0.25"},{"location":"releasenotes/pytorch/pytorch-inference-2.6-gpu-sagemaker/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/pytorch/pytorch-inference-2.6-gpu-sagemaker/#reference","title":"Reference","text":""},{"location":"releasenotes/pytorch/pytorch-inference-2.6-gpu-sagemaker/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:2.6.0-gpu-py312-cu124-ubuntu22.04-sagemaker\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:2.6-gpu-py312-cu124-ubuntu22.04-sagemaker-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:2.6.0-gpu-py312\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:2.6-gpu-py312\n\npublic.ecr.aws/deep-learning-containers/pytorch-inference:2.6.0-gpu-py312-cu124-ubuntu22.04-sagemaker\n\npublic.ecr.aws/deep-learning-containers/pytorch-inference:2.6-gpu-py312-cu124-ubuntu22.04-sagemaker-v1\n\npublic.ecr.aws/deep-learning-containers/pytorch-inference:2.6.0-gpu-py312\n\npublic.ecr.aws/deep-learning-containers/pytorch-inference:2.6-gpu-py312\n</code></pre>"},{"location":"releasenotes/pytorch/pytorch-inference-2.6-gpu-sagemaker/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/pytorch/pytorch-inference-arm64-2.6-cpu-ec2/","title":"AWS Deep Learning Containers for PyTorch Inference ARM64 2.6 on EC2, ECS, EKS","text":"<p>AWS Deep Learning Containers for EC2, ECS, EKS are now available with PyTorch 2.6.</p>"},{"location":"releasenotes/pytorch/pytorch-inference-arm64-2.6-cpu-ec2/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced containers for PyTorch 2.6.0 for Inference on EC2, ECS, EKS (ARM64)</p> </li> <li> <p>Added Python 3.12 support</p> </li> <li> <p>Starting with PyTorch 2.6, we are removing Conda from the DLCs and installing all Python packages from PyPI</p> </li> </ul>"},{"location":"releasenotes/pytorch/pytorch-inference-arm64-2.6-cpu-ec2/#core-packages","title":"Core Packages","text":"Package Version Python 3.12.8 PyTorch 2.6.0 TorchVision 0.21.0 TorchAudio 2.6.0 TorchText 0.18.0 TorchData 0.10.1 TorchServe 0.12.0 OpenMPI 4.1.7"},{"location":"releasenotes/pytorch/pytorch-inference-arm64-2.6-cpu-ec2/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/pytorch/pytorch-inference-arm64-2.6-cpu-ec2/#reference","title":"Reference","text":""},{"location":"releasenotes/pytorch/pytorch-inference-arm64-2.6-cpu-ec2/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference-arm64:2.6.0-cpu-py312-ubuntu22.04-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference-arm64:2.6-cpu-py312-ubuntu22.04-ec2-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference-arm64:2.6.0-cpu-py312-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference-arm64:2.6-cpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-inference-arm64:2.6.0-cpu-py312-ubuntu22.04-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-inference-arm64:2.6-cpu-py312-ubuntu22.04-ec2-v1\n\npublic.ecr.aws/deep-learning-containers/pytorch-inference-arm64:2.6.0-cpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-inference-arm64:2.6-cpu-py312-ec2\n</code></pre>"},{"location":"releasenotes/pytorch/pytorch-inference-arm64-2.6-cpu-ec2/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/pytorch/pytorch-inference-arm64-2.6-cpu-sagemaker/","title":"AWS Deep Learning Containers for PyTorch Inference ARM64 2.6 on SageMaker","text":"<p>AWS Deep Learning Containers for SageMaker are now available with PyTorch 2.6.</p>"},{"location":"releasenotes/pytorch/pytorch-inference-arm64-2.6-cpu-sagemaker/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced containers for PyTorch 2.6.0 for Inference on SageMaker (ARM64)</p> </li> <li> <p>Added Python 3.12 support</p> </li> <li> <p>Starting with PyTorch 2.6, we are removing Conda from the DLCs and installing all Python packages from PyPI</p> </li> </ul>"},{"location":"releasenotes/pytorch/pytorch-inference-arm64-2.6-cpu-sagemaker/#core-packages","title":"Core Packages","text":"Package Version Python 3.12.8 PyTorch 2.6.0 TorchVision 0.21.0 TorchAudio 2.6.0 TorchText 0.18.0 TorchData 0.10.1 TorchServe 0.12.0 OpenMPI 4.1.7 SageMaker PyTorch Inference Toolkit 2.0.25"},{"location":"releasenotes/pytorch/pytorch-inference-arm64-2.6-cpu-sagemaker/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/pytorch/pytorch-inference-arm64-2.6-cpu-sagemaker/#reference","title":"Reference","text":""},{"location":"releasenotes/pytorch/pytorch-inference-arm64-2.6-cpu-sagemaker/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference-arm64:2.6.0-cpu-py312-ubuntu22.04-sagemaker\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference-arm64:2.6-cpu-py312-ubuntu22.04-sagemaker-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference-arm64:2.6.0-cpu-py312\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference-arm64:2.6-cpu-py312\n\npublic.ecr.aws/deep-learning-containers/pytorch-inference-arm64:2.6.0-cpu-py312-ubuntu22.04-sagemaker\n\npublic.ecr.aws/deep-learning-containers/pytorch-inference-arm64:2.6-cpu-py312-ubuntu22.04-sagemaker-v1\n\npublic.ecr.aws/deep-learning-containers/pytorch-inference-arm64:2.6.0-cpu-py312\n\npublic.ecr.aws/deep-learning-containers/pytorch-inference-arm64:2.6-cpu-py312\n</code></pre>"},{"location":"releasenotes/pytorch/pytorch-inference-arm64-2.6-cpu-sagemaker/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/pytorch/pytorch-inference-arm64-2.6-gpu-ec2/","title":"AWS Deep Learning Containers for PyTorch Inference ARM64 2.6 on EC2, ECS, EKS","text":"<p>AWS Deep Learning Containers for EC2, ECS, EKS are now available with PyTorch 2.6.</p>"},{"location":"releasenotes/pytorch/pytorch-inference-arm64-2.6-gpu-ec2/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced containers for PyTorch 2.6.0 for Inference on EC2, ECS, EKS (ARM64)</p> </li> <li> <p>Added Python 3.12 support</p> </li> <li> <p>Starting with PyTorch 2.6, we are removing Conda from the DLCs and installing all Python packages from PyPI</p> </li> </ul>"},{"location":"releasenotes/pytorch/pytorch-inference-arm64-2.6-gpu-ec2/#core-packages","title":"Core Packages","text":"Package Version Python 3.12.8 PyTorch 2.6.0 TorchVision 0.21.0 TorchAudio 2.6.0 TorchText 0.18.0 TorchData 0.10.1 TorchServe 0.12.0 CUDA 12.4.1 cuDNN 9.1.0.70 NCCL 2.21.5 OpenMPI 4.1.7"},{"location":"releasenotes/pytorch/pytorch-inference-arm64-2.6-gpu-ec2/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/pytorch/pytorch-inference-arm64-2.6-gpu-ec2/#reference","title":"Reference","text":""},{"location":"releasenotes/pytorch/pytorch-inference-arm64-2.6-gpu-ec2/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference-arm64:2.6.0-gpu-py312-cu124-ubuntu22.04-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference-arm64:2.6-gpu-py312-cu124-ubuntu22.04-ec2-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference-arm64:2.6.0-gpu-py312-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference-arm64:2.6-gpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-inference-arm64:2.6.0-gpu-py312-cu124-ubuntu22.04-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-inference-arm64:2.6-gpu-py312-cu124-ubuntu22.04-ec2-v1\n\npublic.ecr.aws/deep-learning-containers/pytorch-inference-arm64:2.6.0-gpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-inference-arm64:2.6-gpu-py312-ec2\n</code></pre>"},{"location":"releasenotes/pytorch/pytorch-inference-arm64-2.6-gpu-ec2/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.6-cpu-ec2/","title":"AWS Deep Learning Containers for PyTorch Training 2.6 on EC2, ECS, EKS","text":"<p>AWS Deep Learning Containers for EC2, ECS, EKS are now available with PyTorch 2.6.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.6-cpu-ec2/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced containers for PyTorch 2.6 for Training on EC2, ECS, EKS</p> </li> <li> <p>Starting with PyTorch 2.6, we are removing Conda from the DLCs and installing all Python packages from PyPI</p> </li> <li> <p>Added Python 3.12, Ubuntu 22.04 support</p> </li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.6-cpu-ec2/#core-packages","title":"Core Packages","text":"Package Version Python 3.12.8 PyTorch 2.6.0 TorchVision 0.21.0 TorchAudio 2.6.0 TorchText 0.18.0 TorchData 0.10.1"},{"location":"releasenotes/pytorch/pytorch-training-2.6-cpu-ec2/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.6-cpu-ec2/#reference","title":"Reference","text":""},{"location":"releasenotes/pytorch/pytorch-training-2.6-cpu-ec2/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.6.0-cpu-py312-ubuntu22.04-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.6-cpu-py312-ubuntu22.04-ec2-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.6.0-cpu-py312-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.6-cpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.6.0-cpu-py312-ubuntu22.04-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.6-cpu-py312-ubuntu22.04-ec2-v1\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.6.0-cpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.6-cpu-py312-ec2\n</code></pre>"},{"location":"releasenotes/pytorch/pytorch-training-2.6-cpu-ec2/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.6-cpu-sagemaker/","title":"AWS Deep Learning Containers for PyTorch Training 2.6 on SageMaker","text":"<p>AWS Deep Learning Containers for SageMaker are now available with PyTorch 2.6.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.6-cpu-sagemaker/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced containers for PyTorch 2.6 for Training on SageMaker</p> </li> <li> <p>Starting with PyTorch 2.6, we are removing Conda from the DLCs and installing all Python packages from PyPI</p> </li> <li> <p>Added Python 3.12, Ubuntu 22.04 support</p> </li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.6-cpu-sagemaker/#core-packages","title":"Core Packages","text":"Package Version Python 3.12.8 PyTorch 2.6.0 TorchVision 0.21.0 TorchAudio 2.6.0 TorchText 0.18.0 TorchData 0.10.1"},{"location":"releasenotes/pytorch/pytorch-training-2.6-cpu-sagemaker/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.6-cpu-sagemaker/#reference","title":"Reference","text":""},{"location":"releasenotes/pytorch/pytorch-training-2.6-cpu-sagemaker/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.6.0-cpu-py312-ubuntu22.04-sagemaker\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.6-cpu-py312-ubuntu22.04-sagemaker-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.6.0-cpu-py312\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.6-cpu-py312\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.6.0-cpu-py312-ubuntu22.04-sagemaker\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.6-cpu-py312-ubuntu22.04-sagemaker-v1\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.6.0-cpu-py312\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.6-cpu-py312\n</code></pre>"},{"location":"releasenotes/pytorch/pytorch-training-2.6-cpu-sagemaker/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.6-gpu-ec2/","title":"AWS Deep Learning Containers for PyTorch Training 2.6 on EC2, ECS, EKS","text":"<p>AWS Deep Learning Containers for EC2, ECS, EKS are now available with PyTorch 2.6.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.6-gpu-ec2/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced containers for PyTorch 2.6 for Training on EC2, ECS, EKS</p> </li> <li> <p>Starting with PyTorch 2.6, we are removing Conda from the DLCs and installing all Python packages from PyPI</p> </li> <li> <p>Added Python 3.12, CUDA 12.6, Ubuntu 22.04 support</p> </li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.6-gpu-ec2/#core-packages","title":"Core Packages","text":"Package Version Python 3.12.8 PyTorch 2.6.0 TorchVision 0.21.0 TorchAudio 2.6.0 TorchText 0.18.0 TorchData 0.10.1 CUDA 12.6.3 cuDNN 9.7.0.66 NCCL 2.23.4 EFA 1.38.0 Transformer Engine 2.0 Flash Attention 2.7.3 GDRCopy 2.5"},{"location":"releasenotes/pytorch/pytorch-training-2.6-gpu-ec2/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.6-gpu-ec2/#reference","title":"Reference","text":""},{"location":"releasenotes/pytorch/pytorch-training-2.6-gpu-ec2/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.6.0-gpu-py312-cu126-ubuntu22.04-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.6-gpu-py312-cu126-ubuntu22.04-ec2-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.6.0-gpu-py312-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.6-gpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.6.0-gpu-py312-cu126-ubuntu22.04-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.6-gpu-py312-cu126-ubuntu22.04-ec2-v1\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.6.0-gpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.6-gpu-py312-ec2\n</code></pre>"},{"location":"releasenotes/pytorch/pytorch-training-2.6-gpu-ec2/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.6-gpu-sagemaker/","title":"AWS Deep Learning Containers for PyTorch Training 2.6 on SageMaker","text":"<p>AWS Deep Learning Containers for SageMaker are now available with PyTorch 2.6.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.6-gpu-sagemaker/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced containers for PyTorch 2.6 for Training on SageMaker</p> </li> <li> <p>Starting with PyTorch 2.6, we are removing Conda from the DLCs and installing all Python packages from PyPI</p> </li> <li> <p>Added Python 3.12, CUDA 12.6, Ubuntu 22.04 support</p> </li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.6-gpu-sagemaker/#core-packages","title":"Core Packages","text":"Package Version Python 3.12.8 PyTorch 2.6.0 TorchVision 0.21.0 TorchAudio 2.6.0 TorchText 0.18.0 TorchData 0.10.1 CUDA 12.6.3 cuDNN 9.7.0.66 NCCL 2.23.4 EFA 1.38.0 Transformer Engine 2.0 Flash Attention 2.7.3 GDRCopy 2.5"},{"location":"releasenotes/pytorch/pytorch-training-2.6-gpu-sagemaker/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.6-gpu-sagemaker/#reference","title":"Reference","text":""},{"location":"releasenotes/pytorch/pytorch-training-2.6-gpu-sagemaker/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.6.0-gpu-py312-cu126-ubuntu22.04-sagemaker\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.6-gpu-py312-cu126-ubuntu22.04-sagemaker-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.6.0-gpu-py312\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.6-gpu-py312\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.6.0-gpu-py312-cu126-ubuntu22.04-sagemaker\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.6-gpu-py312-cu126-ubuntu22.04-sagemaker-v1\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.6.0-gpu-py312\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.6-gpu-py312\n</code></pre>"},{"location":"releasenotes/pytorch/pytorch-training-2.6-gpu-sagemaker/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.7-cpu-ec2/","title":"AWS Deep Learning Containers for PyTorch Training 2.7 on EC2, ECS, EKS","text":"<p>AWS Deep Learning Containers for EC2, ECS, EKS are now available with PyTorch 2.7.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.7-cpu-ec2/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced containers for PyTorch 2.7 for Training on EC2, ECS, EKS</p> </li> <li> <p>Added Python 3.12 support</p> </li> <li> <p>Added PyTorch domain libraries: torchtnt 0.2.4, torchdata 0.11.0, torchaudio 2.7.1, torchvision 0.22.1</p> </li> <li> <p>Added Ubuntu 22.04 support</p> </li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.7-cpu-ec2/#core-packages","title":"Core Packages","text":"Package Version Python 3.12.10 PyTorch 2.7.1 TorchVision 0.22.1 TorchAudio 2.7.1 TorchData 0.11.0 TorchTNT 0.2.4"},{"location":"releasenotes/pytorch/pytorch-training-2.7-cpu-ec2/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.7-cpu-ec2/#reference","title":"Reference","text":""},{"location":"releasenotes/pytorch/pytorch-training-2.7-cpu-ec2/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.7.1-cpu-py312-ubuntu22.04-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.7-cpu-py312-ubuntu22.04-ec2-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.7.1-cpu-py312-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.7-cpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.7.1-cpu-py312-ubuntu22.04-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.7-cpu-py312-ubuntu22.04-ec2-v1\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.7.1-cpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.7-cpu-py312-ec2\n</code></pre>"},{"location":"releasenotes/pytorch/pytorch-training-2.7-cpu-ec2/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.7-cpu-sagemaker/","title":"AWS Deep Learning Containers for PyTorch Training 2.7 on SageMaker","text":"<p>AWS Deep Learning Containers for SageMaker are now available with PyTorch 2.7.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.7-cpu-sagemaker/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced containers for PyTorch 2.7 for Training on SageMaker</p> </li> <li> <p>Added Python 3.12 support</p> </li> <li> <p>Added PyTorch domain libraries: torchtnt 0.2.4, torchdata 0.11.0, torchaudio 2.7.1, torchvision 0.22.1</p> </li> <li> <p>Added Ubuntu 22.04 support</p> </li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.7-cpu-sagemaker/#core-packages","title":"Core Packages","text":"Package Version Python 3.12.10 PyTorch 2.7.1 TorchVision 0.22.1 TorchAudio 2.7.1 TorchData 0.11.0 TorchTNT 0.2.4"},{"location":"releasenotes/pytorch/pytorch-training-2.7-cpu-sagemaker/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.7-cpu-sagemaker/#reference","title":"Reference","text":""},{"location":"releasenotes/pytorch/pytorch-training-2.7-cpu-sagemaker/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.7.1-cpu-py312-ubuntu22.04-sagemaker\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.7-cpu-py312-ubuntu22.04-sagemaker-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.7.1-cpu-py312\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.7-cpu-py312\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.7.1-cpu-py312-ubuntu22.04-sagemaker\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.7-cpu-py312-ubuntu22.04-sagemaker-v1\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.7.1-cpu-py312\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.7-cpu-py312\n</code></pre>"},{"location":"releasenotes/pytorch/pytorch-training-2.7-cpu-sagemaker/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.7-gpu-ec2/","title":"AWS Deep Learning Containers for PyTorch Training 2.7 on EC2, ECS, EKS","text":"<p>AWS Deep Learning Containers for EC2, ECS, EKS are now available with PyTorch 2.7.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.7-gpu-ec2/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced containers for PyTorch 2.7 for Training on EC2, ECS, EKS</p> </li> <li> <p>Added Python 3.12 support</p> </li> <li> <p>Added PyTorch domain libraries: torchtnt 0.2.4, torchdata 0.11.0, torchaudio 2.7.1, torchvision 0.22.1</p> </li> <li> <p>Added CUDA 12.8, Ubuntu 22.04 support</p> </li> <li> <p>Added fastai 2.8.2 support</p> </li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.7-gpu-ec2/#core-packages","title":"Core Packages","text":"Package Version Python 3.12.10 PyTorch 2.7.1 TorchVision 0.22.1 TorchAudio 2.7.1 TorchData 0.11.0 TorchTNT 0.2.4 CUDA 12.8.0 cuDNN 9.7.1.26 NCCL 2.26.2 EFA 1.40.0 Transformer Engine 2.3 Flash Attention 2.7.4.post1 GDRCopy 2.5"},{"location":"releasenotes/pytorch/pytorch-training-2.7-gpu-ec2/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.7-gpu-ec2/#reference","title":"Reference","text":""},{"location":"releasenotes/pytorch/pytorch-training-2.7-gpu-ec2/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.7.1-gpu-py312-cu128-ubuntu22.04-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.7-gpu-py312-cu128-ubuntu22.04-ec2-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.7.1-gpu-py312-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.7-gpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.7.1-gpu-py312-cu128-ubuntu22.04-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.7-gpu-py312-cu128-ubuntu22.04-ec2-v1\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.7.1-gpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.7-gpu-py312-ec2\n</code></pre>"},{"location":"releasenotes/pytorch/pytorch-training-2.7-gpu-ec2/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.7-gpu-sagemaker/","title":"AWS Deep Learning Containers for PyTorch Training 2.7 on SageMaker","text":"<p>AWS Deep Learning Containers for SageMaker are now available with PyTorch 2.7.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.7-gpu-sagemaker/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced containers for PyTorch 2.7 for Training on SageMaker</p> </li> <li> <p>Added Python 3.12 support</p> </li> <li> <p>Added PyTorch domain libraries: torchtnt 0.2.4, torchdata 0.11.0, torchaudio 2.7.1, torchvision 0.22.1</p> </li> <li> <p>Added CUDA 12.8, Ubuntu 22.04 support</p> </li> <li> <p>Added fastai 2.8.2 support</p> </li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.7-gpu-sagemaker/#core-packages","title":"Core Packages","text":"Package Version Python 3.12.10 PyTorch 2.7.1 TorchVision 0.22.1 TorchAudio 2.7.1 TorchData 0.11.0 TorchTNT 0.2.4 CUDA 12.8.0 cuDNN 9.7.1.26 NCCL 2.26.2 EFA 1.40.0 Transformer Engine 2.3 Flash Attention 2.7.4.post1 GDRCopy 2.5"},{"location":"releasenotes/pytorch/pytorch-training-2.7-gpu-sagemaker/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.7-gpu-sagemaker/#reference","title":"Reference","text":""},{"location":"releasenotes/pytorch/pytorch-training-2.7-gpu-sagemaker/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.7.1-gpu-py312-cu128-ubuntu22.04-sagemaker\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.7-gpu-py312-cu128-ubuntu22.04-sagemaker-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.7.1-gpu-py312\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.7-gpu-py312\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.7.1-gpu-py312-cu128-ubuntu22.04-sagemaker\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.7-gpu-py312-cu128-ubuntu22.04-sagemaker-v1\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.7.1-gpu-py312\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.7-gpu-py312\n</code></pre>"},{"location":"releasenotes/pytorch/pytorch-training-2.7-gpu-sagemaker/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.8-cpu-ec2/","title":"AWS Deep Learning Containers for PyTorch Training 2.8 on EC2, ECS, EKS","text":"<p>AWS Deep Learning Containers for EC2, ECS, EKS are now available with PyTorch 2.8.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.8-cpu-ec2/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced containers for PyTorch 2.8 for Training on EC2, ECS, EKS</p> </li> <li> <p>Added Python 3.12 support</p> </li> <li> <p>Added PyTorch domain libraries: torchtnt 0.2.4, torchdata 0.11.0, torchaudio 2.8.0, torchvision 0.23.0</p> </li> <li> <p>Added Ubuntu 22.04 support</p> </li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.8-cpu-ec2/#core-packages","title":"Core Packages","text":"Package Version Python 3.12.10 PyTorch 2.8.0 TorchVision 0.23.0 TorchAudio 2.8.0 TorchData 0.11.0 TorchTNT 0.2.4"},{"location":"releasenotes/pytorch/pytorch-training-2.8-cpu-ec2/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.8-cpu-ec2/#reference","title":"Reference","text":""},{"location":"releasenotes/pytorch/pytorch-training-2.8-cpu-ec2/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.8.0-cpu-py312-ubuntu22.04-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.8-cpu-py312-ubuntu22.04-ec2-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.8.0-cpu-py312-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.8-cpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.8.0-cpu-py312-ubuntu22.04-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.8-cpu-py312-ubuntu22.04-ec2-v1\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.8.0-cpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.8-cpu-py312-ec2\n</code></pre>"},{"location":"releasenotes/pytorch/pytorch-training-2.8-cpu-ec2/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.8-cpu-sagemaker/","title":"AWS Deep Learning Containers for PyTorch Training 2.8 on SageMaker","text":"<p>AWS Deep Learning Containers for SageMaker are now available with PyTorch 2.8.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.8-cpu-sagemaker/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced containers for PyTorch 2.8 for Training on SageMaker</p> </li> <li> <p>Added Python 3.12 support</p> </li> <li> <p>Added PyTorch domain libraries: torchtnt 0.2.4, torchdata 0.11.0, torchaudio 2.8.0, torchvision 0.23.0</p> </li> <li> <p>Added Ubuntu 22.04 support</p> </li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.8-cpu-sagemaker/#core-packages","title":"Core Packages","text":"Package Version Python 3.12.10 PyTorch 2.8.0 TorchVision 0.23.0 TorchAudio 2.8.0 TorchData 0.11.0 TorchTNT 0.2.4"},{"location":"releasenotes/pytorch/pytorch-training-2.8-cpu-sagemaker/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.8-cpu-sagemaker/#reference","title":"Reference","text":""},{"location":"releasenotes/pytorch/pytorch-training-2.8-cpu-sagemaker/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.8.0-cpu-py312-ubuntu22.04-sagemaker\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.8-cpu-py312-ubuntu22.04-sagemaker-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.8.0-cpu-py312\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.8-cpu-py312\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.8.0-cpu-py312-ubuntu22.04-sagemaker\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.8-cpu-py312-ubuntu22.04-sagemaker-v1\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.8.0-cpu-py312\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.8-cpu-py312\n</code></pre>"},{"location":"releasenotes/pytorch/pytorch-training-2.8-cpu-sagemaker/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.8-gpu-ec2/","title":"AWS Deep Learning Containers for PyTorch Training 2.8 on EC2, ECS, EKS","text":"<p>AWS Deep Learning Containers for EC2, ECS, EKS are now available with PyTorch 2.8.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.8-gpu-ec2/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced containers for PyTorch 2.8 for Training on EC2, ECS, EKS</p> </li> <li> <p>Added Python 3.12 support</p> </li> <li> <p>Added PyTorch domain libraries: torchtnt 0.2.4, torchdata 0.11.0, torchaudio 2.8.0, torchvision 0.23.0</p> </li> <li> <p>Added CUDA 12.9, Ubuntu 22.04 support</p> </li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.8-gpu-ec2/#core-packages","title":"Core Packages","text":"Package Version Python 3.12.10 PyTorch 2.8.0 TorchVision 0.23.0 TorchAudio 2.8.0 TorchData 0.11.0 TorchTNT 0.2.4 CUDA 12.9.1 cuDNN 9.10.2.21 NCCL 2.27.3-1 EFA 1.43.1 Transformer Engine 2.5 Flash Attention 2.8.3 GDRCopy 2.5.1"},{"location":"releasenotes/pytorch/pytorch-training-2.8-gpu-ec2/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.8-gpu-ec2/#reference","title":"Reference","text":""},{"location":"releasenotes/pytorch/pytorch-training-2.8-gpu-ec2/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.8.0-gpu-py312-cu129-ubuntu22.04-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.8-gpu-py312-cu129-ubuntu22.04-ec2-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.8.0-gpu-py312-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.8-gpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.8.0-gpu-py312-cu129-ubuntu22.04-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.8-gpu-py312-cu129-ubuntu22.04-ec2-v1\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.8.0-gpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.8-gpu-py312-ec2\n</code></pre>"},{"location":"releasenotes/pytorch/pytorch-training-2.8-gpu-ec2/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.8-gpu-sagemaker/","title":"AWS Deep Learning Containers for PyTorch Training 2.8 on SageMaker","text":"<p>AWS Deep Learning Containers for SageMaker are now available with PyTorch 2.8.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.8-gpu-sagemaker/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced containers for PyTorch 2.8 for Training on SageMaker</p> </li> <li> <p>Added Python 3.12 support</p> </li> <li> <p>Added PyTorch domain libraries: torchtnt 0.2.4, torchdata 0.11.0, torchaudio 2.8.0, torchvision 0.23.0</p> </li> <li> <p>Added CUDA 12.9, Ubuntu 22.04 support</p> </li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.8-gpu-sagemaker/#core-packages","title":"Core Packages","text":"Package Version Python 3.12.10 PyTorch 2.8.0 TorchVision 0.23.0 TorchAudio 2.8.0 TorchData 0.11.0 TorchTNT 0.2.4 CUDA 12.9.1 cuDNN 9.10.2.21 NCCL 2.27.3-1 EFA 1.43.1 Transformer Engine 2.5 Flash Attention 2.8.3 GDRCopy 2.5.1"},{"location":"releasenotes/pytorch/pytorch-training-2.8-gpu-sagemaker/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.8-gpu-sagemaker/#reference","title":"Reference","text":""},{"location":"releasenotes/pytorch/pytorch-training-2.8-gpu-sagemaker/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.8.0-gpu-py312-cu129-ubuntu22.04-sagemaker\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.8-gpu-py312-cu129-ubuntu22.04-sagemaker-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.8.0-gpu-py312\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.8-gpu-py312\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.8.0-gpu-py312-cu129-ubuntu22.04-sagemaker\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.8-gpu-py312-cu129-ubuntu22.04-sagemaker-v1\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.8.0-gpu-py312\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.8-gpu-py312\n</code></pre>"},{"location":"releasenotes/pytorch/pytorch-training-2.8-gpu-sagemaker/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.9-cpu-ec2/","title":"AWS Deep Learning Containers for PyTorch Training 2.9 on EC2, ECS, EKS","text":"<p>AWS Deep Learning Containers for EC2, ECS, EKS are now available with PyTorch 2.9.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.9-cpu-ec2/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced containers for PyTorch 2.9 for Training on EC2, ECS, EKS</p> </li> <li> <p>Added Python 3.12 support</p> </li> <li> <p>Added PyTorch domain libraries: torchtnt 0.2.4, torchdata 0.11.0, torchaudio 2.9.0, torchvision 0.24.0</p> </li> <li> <p>Added Ubuntu 22.04 support</p> </li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.9-cpu-ec2/#core-packages","title":"Core Packages","text":"Package Version Python 3.12.10 PyTorch 2.9.0 TorchVision 0.24.0 TorchAudio 2.9.0 TorchData 0.11.0 TorchTNT 0.2.4"},{"location":"releasenotes/pytorch/pytorch-training-2.9-cpu-ec2/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.9-cpu-ec2/#reference","title":"Reference","text":""},{"location":"releasenotes/pytorch/pytorch-training-2.9-cpu-ec2/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.9.0-cpu-py312-ubuntu22.04-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.9-cpu-py312-ubuntu22.04-ec2-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.9.0-cpu-py312-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.9-cpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.9.0-cpu-py312-ubuntu22.04-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.9-cpu-py312-ubuntu22.04-ec2-v1\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.9.0-cpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.9-cpu-py312-ec2\n</code></pre>"},{"location":"releasenotes/pytorch/pytorch-training-2.9-cpu-ec2/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.9-cpu-sagemaker/","title":"AWS Deep Learning Containers for PyTorch Training 2.9 on SageMaker","text":"<p>AWS Deep Learning Containers for SageMaker are now available with PyTorch 2.9.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.9-cpu-sagemaker/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced containers for PyTorch 2.9 for Training on SageMaker</p> </li> <li> <p>Added Python 3.12 support</p> </li> <li> <p>Added PyTorch domain libraries: torchtnt 0.2.4, torchdata 0.11.0, torchaudio 2.9.0, torchvision 0.24.0</p> </li> <li> <p>Added Ubuntu 22.04 support</p> </li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.9-cpu-sagemaker/#core-packages","title":"Core Packages","text":"Package Version Python 3.12.10 PyTorch 2.9.0 TorchVision 0.24.0 TorchAudio 2.9.0 TorchData 0.11.0 TorchTNT 0.2.4"},{"location":"releasenotes/pytorch/pytorch-training-2.9-cpu-sagemaker/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.9-cpu-sagemaker/#reference","title":"Reference","text":""},{"location":"releasenotes/pytorch/pytorch-training-2.9-cpu-sagemaker/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.9.0-cpu-py312-ubuntu22.04-sagemaker\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.9-cpu-py312-ubuntu22.04-sagemaker-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.9.0-cpu-py312\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.9-cpu-py312\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.9.0-cpu-py312-ubuntu22.04-sagemaker\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.9-cpu-py312-ubuntu22.04-sagemaker-v1\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.9.0-cpu-py312\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.9-cpu-py312\n</code></pre>"},{"location":"releasenotes/pytorch/pytorch-training-2.9-cpu-sagemaker/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.9-gpu-ec2/","title":"AWS Deep Learning Containers for PyTorch Training 2.9 on EC2, ECS, EKS","text":"<p>AWS Deep Learning Containers for EC2, ECS, EKS are now available with PyTorch 2.9.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.9-gpu-ec2/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced containers for PyTorch 2.9 for Training on EC2, ECS, EKS</p> </li> <li> <p>Added Python 3.12 support</p> </li> <li> <p>Added PyTorch domain libraries: torchtnt 0.2.4, torchdata 0.11.0, torchaudio 2.9.0, torchvision 0.24.0</p> </li> <li> <p>Added CUDA 13.0, Ubuntu 22.04 support</p> </li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.9-gpu-ec2/#core-packages","title":"Core Packages","text":"Package Version Python 3.12.10 PyTorch 2.9.0 TorchVision 0.24.0 TorchAudio 2.9.0 TorchData 0.11.0 TorchTNT 0.2.4 CUDA 13.0.0 cuDNN 9.13.0.50 NCCL 2.27.7-1 EFA 1.44.0 Transformer Engine 2.9 Flash Attention 2.8.3 GDRCopy 2.5.1"},{"location":"releasenotes/pytorch/pytorch-training-2.9-gpu-ec2/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.9-gpu-ec2/#reference","title":"Reference","text":""},{"location":"releasenotes/pytorch/pytorch-training-2.9-gpu-ec2/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.9.0-gpu-py312-cu130-ubuntu22.04-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.9-gpu-py312-cu130-ubuntu22.04-ec2-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.9.0-gpu-py312-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.9-gpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.9.0-gpu-py312-cu130-ubuntu22.04-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.9-gpu-py312-cu130-ubuntu22.04-ec2-v1\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.9.0-gpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.9-gpu-py312-ec2\n</code></pre>"},{"location":"releasenotes/pytorch/pytorch-training-2.9-gpu-ec2/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.9-gpu-sagemaker/","title":"AWS Deep Learning Containers for PyTorch Training 2.9 on SageMaker","text":"<p>AWS Deep Learning Containers for SageMaker are now available with PyTorch 2.9.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.9-gpu-sagemaker/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced containers for PyTorch 2.9 for Training on SageMaker</p> </li> <li> <p>Added Python 3.12 support</p> </li> <li> <p>Added PyTorch domain libraries: torchtnt 0.2.4, torchdata 0.11.0, torchaudio 2.9.0, torchvision 0.24.0</p> </li> <li> <p>Added CUDA 13.0, Ubuntu 22.04 support</p> </li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-2.9-gpu-sagemaker/#core-packages","title":"Core Packages","text":"Package Version Python 3.12.10 PyTorch 2.9.0 TorchVision 0.24.0 TorchAudio 2.9.0 TorchData 0.11.0 TorchTNT 0.2.4 CUDA 13.0.0 cuDNN 9.13.0.50 NCCL 2.27.7-1 EFA 1.44.0 Transformer Engine 2.9 Flash Attention 2.8.3 GDRCopy 2.5.1"},{"location":"releasenotes/pytorch/pytorch-training-2.9-gpu-sagemaker/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/pytorch/pytorch-training-2.9-gpu-sagemaker/#reference","title":"Reference","text":""},{"location":"releasenotes/pytorch/pytorch-training-2.9-gpu-sagemaker/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.9.0-gpu-py312-cu130-ubuntu22.04-sagemaker\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.9-gpu-py312-cu130-ubuntu22.04-sagemaker-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.9.0-gpu-py312\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.9-gpu-py312\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.9.0-gpu-py312-cu130-ubuntu22.04-sagemaker\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.9-gpu-py312-cu130-ubuntu22.04-sagemaker-v1\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.9.0-gpu-py312\n\npublic.ecr.aws/deep-learning-containers/pytorch-training:2.9-gpu-py312\n</code></pre>"},{"location":"releasenotes/pytorch/pytorch-training-2.9-gpu-sagemaker/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-arm64-2.7-gpu-ec2/","title":"AWS Deep Learning Containers for PyTorch Training ARM64 2.7 on EC2, ECS, EKS","text":"<p>AWS Deep Learning Containers for EC2, ECS, EKS are now available with PyTorch 2.7.</p>"},{"location":"releasenotes/pytorch/pytorch-training-arm64-2.7-gpu-ec2/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced containers for PyTorch 2.7.0 for Training on EC2, ECS, EKS (ARM64)</p> </li> <li> <p>Added Python 3.12 support</p> </li> </ul>"},{"location":"releasenotes/pytorch/pytorch-training-arm64-2.7-gpu-ec2/#core-packages","title":"Core Packages","text":"Package Version Python 3.12.8 PyTorch 2.7.0 TorchVision 0.22.0 TorchAudio 2.7.0 TorchText 0.18.0 TorchData 0.11.0 CUDA 12.8.0 cuDNN 9.8.0.87 NCCL 2.27.5 EFA 1.42.0 Transformer Engine 2.0 Flash Attention 2.7.3 GDRCopy 2.5"},{"location":"releasenotes/pytorch/pytorch-training-arm64-2.7-gpu-ec2/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/pytorch/pytorch-training-arm64-2.7-gpu-ec2/#reference","title":"Reference","text":""},{"location":"releasenotes/pytorch/pytorch-training-arm64-2.7-gpu-ec2/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training-arm64:2.7.0-gpu-py312-cu128-ubuntu22.04-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training-arm64:2.7-gpu-py312-cu128-ubuntu22.04-ec2-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training-arm64:2.7.0-gpu-py312-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training-arm64:2.7-gpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-training-arm64:2.7.0-gpu-py312-cu128-ubuntu22.04-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-training-arm64:2.7-gpu-py312-cu128-ubuntu22.04-ec2-v1\n\npublic.ecr.aws/deep-learning-containers/pytorch-training-arm64:2.7.0-gpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/pytorch-training-arm64:2.7-gpu-py312-ec2\n</code></pre>"},{"location":"releasenotes/pytorch/pytorch-training-arm64-2.7-gpu-ec2/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/sglang/","title":"SGLang Release Notes","text":"<p>Release notes for AWS Deep Learning Containers with SGLang.</p>"},{"location":"releasenotes/sglang/#sglang-05","title":"SGLang 0.5","text":"Framework Version Accelerator Platform Link SGLang 0.5.8 GPU SageMaker Release Notes SGLang 0.5.7 GPU SageMaker Release Notes SGLang 0.5.6 GPU SageMaker Release Notes SGLang 0.5.5 GPU SageMaker Release Notes"},{"location":"releasenotes/sglang/sglang-0.5.5-gpu-sagemaker/","title":"AWS Deep Learning Containers for SGLang 0.5.5 on SageMaker","text":"<p>AWS Deep Learning Containers for SageMaker are now available with SGLang 0.5.5.</p>"},{"location":"releasenotes/sglang/sglang-0.5.5-gpu-sagemaker/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced SGLang 0.5.5 containers for SageMaker</p> </li> <li> <p>Added Python 3.12 support</p> </li> </ul>"},{"location":"releasenotes/sglang/sglang-0.5.5-gpu-sagemaker/#core-packages","title":"Core Packages","text":"Package Version SGLang 0.5.5 SGLang Kernel 0.3.16.post5 PyTorch 2.8.0 TorchVision 0.23.0 TorchAudio 2.8.0 TorchAO 0.9.0 Transformers 4.57.1 FlashInfer 0.5.0 CUDA 12.9.1 cuDNN 9.10.2.21-1 NCCL 2.27.3-1 EFA 1.43.3 GDRCopy 2.5.1"},{"location":"releasenotes/sglang/sglang-0.5.5-gpu-sagemaker/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/sglang/sglang-0.5.5-gpu-sagemaker/#reference","title":"Reference","text":""},{"location":"releasenotes/sglang/sglang-0.5.5-gpu-sagemaker/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/sglang:0.5.5-gpu-py312-cu129-ubuntu22.04-sagemaker\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/sglang:0.5.5-gpu-py312\n\npublic.ecr.aws/deep-learning-containers/sglang:0.5.5-gpu-py312-cu129-ubuntu22.04-sagemaker\n\npublic.ecr.aws/deep-learning-containers/sglang:0.5.5-gpu-py312\n</code></pre>"},{"location":"releasenotes/sglang/sglang-0.5.5-gpu-sagemaker/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/sglang/sglang-0.5.6-gpu-sagemaker/","title":"AWS Deep Learning Containers for SGLang 0.5.6 on SageMaker","text":"<p>AWS Deep Learning Containers for SageMaker are now available with SGLang 0.5.6.</p>"},{"location":"releasenotes/sglang/sglang-0.5.6-gpu-sagemaker/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced SGLang 0.5.6 containers for SageMaker</p> </li> <li> <p>Added Python 3.12 support</p> </li> </ul>"},{"location":"releasenotes/sglang/sglang-0.5.6-gpu-sagemaker/#core-packages","title":"Core Packages","text":"Package Version SGLang 0.5.6 SGLang Kernel 0.3.18.post2 PyTorch 2.9.1 TorchVision 0.24.1 TorchAudio 2.9.1 TorchAO 0.9.0 Transformers 4.57.1 FlashInfer 0.5.3 CUDA 12.9.1 cuDNN 9.10.2.21-1 NCCL 2.27.3-1 EFA 43.3 GDRCopy 2.5.1"},{"location":"releasenotes/sglang/sglang-0.5.6-gpu-sagemaker/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/sglang/sglang-0.5.6-gpu-sagemaker/#reference","title":"Reference","text":""},{"location":"releasenotes/sglang/sglang-0.5.6-gpu-sagemaker/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/sglang:0.5.6-gpu-py312-cu129-ubuntu22.04-sagemaker\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/sglang:0.5.6-gpu-py312\n\npublic.ecr.aws/deep-learning-containers/sglang:0.5.6-gpu-py312-cu129-ubuntu22.04-sagemaker\n\npublic.ecr.aws/deep-learning-containers/sglang:0.5.6-gpu-py312\n</code></pre>"},{"location":"releasenotes/sglang/sglang-0.5.6-gpu-sagemaker/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/sglang/sglang-0.5.7-gpu-sagemaker/","title":"AWS Deep Learning Containers for SGLang 0.5.7 on SageMaker","text":"<p>AWS Deep Learning Containers for SageMaker are now available with SGLang 0.5.7.</p>"},{"location":"releasenotes/sglang/sglang-0.5.7-gpu-sagemaker/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced SGLang 0.5.7 containers for SageMaker</p> </li> <li> <p>Added Python 3.12 support</p> </li> <li> <p>Ubuntu 24.04 support</p> </li> </ul>"},{"location":"releasenotes/sglang/sglang-0.5.7-gpu-sagemaker/#core-packages","title":"Core Packages","text":"Package Version SGLang 0.5.7 SGLang Kernel 0.3.20 PyTorch 2.9.1 TorchVision 0.24.1 TorchAudio 2.9.1 TorchAO 0.9.0 Transformers 4.57.1 FlashInfer 0.5.3 CUDA 12.9.1 cuDNN 9.10.2.21-1 NCCL 2.27.3-1 EFA 1.46.0 GDRCopy 2.5.1"},{"location":"releasenotes/sglang/sglang-0.5.7-gpu-sagemaker/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/sglang/sglang-0.5.7-gpu-sagemaker/#reference","title":"Reference","text":""},{"location":"releasenotes/sglang/sglang-0.5.7-gpu-sagemaker/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/sglang:0.5.7-gpu-py312-cu129-ubuntu24.04-sagemaker\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/sglang:0.5.7-gpu-py312\n\npublic.ecr.aws/deep-learning-containers/sglang:0.5.7-gpu-py312-cu129-ubuntu24.04-sagemaker\n\npublic.ecr.aws/deep-learning-containers/sglang:0.5.7-gpu-py312\n</code></pre>"},{"location":"releasenotes/sglang/sglang-0.5.7-gpu-sagemaker/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/sglang/sglang-0.5.8-gpu-sagemaker/","title":"AWS Deep Learning Containers for SGLang 0.5.8 on SageMaker","text":"<p>AWS Deep Learning Containers for SageMaker are now available with SGLang 0.5.8.</p>"},{"location":"releasenotes/sglang/sglang-0.5.8-gpu-sagemaker/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced SGLang 0.5.8 containers for SageMaker</p> </li> <li> <p>Added Python 3.12 support</p> </li> </ul>"},{"location":"releasenotes/sglang/sglang-0.5.8-gpu-sagemaker/#core-packages","title":"Core Packages","text":"Package Version SGLang 0.5.8 SGLang Kernel 0.3.21 PyTorch 2.9.1 TorchVision 0.24.1 TorchAudio 2.9.1 TorchAO 0.9.0 Transformers 4.57.1 FlashInfer 0.6.1 CUDA 12.9.1 cuDNN 9.10.2.21-1 NCCL 2.27.3-1 EFA 1.46.0 GDRCopy 2.5.1"},{"location":"releasenotes/sglang/sglang-0.5.8-gpu-sagemaker/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/sglang/sglang-0.5.8-gpu-sagemaker/#reference","title":"Reference","text":""},{"location":"releasenotes/sglang/sglang-0.5.8-gpu-sagemaker/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/sglang:0.5.8-gpu-py312-cu129-ubuntu24.04-sagemaker\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/sglang:0.5-gpu-py312-cu129-ubuntu24.04-sagemaker-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/sglang:0.5.8-gpu-py312\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/sglang:0.5-gpu-py312\n\npublic.ecr.aws/deep-learning-containers/sglang:0.5.8-gpu-py312-cu129-ubuntu24.04-sagemaker\n\npublic.ecr.aws/deep-learning-containers/sglang:0.5-gpu-py312-cu129-ubuntu24.04-sagemaker-v1\n\npublic.ecr.aws/deep-learning-containers/sglang:0.5.8-gpu-py312\n\npublic.ecr.aws/deep-learning-containers/sglang:0.5-gpu-py312\n</code></pre>"},{"location":"releasenotes/sglang/sglang-0.5.8-gpu-sagemaker/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/tensorflow/","title":"TensorFlow Release Notes","text":"<p>Release notes for AWS Deep Learning Containers with TensorFlow.</p>"},{"location":"releasenotes/tensorflow/#tensorflow-219","title":"TensorFlow 2.19","text":"Framework Version Accelerator Platform Link TensorFlow Training 2.19 GPU SageMaker Release Notes TensorFlow Training 2.19 CPU SageMaker Release Notes TensorFlow Inference 2.19 GPU SageMaker Release Notes TensorFlow Inference 2.19 CPU SageMaker Release Notes TensorFlow Inference ARM64 2.19 CPU SageMaker Release Notes"},{"location":"releasenotes/tensorflow/tensorflow-inference-2.19-cpu-sagemaker/","title":"AWS Deep Learning Containers for TensorFlow Inference 2.19 on SageMaker","text":"<p>AWS Deep Learning Containers for SageMaker are now available with TensorFlow 2.19.</p>"},{"location":"releasenotes/tensorflow/tensorflow-inference-2.19-cpu-sagemaker/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced TensorFlow 2.19 inference containers for SageMaker</p> </li> <li> <p>Added Python 3.12 support</p> </li> </ul>"},{"location":"releasenotes/tensorflow/tensorflow-inference-2.19-cpu-sagemaker/#core-packages","title":"Core Packages","text":"Package Version TensorFlow 2.19.0 Python 3.12.10"},{"location":"releasenotes/tensorflow/tensorflow-inference-2.19-cpu-sagemaker/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/tensorflow/tensorflow-inference-2.19-cpu-sagemaker/#reference","title":"Reference","text":""},{"location":"releasenotes/tensorflow/tensorflow-inference-2.19-cpu-sagemaker/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-inference:2.19.0-cpu-py312-ubuntu22.04-sagemaker\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-inference:2.19-cpu-py312-ubuntu22.04-sagemaker-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-inference:2.19.0-cpu-py312-sagemaker\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-inference:2.19-cpu-py312-sagemaker\n\npublic.ecr.aws/deep-learning-containers/tensorflow-inference:2.19.0-cpu-py312-ubuntu22.04-sagemaker\n\npublic.ecr.aws/deep-learning-containers/tensorflow-inference:2.19-cpu-py312-ubuntu22.04-sagemaker-v1\n\npublic.ecr.aws/deep-learning-containers/tensorflow-inference:2.19.0-cpu-py312-sagemaker\n\npublic.ecr.aws/deep-learning-containers/tensorflow-inference:2.19-cpu-py312-sagemaker\n</code></pre>"},{"location":"releasenotes/tensorflow/tensorflow-inference-2.19-cpu-sagemaker/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/tensorflow/tensorflow-inference-2.19-gpu-sagemaker/","title":"AWS Deep Learning Containers for TensorFlow Inference 2.19 on SageMaker","text":"<p>AWS Deep Learning Containers for SageMaker are now available with TensorFlow 2.19.</p>"},{"location":"releasenotes/tensorflow/tensorflow-inference-2.19-gpu-sagemaker/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced TensorFlow 2.19 inference containers for SageMaker</p> </li> <li> <p>Added Python 3.12 support</p> </li> </ul>"},{"location":"releasenotes/tensorflow/tensorflow-inference-2.19-gpu-sagemaker/#core-packages","title":"Core Packages","text":"Package Version TensorFlow 2.19.0 Python 3.12.10 CUDA 12.2.0 cuDNN 8.9.4.25 NCCL 2.18.3"},{"location":"releasenotes/tensorflow/tensorflow-inference-2.19-gpu-sagemaker/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/tensorflow/tensorflow-inference-2.19-gpu-sagemaker/#reference","title":"Reference","text":""},{"location":"releasenotes/tensorflow/tensorflow-inference-2.19-gpu-sagemaker/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-inference:2.19.0-gpu-py312-cu122-ubuntu22.04-sagemaker\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-inference:2.19-gpu-py312-cu122-ubuntu22.04-sagemaker-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-inference:2.19.0-gpu-py312-sagemaker\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-inference:2.19-gpu-py312-sagemaker\n\npublic.ecr.aws/deep-learning-containers/tensorflow-inference:2.19.0-gpu-py312-cu122-ubuntu22.04-sagemaker\n\npublic.ecr.aws/deep-learning-containers/tensorflow-inference:2.19-gpu-py312-cu122-ubuntu22.04-sagemaker-v1\n\npublic.ecr.aws/deep-learning-containers/tensorflow-inference:2.19.0-gpu-py312-sagemaker\n\npublic.ecr.aws/deep-learning-containers/tensorflow-inference:2.19-gpu-py312-sagemaker\n</code></pre>"},{"location":"releasenotes/tensorflow/tensorflow-inference-2.19-gpu-sagemaker/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/tensorflow/tensorflow-inference-arm64-2.19-cpu-sagemaker/","title":"AWS Deep Learning Containers for TensorFlow Inference ARM64 2.19 on SageMaker","text":"<p>AWS Deep Learning Containers for SageMaker are now available with TensorFlow 2.19.</p>"},{"location":"releasenotes/tensorflow/tensorflow-inference-arm64-2.19-cpu-sagemaker/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced TensorFlow 2.19 ARM64 inference containers for SageMaker</p> </li> <li> <p>Added Python 3.12 support</p> </li> </ul>"},{"location":"releasenotes/tensorflow/tensorflow-inference-arm64-2.19-cpu-sagemaker/#core-packages","title":"Core Packages","text":"Package Version TensorFlow 2.19.0 Python 3.12.10"},{"location":"releasenotes/tensorflow/tensorflow-inference-arm64-2.19-cpu-sagemaker/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/tensorflow/tensorflow-inference-arm64-2.19-cpu-sagemaker/#reference","title":"Reference","text":""},{"location":"releasenotes/tensorflow/tensorflow-inference-arm64-2.19-cpu-sagemaker/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-inference-arm64:2.19.0-cpu-py312-ubuntu22.04-sagemaker\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-inference-arm64:2.19-cpu-py312-ubuntu22.04-sagemaker-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-inference-arm64:2.19.0-cpu-py312-sagemaker\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-inference-arm64:2.19-cpu-py312-sagemaker\n\npublic.ecr.aws/deep-learning-containers/tensorflow-inference-arm64:2.19.0-cpu-py312-ubuntu22.04-sagemaker\n\npublic.ecr.aws/deep-learning-containers/tensorflow-inference-arm64:2.19-cpu-py312-ubuntu22.04-sagemaker-v1\n\npublic.ecr.aws/deep-learning-containers/tensorflow-inference-arm64:2.19.0-cpu-py312-sagemaker\n\npublic.ecr.aws/deep-learning-containers/tensorflow-inference-arm64:2.19-cpu-py312-sagemaker\n</code></pre>"},{"location":"releasenotes/tensorflow/tensorflow-inference-arm64-2.19-cpu-sagemaker/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/tensorflow/tensorflow-training-2.19-cpu-sagemaker/","title":"AWS Deep Learning Containers for TensorFlow Training 2.19 on SageMaker","text":"<p>AWS Deep Learning Containers for SageMaker are now available with TensorFlow 2.19.</p>"},{"location":"releasenotes/tensorflow/tensorflow-training-2.19-cpu-sagemaker/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced TensorFlow 2.19 training containers for SageMaker</p> </li> <li> <p>Added Python 3.12 support</p> </li> </ul>"},{"location":"releasenotes/tensorflow/tensorflow-training-2.19-cpu-sagemaker/#core-packages","title":"Core Packages","text":"Package Version TensorFlow 2.19.0 Python 3.12.11 OpenMPI 4.1.8"},{"location":"releasenotes/tensorflow/tensorflow-training-2.19-cpu-sagemaker/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/tensorflow/tensorflow-training-2.19-cpu-sagemaker/#reference","title":"Reference","text":""},{"location":"releasenotes/tensorflow/tensorflow-training-2.19-cpu-sagemaker/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:2.19.0-cpu-py312-ubuntu22.04-sagemaker\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:2.19-cpu-py312-ubuntu22.04-sagemaker-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:2.19.0-cpu-py312-sagemaker\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:2.19-cpu-py312-sagemaker\n\npublic.ecr.aws/deep-learning-containers/tensorflow-training:2.19.0-cpu-py312-ubuntu22.04-sagemaker\n\npublic.ecr.aws/deep-learning-containers/tensorflow-training:2.19-cpu-py312-ubuntu22.04-sagemaker-v1\n\npublic.ecr.aws/deep-learning-containers/tensorflow-training:2.19.0-cpu-py312-sagemaker\n\npublic.ecr.aws/deep-learning-containers/tensorflow-training:2.19-cpu-py312-sagemaker\n</code></pre>"},{"location":"releasenotes/tensorflow/tensorflow-training-2.19-cpu-sagemaker/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/tensorflow/tensorflow-training-2.19-gpu-sagemaker/","title":"AWS Deep Learning Containers for TensorFlow Training 2.19 on SageMaker","text":"<p>AWS Deep Learning Containers for SageMaker are now available with TensorFlow 2.19.</p>"},{"location":"releasenotes/tensorflow/tensorflow-training-2.19-gpu-sagemaker/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced TensorFlow 2.19 training containers for SageMaker</p> </li> <li> <p>Added Python 3.12 support</p> </li> </ul>"},{"location":"releasenotes/tensorflow/tensorflow-training-2.19-gpu-sagemaker/#core-packages","title":"Core Packages","text":"Package Version TensorFlow 2.19.0 Python 3.12.11 CUDA 12.5.1 cuDNN 9.3.0.75 EFA 1.40.0 OpenMPI 4.1.8"},{"location":"releasenotes/tensorflow/tensorflow-training-2.19-gpu-sagemaker/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/tensorflow/tensorflow-training-2.19-gpu-sagemaker/#reference","title":"Reference","text":""},{"location":"releasenotes/tensorflow/tensorflow-training-2.19-gpu-sagemaker/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:2.19.0-gpu-py312-cu125-ubuntu22.04-sagemaker\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:2.19-gpu-py312-cu125-ubuntu22.04-sagemaker-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:2.19.0-gpu-py312-sagemaker\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:2.19-gpu-py312-sagemaker\n\npublic.ecr.aws/deep-learning-containers/tensorflow-training:2.19.0-gpu-py312-cu125-ubuntu22.04-sagemaker\n\npublic.ecr.aws/deep-learning-containers/tensorflow-training:2.19-gpu-py312-cu125-ubuntu22.04-sagemaker-v1\n\npublic.ecr.aws/deep-learning-containers/tensorflow-training:2.19.0-gpu-py312-sagemaker\n\npublic.ecr.aws/deep-learning-containers/tensorflow-training:2.19-gpu-py312-sagemaker\n</code></pre>"},{"location":"releasenotes/tensorflow/tensorflow-training-2.19-gpu-sagemaker/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/vllm/","title":"vLLM Release Notes","text":"<p>Release notes for AWS Deep Learning Containers with vLLM.</p>"},{"location":"releasenotes/vllm/#vllm-014","title":"vLLM 0.14","text":"Framework Version Accelerator Platform Link vLLM 0.14.0 GPU SageMaker Release Notes vLLM 0.14.0 GPU EC2, ECS, EKS Release Notes"},{"location":"releasenotes/vllm/#vllm-013","title":"vLLM 0.13","text":"Framework Version Accelerator Platform Link vLLM 0.13.0 GPU SageMaker Release Notes vLLM 0.13.0 GPU EC2, ECS, EKS Release Notes"},{"location":"releasenotes/vllm/#vllm-010","title":"vLLM 0.10","text":"Framework Version Accelerator Platform Link vLLM ARM64 0.10.2 GPU EC2, ECS, EKS Release Notes"},{"location":"releasenotes/vllm/vllm-0.13.0-gpu-ec2/","title":"AWS Deep Learning Containers for vLLM 0.13.0 on EC2, ECS, EKS","text":"<p>AWS Deep Learning Containers for EC2, ECS, EKS are now available with vLLM 0.13.0.</p>"},{"location":"releasenotes/vllm/vllm-0.13.0-gpu-ec2/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced vLLM 0.13.0 containers for EC2, ECS, EKS</p> </li> <li> <p>Added Python 3.12 support</p> </li> </ul>"},{"location":"releasenotes/vllm/vllm-0.13.0-gpu-ec2/#core-packages","title":"Core Packages","text":"Package Version vLLM 0.13.0 PyTorch 2.9.0 TorchVision 0.24.0 TorchAudio 2.9.0 FlashInfer 0.5.3 CUDA 12.9.1 cuDNN 9.10.2.21 NCCL 2.28.3 EFA 1.45.1 GDRCopy 2.5.1"},{"location":"releasenotes/vllm/vllm-0.13.0-gpu-ec2/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/vllm/vllm-0.13.0-gpu-ec2/#reference","title":"Reference","text":""},{"location":"releasenotes/vllm/vllm-0.13.0-gpu-ec2/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/vllm:0.13.0-gpu-py312-cu129-ubuntu22.04-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/vllm:0.13-gpu-py312-cu129-ubuntu22.04-ec2-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/vllm:0.13.0-gpu-py312-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/vllm:0.13-gpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/vllm:0.13.0-gpu-py312-cu129-ubuntu22.04-ec2\n\npublic.ecr.aws/deep-learning-containers/vllm:0.13-gpu-py312-cu129-ubuntu22.04-ec2-v1\n\npublic.ecr.aws/deep-learning-containers/vllm:0.13.0-gpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/vllm:0.13-gpu-py312-ec2\n</code></pre>"},{"location":"releasenotes/vllm/vllm-0.13.0-gpu-ec2/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/vllm/vllm-0.13.0-gpu-sagemaker/","title":"AWS Deep Learning Containers for vLLM 0.13.0 on SageMaker","text":"<p>AWS Deep Learning Containers for SageMaker are now available with vLLM 0.13.0.</p>"},{"location":"releasenotes/vllm/vllm-0.13.0-gpu-sagemaker/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced vLLM 0.13.0 containers for SageMaker</p> </li> <li> <p>Added Python 3.12 support</p> </li> </ul>"},{"location":"releasenotes/vllm/vllm-0.13.0-gpu-sagemaker/#core-packages","title":"Core Packages","text":"Package Version vLLM 0.13.0 PyTorch 2.9.0 TorchVision 0.24.0 TorchAudio 2.9.0 FlashInfer 0.5.3 CUDA 12.9.1 cuDNN 9.10.2.21 NCCL 2.28.3 EFA 1.45.1 GDRCopy 2.5.1"},{"location":"releasenotes/vllm/vllm-0.13.0-gpu-sagemaker/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/vllm/vllm-0.13.0-gpu-sagemaker/#reference","title":"Reference","text":""},{"location":"releasenotes/vllm/vllm-0.13.0-gpu-sagemaker/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/vllm:0.13.0-gpu-py312-cu129-ubuntu22.04-sagemaker\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/vllm:0.13-gpu-py312-cu129-ubuntu22.04-sagemaker-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/vllm:0.13.0-gpu-py312\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/vllm:0.13-gpu-py312\n\npublic.ecr.aws/deep-learning-containers/vllm:0.13.0-gpu-py312-cu129-ubuntu22.04-sagemaker\n\npublic.ecr.aws/deep-learning-containers/vllm:0.13-gpu-py312-cu129-ubuntu22.04-sagemaker-v1\n\npublic.ecr.aws/deep-learning-containers/vllm:0.13.0-gpu-py312\n\npublic.ecr.aws/deep-learning-containers/vllm:0.13-gpu-py312\n</code></pre>"},{"location":"releasenotes/vllm/vllm-0.13.0-gpu-sagemaker/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/vllm/vllm-0.14.0-gpu-ec2/","title":"AWS Deep Learning Containers for vLLM 0.14.0 on EC2, ECS, EKS","text":"<p>AWS Deep Learning Containers for EC2, ECS, EKS are now available with vLLM 0.14.0.</p>"},{"location":"releasenotes/vllm/vllm-0.14.0-gpu-ec2/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced vLLM 0.14.0 containers for EC2, ECS, EKS</p> </li> <li> <p>Added Python 3.12 support</p> </li> </ul>"},{"location":"releasenotes/vllm/vllm-0.14.0-gpu-ec2/#core-packages","title":"Core Packages","text":"Package Version vLLM 0.14.0 PyTorch 2.9.1 TorchVision 0.24.1 TorchAudio 2.9.1 FlashInfer 0.5.3 CUDA 12.9.1 cuDNN 9.10.2.21 NCCL 2.28.3 EFA 1.46.0 GDRCopy 2.5.1"},{"location":"releasenotes/vllm/vllm-0.14.0-gpu-ec2/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/vllm/vllm-0.14.0-gpu-ec2/#reference","title":"Reference","text":""},{"location":"releasenotes/vllm/vllm-0.14.0-gpu-ec2/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/vllm:0.14.0-gpu-py312-cu129-ubuntu22.04-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/vllm:0.14-gpu-py312-cu129-ubuntu22.04-ec2-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/vllm:0.14.0-gpu-py312-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/vllm:0.14-gpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/vllm:0.14.0-gpu-py312-cu129-ubuntu22.04-ec2\n\npublic.ecr.aws/deep-learning-containers/vllm:0.14-gpu-py312-cu129-ubuntu22.04-ec2-v1\n\npublic.ecr.aws/deep-learning-containers/vllm:0.14.0-gpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/vllm:0.14-gpu-py312-ec2\n</code></pre>"},{"location":"releasenotes/vllm/vllm-0.14.0-gpu-ec2/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/vllm/vllm-0.14.0-gpu-sagemaker/","title":"AWS Deep Learning Containers for vLLM 0.14.0 on SageMaker","text":"<p>AWS Deep Learning Containers for SageMaker are now available with vLLM 0.14.0.</p>"},{"location":"releasenotes/vllm/vllm-0.14.0-gpu-sagemaker/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced vLLM 0.14.0 containers for SageMaker</p> </li> <li> <p>Added Python 3.12 support</p> </li> </ul>"},{"location":"releasenotes/vllm/vllm-0.14.0-gpu-sagemaker/#core-packages","title":"Core Packages","text":"Package Version vLLM 0.14.0 PyTorch 2.9.1 TorchVision 0.24.1 TorchAudio 2.9.1 FlashInfer 0.5.3 CUDA 12.9.1 cuDNN 9.10.2.21 NCCL 2.28.3 EFA 1.46.0 GDRCopy 2.5.1"},{"location":"releasenotes/vllm/vllm-0.14.0-gpu-sagemaker/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/vllm/vllm-0.14.0-gpu-sagemaker/#reference","title":"Reference","text":""},{"location":"releasenotes/vllm/vllm-0.14.0-gpu-sagemaker/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/vllm:0.14.0-gpu-py312-cu129-ubuntu22.04-sagemaker\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/vllm:0.14-gpu-py312-cu129-ubuntu22.04-sagemaker-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/vllm:0.14.0-gpu-py312\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/vllm:0.14-gpu-py312\n\npublic.ecr.aws/deep-learning-containers/vllm:0.14.0-gpu-py312-cu129-ubuntu22.04-sagemaker\n\npublic.ecr.aws/deep-learning-containers/vllm:0.14-gpu-py312-cu129-ubuntu22.04-sagemaker-v1\n\npublic.ecr.aws/deep-learning-containers/vllm:0.14.0-gpu-py312\n\npublic.ecr.aws/deep-learning-containers/vllm:0.14-gpu-py312\n</code></pre>"},{"location":"releasenotes/vllm/vllm-0.14.0-gpu-sagemaker/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"releasenotes/vllm/vllm-arm64-0.10.2-gpu-ec2/","title":"AWS Deep Learning Containers for vLLM ARM64 0.10.2 on EC2, ECS, EKS","text":"<p>AWS Deep Learning Containers for EC2, ECS, EKS are now available with vLLM 0.10.2.</p>"},{"location":"releasenotes/vllm/vllm-arm64-0.10.2-gpu-ec2/#announcements","title":"Announcements","text":"<ul> <li> <p>Introduced vLLM ARM64 0.10.2 containers for EC2, ECS, EKS</p> </li> <li> <p>Added Python 3.12 support</p> </li> </ul>"},{"location":"releasenotes/vllm/vllm-arm64-0.10.2-gpu-ec2/#core-packages","title":"Core Packages","text":"Package Version vLLM 0.10.2 PyTorch 2.8.0 TorchVision 0.23.0 TorchAudio 2.8.0 CUDA 12.9.1 cuDNN 9.10.2.21 NCCL 2.27.3-1 EFA 1.43.2"},{"location":"releasenotes/vllm/vllm-arm64-0.10.2-gpu-ec2/#security-advisory","title":"Security Advisory","text":"<p>AWS recommends that customers monitor critical security updates in the AWS Security Bulletin.</p>"},{"location":"releasenotes/vllm/vllm-arm64-0.10.2-gpu-ec2/#reference","title":"Reference","text":""},{"location":"releasenotes/vllm/vllm-arm64-0.10.2-gpu-ec2/#docker-image-uris","title":"Docker Image URIs","text":"<pre><code>763104351884.dkr.ecr.us-west-2.amazonaws.com/vllm-arm64:0.10.2-gpu-py312-cu129-ubuntu22.04-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/vllm-arm64:0.10-gpu-py312-cu129-ubuntu22.04-ec2-v1\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/vllm-arm64:0.10.2-gpu-py312-ec2\n\n763104351884.dkr.ecr.us-west-2.amazonaws.com/vllm-arm64:0.10-gpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/vllm-arm64:0.10.2-gpu-py312-cu129-ubuntu22.04-ec2\n\npublic.ecr.aws/deep-learning-containers/vllm-arm64:0.10-gpu-py312-cu129-ubuntu22.04-ec2-v1\n\npublic.ecr.aws/deep-learning-containers/vllm-arm64:0.10.2-gpu-py312-ec2\n\npublic.ecr.aws/deep-learning-containers/vllm-arm64:0.10-gpu-py312-ec2\n</code></pre>"},{"location":"releasenotes/vllm/vllm-arm64-0.10.2-gpu-ec2/#quick-links","title":"Quick Links","text":"<ul> <li>Available Images</li> <li>Support Policy</li> <li>GitHub Repository</li> </ul>"},{"location":"security/","title":"Security in AWS Deep Learning Containers","text":"<p>Cloud security at AWS is the highest priority. As an AWS customer, you benefit from a data center and network architecture that is built to meet the requirements of the most security-sensitive organizations.</p> <p>Security is a shared responsibility between AWS and you. The shared responsibility model describes this as security of the cloud and security in the cloud:</p> <ul> <li>Security of the cloud \u2013 AWS is responsible for protecting the infrastructure that runs AWS services in the AWS Cloud.   AWS also provides you with services that you can use securely. Third-party auditors regularly test and verify the effectiveness of our   security as part of the AWS Compliance Programs. To learn about the compliance programs that   apply to Deep Learning Containers, see AWS Services in Scope by Compliance Program.</li> <li>Security in the cloud \u2013 Your responsibility is determined by the AWS service that you use. You are also responsible for other factors   including the sensitivity of your data, your company's requirements, and applicable laws and regulations.</li> </ul> <p>This documentation helps you understand how to apply the shared responsibility model when using Deep Learning Containers. The following topics show you how to configure Deep Learning Containers to meet your security and compliance objectives. You also learn how to use other AWS services that help you to monitor and secure your Deep Learning Containers resources.</p> <p>For more information, see Security in Amazon EC2, Security in Amazon ECS, Security in Amazon EKS, and Security in Amazon SageMaker AI.</p>"},{"location":"security/compliance/","title":"Compliance Validation for AWS Deep Learning Containers","text":"<p>Third-party auditors assess the security and compliance of services as part of multiple AWS compliance programs. For information on the supported compliance programs, see Compliance Validation for Amazon EC2, Compliance Validation for Amazon ECS, Compliance Validation for Amazon EKS, and Compliance Validation for Amazon SageMaker AI.</p> <p>For a list of AWS services in scope of specific compliance programs, see AWS Services in Scope by Compliance Program. For general information, see AWS Compliance Programs.</p> <p>You can download third-party audit reports using AWS Artifact. For more information, see Downloading Reports in AWS Artifact.</p> <p>Your compliance responsibility when using Deep Learning Containers is determined by the sensitivity of your data, your company's compliance objectives, and applicable laws and regulations. AWS provides the following resources to help with compliance:</p> <ul> <li>Security and Compliance Quick Start Guides   \u2013 These deployment guides discuss architectural considerations and provide steps for deploying security- and compliance-focused baseline   environments on AWS.</li> <li>AWS Compliance Resources \u2013 This collection of workbooks and guides might apply to your   industry and location.</li> <li>Evaluating Resources with Rules in the AWS Config Developer   Guide \u2013 The AWS Config service assesses how well your resource configurations comply with internal practices, industry guidelines, and   regulations.</li> <li>AWS Security Hub CSPM \u2013 This AWS service provides a   comprehensive view of your security state within AWS that helps you check your compliance with security industry standards and best practices.</li> </ul>"},{"location":"security/data-protection/","title":"Data Protection in AWS Deep Learning Containers","text":"<p>The AWS shared responsibility model applies to data protection in AWS Deep Learning Containers. As described in this model, AWS is responsible for protecting the global infrastructure that runs all of the AWS Cloud. You are responsible for maintaining control over your content that is hosted on this infrastructure. You are also responsible for the security configuration and management tasks for the AWS services that you use. For more information about data privacy, see the Data Privacy FAQ. For information about data protection in Europe, see the AWS Shared Responsibility Model and GDPR blog post on the AWS Security Blog.</p> <p>For data protection purposes, we recommend that you protect AWS account credentials and set up individual users with AWS IAM Identity Center or AWS Identity and Access Management (IAM). That way, each user is given only the permissions necessary to fulfill their job duties. We also recommend that you secure your data in the following ways:</p> <ul> <li>Use multi-factor authentication (MFA) with each account.</li> <li>Use SSL/TLS to communicate with AWS resources. We require TLS 1.2 and recommend TLS 1.3.</li> <li>Set up API and user activity logging with AWS CloudTrail. For information about using CloudTrail trails to capture AWS activities, see   Working with CloudTrail trails in the AWS CloudTrail   User Guide.</li> <li>Use AWS encryption solutions, along with all default security controls within AWS services.</li> <li>Use advanced managed security services such as Amazon Macie, which assists in discovering and securing sensitive data that is stored in Amazon S3.</li> <li>If you require FIPS 140-3 validated cryptographic modules when accessing AWS through a command line interface or an API, use a FIPS endpoint.   For more information about the available FIPS endpoints, see   Federal Information Processing Standard (FIPS) 140-3.</li> </ul> <p>We strongly recommend that you never put confidential or sensitive information, such as your customers' email addresses, into tags or free-form text fields such as a Name field. This includes when you work with Deep Learning Containers or other AWS services using the console, API, AWS CLI, or AWS SDKs. Any data that you enter into tags or free-form text fields used for names may be used for billing or diagnostic logs. If you provide a URL to an external server, we strongly recommend that you do not include credentials information in the URL to validate your request to that server.</p>"},{"location":"security/identity-and-access-management/","title":"Identity and Access Management in AWS Deep Learning Containers","text":"<p>AWS Identity and Access Management (IAM) is an AWS service that helps an administrator securely control access to AWS resources. IAM administrators control who can be authenticated (signed in) and authorized (have permissions) to use Deep Learning Containers resources. IAM is an AWS service that you can use with no additional charge.</p> <p>For more information on Identity and Access Management, see Identity and Access Management for Amazon EC2, Identity and Access Management for Amazon ECS, Identity and Access Management for Amazon EKS, and Identity and Access Management for Amazon SageMaker AI.</p>"},{"location":"security/identity-and-access-management/#authenticating-with-identities","title":"Authenticating With Identities","text":"<p>Authentication is how you sign in to AWS using your identity credentials. You must be authenticated as the AWS account root user, an IAM user, or by assuming an IAM role.</p> <p>You can sign in as a federated identity using credentials from an identity source like AWS IAM Identity Center (IAM Identity Center), single sign-on authentication, or Google/Facebook credentials. For more information about signing in, see How to sign in to your AWS account in the AWS Sign-In User Guide.</p> <p>For programmatic access, AWS provides an SDK and CLI to cryptographically sign requests. For more information, see AWS Signature Version 4 for API requests in the IAM User Guide.</p>"},{"location":"security/identity-and-access-management/#aws-account-root-user","title":"AWS account root user","text":"<p>When you create an AWS account, you begin with one sign-in identity called the AWS account root user that has complete access to all AWS services and resources. We strongly recommend that you don't use the root user for everyday tasks. For tasks that require root user credentials, see Tasks that require root user credentials in the IAM User Guide.</p>"},{"location":"security/identity-and-access-management/#iam-users-and-groups","title":"IAM Users and Groups","text":"<p>An IAM user is an identity with specific permissions for a single person or application. We recommend using temporary credentials instead of IAM users with long-term credentials. For more information, see Require human users to use federation with an identity provider to access AWS using temporary credentials in the IAM User Guide.</p> <p>An IAM group specifies a collection of IAM users and makes permissions easier to manage for large sets of users. For more information, see Use cases for IAM users in the IAM User Guide.</p>"},{"location":"security/identity-and-access-management/#iam-roles","title":"IAM Roles","text":"<p>An IAM role is an identity with specific permissions that provides temporary credentials. You can assume a role by switching from a user to an IAM role (console) or by calling an AWS CLI or AWS API operation. For more information, see Methods to assume a role in the IAM User Guide.</p> <p>IAM roles are useful for federated user access, temporary IAM user permissions, cross-account access, cross-service access, and applications running on Amazon EC2. For more information, see Cross account resource access in IAM in the IAM User Guide.</p>"},{"location":"security/identity-and-access-management/#managing-access-using-policies","title":"Managing Access Using Policies","text":"<p>You control access in AWS by creating policies and attaching them to AWS identities or resources. A policy defines permissions when associated with an identity or resource. AWS evaluates these policies when a principal makes a request. Most policies are stored in AWS as JSON documents. For more information about JSON policy documents, see Overview of JSON policies in the IAM User Guide.</p> <p>Using policies, administrators specify who has access to what by defining which principal can perform actions on what resources, and under what conditions.</p> <p>By default, users and roles have no permissions. An IAM administrator creates IAM policies and adds them to roles, which users can then assume. IAM policies define permissions regardless of the method used to perform the operation.</p>"},{"location":"security/identity-and-access-management/#identity-based-policies","title":"Identity-Based Policies","text":"<p>Identity-based policies are JSON permissions policy documents that you attach to an identity (user, group, or role). These policies control what actions identities can perform, on which resources, and under what conditions. To learn how to create an identity-based policy, see Define custom IAM permissions with customer managed policies in the IAM User Guide.</p> <p>Identity-based policies can be inline policies (embedded directly into a single identity) or managed policies (standalone policies attached to multiple identities). To learn how to choose between managed and inline policies, see Choose between managed policies and inline policies in the IAM User Guide.</p>"},{"location":"security/identity-and-access-management/#resource-based-policies","title":"Resource-Based Policies","text":"<p>Resource-based policies are JSON policy documents that you attach to a resource. Examples include IAM role trust policies and Amazon S3 bucket policies. In services that support resource-based policies, service administrators can use them to control access to a specific resource. You must specify a principal in a resource-based policy.</p> <p>Resource-based policies are inline policies that are located in that service. You can't use AWS managed policies from IAM in a resource-based policy.</p>"},{"location":"security/identity-and-access-management/#access-control-lists-acls","title":"Access Control Lists (ACLs)","text":"<p>Access control lists (ACLs) control which principals (account members, users, or roles) have permissions to access a resource. ACLs are similar to resource-based policies, although they do not use the JSON policy document format.</p> <p>Amazon S3, AWS WAF, and Amazon VPC are examples of services that support ACLs. To learn more about ACLs, see Access control list (ACL) overview in the Amazon Simple Storage Service Developer Guide.</p>"},{"location":"security/identity-and-access-management/#other-policy-types","title":"Other Policy Types","text":"<p>AWS supports additional policy types that can set the maximum permissions granted by more common policy types:</p> <ul> <li>Permissions boundaries \u2013 Set the maximum permissions that an identity-based policy can grant to an IAM entity. For more information, see   Permissions boundaries for IAM entities in the IAM User Guide.</li> <li>Service control policies (SCPs) \u2013 Specify the maximum permissions for an organization or organizational unit in AWS Organizations. For   more information, see Service control policies in the   AWS Organizations User Guide.</li> <li>Resource control policies (RCPs) \u2013 Set the maximum available permissions for resources in your accounts. For more information, see   Resource control policies (RCPs) in the AWS   Organizations User Guide.</li> <li>Session policies \u2013 Advanced policies passed as a parameter when creating a temporary session for a role or federated user. For more information,   see Session policies in the IAM User Guide.</li> </ul>"},{"location":"security/identity-and-access-management/#multiple-policy-types","title":"Multiple Policy Types","text":"<p>When multiple types of policies apply to a request, the resulting permissions are more complicated to understand. To learn how AWS determines whether to allow a request when multiple policy types are involved, see Policy evaluation logic in the IAM User Guide.</p>"},{"location":"security/identity-and-access-management/#iam-with-amazon-emr","title":"IAM with Amazon EMR","text":"<p>You can use AWS Identity and Access Management with Amazon EMR to define users, AWS resources, groups, roles, and policies. You can also control which AWS services these users and roles can access.</p> <p>For more information on using IAM with Amazon EMR, see AWS Identity and Access Management for Amazon EMR.</p>"},{"location":"security/infrastructure-security/","title":"Infrastructure Security in AWS Deep Learning Containers","text":"<p>The infrastructure security of AWS Deep Learning Containers is backed by Amazon EC2, Amazon ECS, Amazon EKS, or Amazon SageMaker AI. For more information, see Infrastructure Security in Amazon EC2, Infrastructure Security in Amazon ECS, Infrastructure Security in Amazon EKS, and Infrastructure Security in Amazon SageMaker AI.</p>"},{"location":"security/logging-and-monitoring/","title":"Monitoring and Usage Tracking in AWS Deep Learning Containers","text":"<p>Your AWS Deep Learning Containers do not come with monitoring utilities. For information on monitoring, see GPU Monitoring and Optimization, Monitoring Amazon EC2, Monitoring Amazon ECS, Monitoring Amazon EKS, and Monitoring Amazon SageMaker AI Studio.</p>"},{"location":"security/logging-and-monitoring/#usage-tracking","title":"Usage Tracking","text":"<p>AWS uses customer feedback and usage information to improve the quality of the services and software we offer to customers. We have added usage data collection to the supported AWS Deep Learning Containers in order to better understand customer usage and guide future improvements. Usage tracking for Deep Learning Containers is activated by default. Customers can change their settings at any point of time to activate or deactivate usage tracking.</p> <p>Usage tracking for AWS Deep Learning Containers collects the instance ID, frameworks, framework versions, container types, and Python versions used for the containers. AWS also logs the event time in which it receives this metadata.</p> <p>No information on the commands used within the containers is collected or retained. No other information about the containers is collected or retained.</p> <p>To opt out of usage tracking, set the <code>OPT_OUT_TRACKING</code> environment variable to true.</p> <pre><code>OPT_OUT_TRACKING=true\n</code></pre>"},{"location":"security/logging-and-monitoring/#failure-rate-tracking","title":"Failure Rate Tracking","text":"<p>When using a first-party AWS Deep Learning Containers container, the SageMaker AI team will collect failure rate metadata to improve the quality of AWS Deep Learning Containers. Failure rate tracking for AWS Deep Learning Containers is active by default. Customers can change their settings to activate or deactivate failure rate tracking when creating an Amazon SageMaker AI endpoint.</p> <p>Failure rate tracking for AWS Deep Learning Containers collects the Instance ID, ModelServer name, ModelServer version, ErrorType, and ErrorCode. AWS also logs the event time in which it receives this metadata.</p> <p>No information on the commands used within the containers is collected or retained. No other information about the containers is collected or retained.</p> <p>To opt out of failure rate tracking, set the <code>OPT_OUT_TRACKING</code> environment variable to <code>true</code>.</p> <pre><code>OPT_OUT_TRACKING=true\n</code></pre>"},{"location":"security/logging-and-monitoring/#usage-tracking-in-the-following-framework-versions","title":"Usage Tracking in the following Framework Versions","text":"<p>While we recommend updating to supported Deep Learning Containers, to opt-out of usage tracking for Deep Learning Containers that use these frameworks, set the <code>OPT_OUT_TRACKING</code> environment variable to true and use a custom entry point to disable the call for the following services:</p> <ul> <li>Amazon EC2 Custom Entrypoints</li> <li>Amazon ECS Custom Entrypoints</li> <li>Amazon EKS Custom Entrypoints</li> </ul>"},{"location":"security/resilience/","title":"Resilience in AWS Deep Learning Containers","text":"<p>The AWS global infrastructure is built around AWS Regions and Availability Zones. AWS Regions provide multiple physically separated and isolated Availability Zones, which are connected with low-latency, high-throughput, and highly redundant networking. With Availability Zones, you can design and operate applications and databases that automatically fail over between zones without interruption. Availability Zones are more highly available, fault tolerant, and scalable than traditional single or multiple data center infrastructures.</p> <p>For more information about AWS Regions and Availability Zones, see AWS Global Infrastructure.</p> <p>For information on features to help support your data resiliency and backup needs, see Resilience in Amazon EC2, Resilience in Amazon EKS, and Resilience in Amazon SageMaker AI.</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Step-by-step guides for using AWS Deep Learning Containers.</p>"},{"location":"tutorials/#training","title":"Training","text":"<ul> <li>EKS Training - Train models on Amazon EKS with PyTorch FSDP</li> </ul>"},{"location":"tutorials/#inference","title":"Inference","text":"<ul> <li>vLLM on SageMaker - Deploy vLLM on SageMaker endpoints</li> <li>DeepSeek on EKS - Deploy DeepSeek models with vLLM on EKS</li> <li>Fraud Detection Demo - End-to-end fraud detection with DeepSeek</li> </ul>"},{"location":"tutorials/#integrations","title":"Integrations","text":"<ul> <li>MLflow - Use MLflow with Deep Learning Containers</li> <li>SOCI - Seekable OCI for faster container startup</li> </ul>"},{"location":"tutorials/CODE_OF_CONDUCT/","title":"CODE OF CONDUCT","text":""},{"location":"tutorials/CODE_OF_CONDUCT/#code-of-conduct","title":"Code of Conduct","text":"<p>This project has adopted the Amazon Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments.</p>"},{"location":"tutorials/CONTRIBUTING/","title":"Contributing Guidelines","text":"<p>Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community.</p> <p>Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.</p>"},{"location":"tutorials/CONTRIBUTING/#reporting-bugsfeature-requests","title":"Reporting Bugs/Feature Requests","text":"<p>We welcome you to use the GitHub issue tracker to report bugs or suggest features.</p> <p>When filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful:</p> <ul> <li>A reproducible test case or series of steps</li> <li>The version of our code being used</li> <li>Any modifications you've made relevant to the bug</li> <li>Anything unusual about your environment or deployment</li> </ul>"},{"location":"tutorials/CONTRIBUTING/#contributing-via-pull-requests","title":"Contributing via Pull Requests","text":"<p>Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that:</p> <ol> <li>You are working against the latest source on the main branch.</li> <li>You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already.</li> <li>You open an issue to discuss any significant work - we would hate for your time to be wasted.</li> </ol> <p>To send us a pull request, please:</p> <ol> <li>Fork the repository.</li> <li>Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change.</li> <li>Ensure local tests pass.</li> <li>Commit to your fork using clear commit messages.</li> <li>Send us a pull request, answering any default questions in the pull request interface.</li> <li>Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation.</li> </ol> <p>GitHub provides additional document on forking a repository and creating a pull request.</p>"},{"location":"tutorials/CONTRIBUTING/#finding-contributions-to-work-on","title":"Finding contributions to work on","text":"<p>Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start.</p>"},{"location":"tutorials/CONTRIBUTING/#code-of-conduct","title":"Code of Conduct","text":"<p>This project has adopted the Amazon Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments.</p>"},{"location":"tutorials/CONTRIBUTING/#security-issue-notifications","title":"Security issue notifications","text":"<p>If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page. Please do not create a public github issue.</p>"},{"location":"tutorials/CONTRIBUTING/#licensing","title":"Licensing","text":"<p>See the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution.</p>"},{"location":"tutorials/SOCI/","title":"SOCI Index for AWS Deep Learning Containers","text":""},{"location":"tutorials/SOCI/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Background on SOCI Index</li> <li>Why SOCI Matters: With vs Without Comparison</li> <li>SOCI Index Components</li> <li>SOCI Index Manifest Version</li> <li>DLC SOCI Supported Frameworks</li> <li>Sample URIs</li> <li>DLC SOCI Tag Introduction</li> <li>Supported Versions</li> <li>Getting Started with SOCI Index</li> <li>Environment</li> <li>Common Setup: Install and Configure SOCI Snapshotter<ul> <li>Prerequisites</li> <li>Install the SOCI Snapshotter</li> <li>Configure SOCI Snapshotter</li> </ul> </li> <li>Choose Your Container Client<ul> <li>Option 1: Using Finch (Recommended for Ease of Use)</li> <li>Option 2: Using nerdctl (Advanced Use Cases)</li> </ul> </li> <li>Performance Comparison: SOCI vs Docker</li> <li>References</li> </ul>"},{"location":"tutorials/SOCI/#background-on-soci-index","title":"Background on SOCI Index","text":"<p>The Software Component Index (SOCI) is a technology that enables efficient container image management through selective file downloading. It uses a layer-based indexing system to map file locations within container images, allowing containers to start with only the necessary files loaded (lazy loading). This approach significantly reduces network bandwidth usage and improves container startup times, making it particularly valuable for organizations managing large container images in cloud environments.</p>"},{"location":"tutorials/SOCI/#why-soci-matters-with-vs-without-comparison","title":"Why SOCI Matters: With vs Without Comparison","text":"<p>Without SOCI (Traditional Approach): - Must download the ENTIRE container image before starting (e.g., 17.6GB image takes ~4m26s) - Container can only start after all layers are downloaded - Total time to run: ~4m26s + startup overhead</p> <p>With SOCI (Lazy Loading): - Only downloads the SOCI index and metadata (~37.9KB takes ~8.9s) - Container starts immediately; additional data fetched on-demand as needed - Total time to run: ~8.9s + startup overhead</p> <p>Key Benefits: - ~97% faster initial pull time (4m26s \u2192 8.9s) - ~99.8% less data downloaded upfront (17.6GB \u2192 ~37.9KB) - Immediate container startup - Ideal for large images, rapid scaling, and bandwidth-constrained environments</p> <p>See the Performance Comparison section for detailed benchmarks.</p>"},{"location":"tutorials/SOCI/#soci-index-components","title":"SOCI Index Components","text":"<p>A SOCI index, which enables lazy loading of container images, consists of two key components:</p> <ol> <li>SOCI Index Manifest: Lists zTOCs and references the target image</li> <li>zTOCs (Zone Table of Contents): Comprising:</li> <li>TOC: A table of contents containing file metadata and corresponding offset information in the decompressed TAR archive</li> <li>zInfo: A collection of checkpoints representing compression engine states at various points in the layer</li> </ol>"},{"location":"tutorials/SOCI/#soci-index-manifest-version","title":"SOCI Index Manifest Version","text":"<p>DLC uses SOCI Index Manifest v2. SOCI Index Manifest v1 and SOCI Index Manifest v2 both allow lazily loading container images. SOCI Index Manifest v1 is a standalone artifact that can be discovered using the OCI Referrers API. SOCI Index Manifest v2 uses SOCI-enabled images where the SOCI index is bundled with the original image into a single, multi-architecture image. The SOCI snapshotter v0.10.0 will no longer consume SOCI Index Manifest v1 by default. For detailed differences between v1 and v2, please refer to SOCI Index Manifest v2 Documentation.</p>"},{"location":"tutorials/SOCI/#dlc-soci-supported-frameworks","title":"DLC SOCI Supported Frameworks","text":"<p>Deep Learning Containers (DLC) provides SOCI index for the following images both in private registry and public gallery: PyTorch Training (x86, arm64), Tensorflow Training, vLLM (x86, arm64), and SGLang.</p>"},{"location":"tutorials/SOCI/#sample-uris","title":"Sample URIs","text":"<ul> <li> <p>ECR Private Registry (requires AWS account to login):   <pre><code>763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:2.9-cpu-py312-ubuntu22.04-sagemaker-v1-soci\n</code></pre></p> </li> <li> <p>ECR Public Gallery (no login required):   <pre><code>public.ecr.aws/deep-learning-containers/pytorch-training:2.9-cpu-py312-ubuntu22.04-sagemaker-v1-soci\n</code></pre></p> </li> </ul>"},{"location":"tutorials/SOCI/#dlc-soci-tag-introduction","title":"DLC SOCI Tag Introduction","text":"<p>For different tracking purposes, DLC images have several tags for one single image. Taking PyTorch training image as an example, it has six tags. Tags without version or with only major version will be overridden by each patch release and always point to the latest release image, while tags with minor version and timestamp will only attach to one specific image.</p> <p>Tags attached to specific image: - <code>2.9.0-gpu-py312-cu130-ubuntu22.04-ec2-v1.2-2025-12-15-22-03-41</code> - <code>2.9.0-gpu-py312-cu130-ubuntu22.04-ec2-v1.2</code></p> <p>Tags always attached to latest image: - <code>2.9-gpu-py312-ec2</code> - <code>2.9-gpu-py312-cu130-ubuntu22.04-ec2-v1</code> - <code>2.9.0-gpu-py312-ec2</code> - <code>2.9.0-gpu-py312-cu130-ubuntu22.04-ec2</code></p> <p>Usage Guidelines: - To use a specific image, use tags with timestamp and minor version - To always get the latest image with CVE patches, use tags without timestamp</p> <p>SOCI-Enabled Tags:</p> <p>For SOCI index enabled images, DLC adds <code>-soci</code> as a tag suffix. For the above image examples, the SOCI tags are:</p> <p>Tags attached to specific image: - <code>2.9.0-gpu-py312-cu130-ubuntu22.04-ec2-v1.2-2025-12-15-22-03-41-soci</code> - <code>2.9.0-gpu-py312-cu130-ubuntu22.04-ec2-v1.2-soci</code></p> <p>Tags always attached to latest image: - <code>2.9-gpu-py312-ec2-soci</code> - <code>2.9-gpu-py312-cu130-ubuntu22.04-ec2-v1-soci</code> - <code>2.9.0-gpu-py312-ec2-soci</code> - <code>2.9.0-gpu-py312-cu130-ubuntu22.04-ec2-soci</code></p>"},{"location":"tutorials/SOCI/#supported-versions","title":"Supported Versions","text":"<p>For the latest supported version releases, please refer to DLC public gallery to check the tags.</p> <ul> <li>Pytorch Training</li> <li>Pytorch Training arm64</li> <li>Tensorflow Training</li> <li>vLLM</li> <li>vLLM arm64</li> <li>SGLang</li> </ul>"},{"location":"tutorials/SOCI/#getting-started-with-soci-index","title":"Getting Started with SOCI Index","text":""},{"location":"tutorials/SOCI/#environment","title":"Environment","text":"<ul> <li>Instance Type: c5.24xlarge</li> <li>AMI: Deep Learning Base OSS Nvidia Driver GPU AMI (Amazon Linux 2023) </li> <li>At the time of writing, we are using AMI date 20250701</li> </ul>"},{"location":"tutorials/SOCI/#common-setup-install-and-configure-soci-snapshotter","title":"Common Setup: Install and Configure SOCI Snapshotter","text":"<p>Both Finch and nerdctl use the same SOCI snapshotter. Follow these steps to set it up:</p>"},{"location":"tutorials/SOCI/#prerequisites","title":"Prerequisites","text":"<p>1. containerd (&gt;= 1.4)</p> <p>Required to run the SOCI snapshotter. To verify your containerd version:</p> <pre><code>containerd --version\n# Expected output:\n# containerd github.com/containerd/containerd/v2 2.0.5 fb4c30d4ede3531652d86197bf3fc9515e5276d9\n</code></pre> <p>2. fuse</p> <p>Used for mounting without root access.</p> <pre><code>sudo yum install fuse\n</code></pre>"},{"location":"tutorials/SOCI/#install-the-soci-snapshotter","title":"Install the SOCI Snapshotter","text":"<p>The SOCI project produces 2 binaries: - soci: The CLI tool used to build/manage SOCI indices - soci-snapshotter-grpc: The daemon (a containerd snapshotter plugin) used for lazy loading</p> <p>Note: Each soci-snapshotter binary is compiled with a specific glibc version. Check your system glibc version for compatibility.</p> <p>To check glibc version:</p> <pre><code>ldd --version\n# Expected output:\n# ldd (GNU libc) 2.34\n# Copyright (C) 2021 Free Software Foundation, Inc.\n# This is free software; see the source for copying conditions. There is NO\n# warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n# Written by Roland McGrath and Ulrich Drepper.\n</code></pre> <p>To install soci-snapshotter:</p> <pre><code>version=\"0.12.1\"\nwget https://github.com/awslabs/soci-snapshotter/releases/download/v${version}/soci-snapshotter-${version}-linux-amd64.tar.gz\nsudo tar -C /usr/local/bin -xvf soci-snapshotter-${version}-linux-amd64.tar.gz soci soci-snapshotter-grpc\n\n# Verify installation\nsoci --version\n# Expected output:\n# soci version v0.12.1 c140af2f22fffade79af74ddcc1c29388d763051\n</code></pre>"},{"location":"tutorials/SOCI/#configure-soci-snapshotter","title":"Configure SOCI Snapshotter","text":"<p>Step 1: Start Snapshotter in Background</p> <pre><code>sudo mkdir -p /run/soci-snapshotter-grpc\n\n# Start the SOCI snapshotter in background\nsudo /usr/local/bin/soci-snapshotter-grpc &amp;\n</code></pre> <p>Expected output:</p> <pre><code>{\"level\":\"info\",\"msg\":\"starting soci-snapshotter-grpc\",\"revision\":\"c140af2f22fffade79af74ddcc1c29388d763051\",\"time\":\"2025-12-16T23:54:48.995428161Z\",\"version\":\"v0.12.1\"}\n{\"emitMetricPeriod\":10000000000,\"fetchPeriod\":500000000,\"level\":\"info\",\"maxQueueSize\":100,\"msg\":\"constructing background fetcher\",\"silencePeriod\":30000000000,\"time\":\"2025-12-16T23:54:49.007876736Z\"}\n{\"address\":\"/run/soci-snapshotter-grpc/soci-snapshotter-grpc.sock\",\"level\":\"info\",\"msg\":\"soci-snapshotter-grpc successfully started\",\"time\":\"2025-12-16T23:54:49.011089880Z\"}\n</code></pre> <p>Verify the snapshotter is running:</p> <pre><code>ps aux | grep soci-snapshotter\n</code></pre> <p>Expected output:</p> <pre><code>root       12809  0.0  0.0 234584  8084 pts/1    S    23:54   0:00 sudo /usr/local/bin/soci-snapshotter-grpc\nroot       12812  0.0  0.0 234584  2516 pts/2    Ss+  23:54   0:00 sudo /usr/local/bin/soci-snapshotter-grpc\nroot       12813  0.0  0.0 2317432 42584 pts/2   Sl   23:54   0:00 /usr/local/bin/soci-snapshotter-grpc\nec2-user   12875  0.0  0.0 222324  2124 pts/1    S+   23:55   0:00 grep --color=auto soci-snapshotter\n</code></pre> <p>Step 2: Configure containerd to Use SOCI Plugin</p> <p>Modify <code>/etc/containerd/config.toml</code>:</p> <pre><code>sudo vim /etc/containerd/config.toml\n</code></pre> <p>Ensure the config includes the following section:</p> <pre><code>[proxy_plugins]\n  [proxy_plugins.soci]\n    type = \"snapshot\"\n    address = \"/run/soci-snapshotter-grpc/soci-snapshotter-grpc.sock\"\n</code></pre>"},{"location":"tutorials/SOCI/#choose-your-container-client","title":"Choose Your Container Client","text":"<p>After setting up the SOCI snapshotter, choose one of the following container clients:</p>"},{"location":"tutorials/SOCI/#option-1-using-finch-recommended-for-ease-of-use","title":"Option 1: Using Finch (Recommended for Ease of Use)","text":"<p>Finch is an open source client for container development that provides a simpler, streamlined experience. Finch uses the same SOCI snapshotter configured above.</p> <p>Advantages of Finch: - Simpler command-line interface - Single package installation via dnf - CNI plugins pre-configured - Unlike nerdctl, Finch includes built-in network configuration, so you don't need to use <code>--network host</code> or manually configure CNI plugins - Ideal for users who want a streamlined experience</p>"},{"location":"tutorials/SOCI/#install-finch","title":"Install Finch","text":"<p>Finch is packaged in the standard Amazon Linux repositories, making installation straightforward:</p> <pre><code>sudo dnf install runfinch-finch\n</code></pre> <p>Verify Finch is installed correctly:</p> <pre><code>sudo finch run public.ecr.aws/finch/hello-finch:latest\n</code></pre> <p>Expected output:</p> <pre><code>.....                                      \n\n                            @@@@@@@@@@@@@@@@@@@                                 \n                        @@@@@@@@@@@@    @@@@@@@@@@@                             \n                      @@@@@@@                  @@@@@@@                          \n                    @@@@@@                        @@@@@@                        \n                  @@@@@@                            @@@@@                       \n                 @@@@@                      @@@#     @@@@@@@@@                  \n                @@@@@                     @@   @@@       @@@@@@@@@@             \n                @@@@%                     @     @@            @@@@@@@@@@@       \n                @@@@                                               @@@@@@@@     \n                @@@@                                         @@@@@@@@@@@&amp;       \n                @@@@@                                  &amp;@@@@@@@@@@@             \n                 @@@@@                               @@@@@@@@                   \n                  @@@@@                            @@@@@(                       \n                   @@@@@@                        @@@@@@                         \n                     @@@@@@@                  @@@@@@@                           \n                        @@@@@@@@@@@@@@@@@@@@@@@@@@                              \n                            @@@@@@@@@@@@@@@@@@\n\n\nHello from Finch!\n\nVisit us @ github.com/runfinch\n</code></pre>"},{"location":"tutorials/SOCI/#pull-soci-images-with-finch","title":"Pull SOCI Images with Finch","text":"<p>Pull a SOCI-enabled DLC image using Finch:</p> <pre><code>sudo finch pull --snapshotter soci public.ecr.aws/deep-learning-containers/pytorch-training:2.8-gpu-py312-cu129-ubuntu22.04-ec2-v1-soci\n</code></pre> <p>Expected output:</p> <pre><code>config-sha256:a4f4e7578325b0651818c5605b87f9eef2ba5a54dcfa896dabe448a6c4c468be:    exists         |++++++++++++++++++++++++++++++++++++++| \nelapsed: 12.3s\n</code></pre>"},{"location":"tutorials/SOCI/#run-containers-with-finch","title":"Run Containers with Finch","text":"<p>Run an interactive container with SOCI lazy loading:</p> <pre><code>sudo finch run --snapshotter soci -it --rm public.ecr.aws/deep-learning-containers/pytorch-training:2.8-gpu-py312-cu129-ubuntu22.04-ec2-v1-soci /bin/bash\n</code></pre> <p>Note: You must use the <code>--snapshotter soci</code> flag with Finch to enable SOCI lazy loading. Without this flag, Finch will download the entire image.</p> <p>For more information about Finch, visit the Finch documentation.</p>"},{"location":"tutorials/SOCI/#option-2-using-nerdctl-advanced-use-cases","title":"Option 2: Using nerdctl (Advanced Use Cases)","text":"<p>Provides more control and flexibility for advanced use cases.</p> <p>Important: nerdctl requires CNI (Container Network Interface) plugin configuration for container networking. Without CNI configuration, you must use the <code>--network host</code> flag when running containers. All examples in this guide use <code>--network host</code> for simplicity.</p>"},{"location":"tutorials/SOCI/#install-nerdctl","title":"Install nerdctl","text":"<p>nerdctl (&gt;= v1.6.0) is required to interact with containerd/registry. Note: SOCI will not work with rootless nerdctl.</p> <p>To install nerdctl:</p> <pre><code>wget https://github.com/containerd/nerdctl/releases/download/v2.1.5/nerdctl-2.1.5-linux-amd64.tar.gz\nsudo tar -C /usr/local/bin -xzf nerdctl-2.1.5-linux-amd64.tar.gz\nsudo chmod +x /usr/local/bin/nerdctl\n\n# Verify installation\nnerdctl --version\n# Expected output:\n# nerdctl version 2.1.5\n</code></pre>"},{"location":"tutorials/SOCI/#verify-nerdctl-is-using-soci","title":"Verify nerdctl is Using SOCI","text":"<pre><code>sudo nerdctl system info\n</code></pre> <p>Look for <code>soci</code> in the Storage plugins:</p> <pre><code>Client:\n Namespace:     default\n Debug Mode:    false\n\nServer:\n Server Version: 2.0.5\n Storage Driver: overlayfs\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Log:     fluentd journald json-file none syslog\n  Storage: native overlayfs soci\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version:   6.1.141-155.222.amzn2023.x86_64\n Operating System: Amazon Linux 2023.7.20250623\n OSType:           linux\n Architecture:     x86_64\n CPUs:             96\n Total Memory:     184.9GiB\n Name:             ip-172-31-53-55.us-west-2.compute.internal\n ID:               cafd1e17-3c20-4557-af0d-425ea0ef25bc\n</code></pre>"},{"location":"tutorials/SOCI/#pull-soci-images-with-nerdctl","title":"Pull SOCI Images with nerdctl","text":"<pre><code>sudo nerdctl pull --snapshotter soci public.ecr.aws/deep-learning-containers/pytorch-training:2.9-gpu-py312-cu130-ubuntu22.04-ec2-v1-soci\n</code></pre> <p>Output:</p> <pre><code>public.ecr.aws/deep-learning-containers/pytorch-training:2.9-gpu-py312-cu130-ubuntu22.04-ec2-v1-soci: resolved       |++++++++++++++++++++++++++++++++++++++| \nindex-sha256:fbacb61760f570e2f8df6eacd65a6285c8699405566fdb73711390e9d81db2b8:    done           |++++++++++++++++++++++++++++++++++++++| \nmanifest-sha256:ca3e4389bb850cf3f43128b98897ac4522e20f354452a41139cfe7f45431da22:     done           |++++++++++++++++++++++++++++++++++++++| \nconfig-sha256:d007250deb6300c72286af12cf0510cf69b0c7227e4b4d50783a03ac8d917275:       done           |++++++++++++++++++++++++++++++++++++++| \nelapsed: 8.9 s     total:  37.9 K (4.3 KiB/s)\n</code></pre> <p>Verify the image:</p> <pre><code>sudo nerdctl images\n</code></pre> <p>Output:</p> <pre><code>REPOSITORY                                                  TAG                                            IMAGE ID        CREATED          PLATFORM       SIZE    BLOB SIZE\npublic.ecr.aws/deep-learning-containers/pytorch-training    2.9-gpu-py312-cu130-ubuntu22.04-ec2-v1-soci    fbacb61760f5    5 minutes ago    linux/amd64    0B      8.792GB\n</code></pre> <p>FUSE mount output:</p> <pre><code>{\"level\":\"info\",\"msg\":\"fuse operation count for image sha256:ca3e4389bb850cf3f43128b98897ac4522e20f354452a41139cfe7f45431da22: node.Getxattr = 13\",\"time\":\"2025-12-17T00:06:33.005689642Z\"}\n{\"level\":\"info\",\"msg\":\"fuse operation count for image sha256:ca3e4389bb850cf3f43128b98897ac4522e20f354452a41139cfe7f45431da22: node.Open = 0\",\"time\":\"2025-12-17T00:06:33.005771355Z\"}\n{\"level\":\"info\",\"msg\":\"fuse operation count for image sha256:ca3e4389bb850cf3f43128b98897ac4522e20f354452a41139cfe7f45431da22: node.Readlink = 0\",\"time\":\"2025-12-17T00:06:33.005780603Z\"}\n{\"level\":\"info\",\"msg\":\"fuse operation count for image sha256:ca3e4389bb850cf3f43128b98897ac4522e20f354452a41139cfe7f45431da22: whiteout.Getattr = 0\",\"time\":\"2025-12-17T00:06:33.005788862Z\"}\n{\"level\":\"info\",\"msg\":\"fuse operation count for image sha256:ca3e4389bb850cf3f43128b98897ac4522e20f354452a41139cfe7f45431da22: node.Listxattr = 0\",\"time\":\"2025-12-17T00:06:33.005798239Z\"}\n{\"level\":\"info\",\"msg\":\"fuse operation count for image sha256:ca3e4389bb850cf3f43128b98897ac4522e20f354452a41139cfe7f45431da22: node.Lookup = 0\",\"time\":\"2025-12-17T00:06:33.005805635Z\"}\n{\"level\":\"info\",\"msg\":\"fuse operation count for image sha256:ca3e4389bb850cf3f43128b98897ac4522e20f354452a41139cfe7f45431da22: node.Readdir = 0\",\"time\":\"2025-12-17T00:06:33.005813816Z\"}\n{\"level\":\"info\",\"msg\":\"fuse operation count for image sha256:ca3e4389bb850cf3f43128b98897ac4522e20f354452a41139cfe7f45431da22: file.Read = 0\",\"time\":\"2025-12-17T00:06:33.005824207Z\"}\n{\"level\":\"info\",\"msg\":\"fuse operation count for image sha256:ca3e4389bb850cf3f43128b98897ac4522e20f354452a41139cfe7f45431da22: file.Getattr = 0\",\"time\":\"2025-12-17T00:06:33.005831686Z\"}\n{\"level\":\"info\",\"msg\":\"fuse operation count for image sha256:ca3e4389bb850cf3f43128b98897ac4522e20f354452a41139cfe7f45431da22: node.Getattr = 13\",\"time\":\"2025-12-17T00:06:33.005839399Z\"}\n</code></pre>"},{"location":"tutorials/SOCI/#run-containers-with-nerdctl","title":"Run Containers with nerdctl","text":"<p>Run an interactive container with SOCI lazy loading:</p> <pre><code>sudo nerdctl run --snapshotter soci -it --rm --network host public.ecr.aws/deep-learning-containers/pytorch-training:2.9-gpu-py312-cu130-ubuntu22.04-ec2-v1-soci /bin/bash\n</code></pre> <p>Note: You must use the <code>--snapshotter soci</code> flag with nerdctl to enable SOCI lazy loading. Without this flag, nerdctl will download the entire image. Additionally, since nerdctl requires CNI configuration, the <code>--network host</code> flag is used in this example.</p>"},{"location":"tutorials/SOCI/#performance-comparison-soci-vs-docker","title":"Performance Comparison: SOCI vs Docker","text":""},{"location":"tutorials/SOCI/#docker-pull-benchmark","title":"Docker Pull Benchmark","text":"<pre><code>time docker pull public.ecr.aws/deep-learning-containers/pytorch-training:2.9-gpu-py312-cu130-ubuntu22.04-ec2-v1\n</code></pre> <p>Output:</p> <pre><code>real    4m26.405s\nuser    0m0.129s\nsys     0m0.127s\n</code></pre> <pre><code>docker images\n</code></pre> <p>Output:</p> <pre><code>REPOSITORY                                                 TAG                                           IMAGE ID       CREATED        SIZE\npublic.ecr.aws/deep-learning-containers/pytorch-training   2.9-gpu-py312-cu130-ubuntu22.04-ec2-v1        d007250deb63   32 hours ago   17.6GB\n</code></pre> <p>Analysis: The image size is 17.6GB. Using Docker to pull the full image takes 4m26.405s, while SOCI only pulls the index and necessary layers, taking just 8.9s(using nerdctl). The remaining layers are pulled during runtime as needed.</p>"},{"location":"tutorials/SOCI/#container-runtime-comparison","title":"Container Runtime Comparison","text":""},{"location":"tutorials/SOCI/#docker-runtime","title":"Docker Runtime","text":"<pre><code>time docker run public.ecr.aws/deep-learning-containers/pytorch-training:2.9-gpu-py312-cu130-ubuntu22.04-ec2-v1 'python -c \"import torch; print(torch.__version__)\"'\n</code></pre> <p>Output:</p> <pre><code>bash: cannot set terminal process group (-1): Inappropriate ioctl for device\nbash: no job control in this shell\n2.9.0+cu130\n\nreal    0m4.308s\nuser    0m0.010s\nsys     0m0.023s\n</code></pre>"},{"location":"tutorials/SOCI/#soci-runtime","title":"SOCI Runtime","text":"<pre><code>time sudo nerdctl run --snapshotter soci --rm --network host public.ecr.aws/deep-learning-containers/pytorch-training:2.9-gpu-py312-cu130-ubuntu22.04-ec2-v1-soci 'python -c \"import torch; print(torch.__version__)\"'\n</code></pre> <p>Output:</p> <pre><code>public.ecr.aws/deep-learning-containers/pytorch-training:2.9-gpu-py312-cu130-ubuntu22.04-ec2-v1-soci: resolved       |++++++++++++++++++++++++++++++++++++++| \nmanifest-sha256:10f1280343bfe3e9acb43bbcf2ac78e0bf636df6181c1dbbfa6e84ac285d0948:    done           |++++++++++++++++++++++++++++++++++++++| \nconfig-sha256:d007250deb6300c72286af12cf0510cf69b0c7227e4b4d50783a03ac8d917275:      exists         |++++++++++++++++++++++++++++++++++++++| \nelapsed: 0.3 s     total:  7.3 Ki (24.3 KiB/s)                                      \nbash: cannot set terminal process group (-1): Inappropriate ioctl for device\nbash: no job control in this shell\n2.9.0+cu130\n\nreal    0m22.785s\nuser    0m0.020s\nsys     0m0.031s\n</code></pre>"},{"location":"tutorials/SOCI/#key-performance-benefits","title":"Key Performance Benefits","text":"<ul> <li>Faster Initial Pull: SOCI reduces initial pull time from 4m26s to 8.9s (~97% reduction)</li> <li>Reduced Bandwidth: Only pulls necessary layers during initial pull</li> <li>Lazy Loading: Additional layers are fetched on-demand during runtime</li> <li>Storage Efficiency: Reduces initial storage footprint while maintaining full functionality</li> </ul>"},{"location":"tutorials/SOCI/#references","title":"References","text":"<p>For more information about SOCI snapshotter and related tools, please refer to the following resources:</p> <ul> <li>SOCI Snapshotter Official Documentation: https://github.com/awslabs/soci-snapshotter</li> <li>SOCI Snapshotter Getting Started Guide: https://github.com/awslabs/soci-snapshotter/blob/main/docs/getting-started.md</li> <li>SOCI Index Manifest v2 Documentation: https://github.com/awslabs/soci-snapshotter/blob/main/docs/soci-index-manifest-v2.md</li> <li>Finch Documentation: https://runfinch.com/docs/managing-finch/linux/installation/</li> <li>AWS Deep Learning Containers Release Notes: https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/dlc-release-notes.html</li> <li>AWS Deep Learning Containers Public Gallery: https://gallery.ecr.aws/deep-learning-containers</li> </ul>"},{"location":"tutorials/mlflow/dlc-with-mlflow/","title":"Using AWS Deep Learning Containers with MLflow on Amazon SageMaker AI","text":"<p>This project demonstrates how to train a TensorFlow model to predict abalone age using AWS Deep Learning Containers (DLC) and track experiments with Amazon SageMaker MLflow.</p>"},{"location":"tutorials/mlflow/dlc-with-mlflow/#project-overview","title":"Project Overview","text":"<p>The Abalone dataset contains physical measurements of abalone (marine snails) used to predict their age. This machine learning model predicts age based on features like length, diameter, height, and various weight measurements using a TensorFlow neural network with comprehensive MLflow experiment tracking.</p>"},{"location":"tutorials/mlflow/dlc-with-mlflow/#solution-architecture","title":"Solution Architecture","text":"<p>Figure 1: Architecture diagram that shows the interaction between various AWS services, AWS DLC and MLflow for the solution.</p> <p></p>"},{"location":"tutorials/mlflow/dlc-with-mlflow/#workflow-overview","title":"Workflow Overview","text":"<ol> <li>Model development with MLflow integration</li> <li>Develop a TensorFlow neural network model for abalone age prediction</li> <li> <p>Integrate MLflow tracking within the model code to log parameters, metrics, and artifacts</p> </li> <li> <p>Container Setup and Configuration on EC2</p> </li> <li>Pull an optimized TensorFlow training container from AWS's public ECR repository</li> <li> <p>Configure the EC2 / DLAMI with access to the MLflow tracking server using an IAM role for EC2</p> </li> <li> <p>Execute model training</p> </li> <li>Execute the training process within the DLC running on EC2</li> <li>Store model artifacts on S3</li> <li> <p>Log all experiment results and register model in MLflow</p> </li> <li> <p>Review results</p> </li> <li>Compare experiment results through the MLflow interface / SageMaker Studio interface</li> </ol>"},{"location":"tutorials/mlflow/dlc-with-mlflow/#prerequisites","title":"Prerequisites","text":"<p>To follow along with this walkthrough, make sure you have the following prerequisites:</p> <ul> <li>AWS account with billing enabled</li> <li>EC2 instance (t3.large or larger) running Ubuntu 20.4 or later with at least 20GB of available disk space for Docker images and containers</li> <li>Docker (latest) installed on the EC2 instance</li> <li>AWS Command Line Interface (AWS CLI) (2.0 or later)</li> <li>IAM role with permissions for:</li> <li>Role for Amazon EC2 to talk to Amazon SageMaker MLflow</li> <li>Amazon ECR (to pull the TensorFlow container)</li> <li>Amazon SageMaker MLflow (to track experiments and register models)</li> <li>Amazon SageMaker AI Studio domain - To create a domain, refer to Guide to getting set up with Amazon SageMaker AI</li> <li>Add <code>sagemaker-mlflow:AccessUI</code> permission to the SageMaker execution role created. This permission will allow you to navigate to MLflow 3.0 from SageMaker AI Studio console</li> <li>MLflow 3.0 tracking server set up in SageMaker AI</li> <li>Internet access from the EC2 instance to download the abalone dataset</li> <li>GitHub repository cloned to your EC2 instance</li> </ul>"},{"location":"tutorials/mlflow/dlc-with-mlflow/#step-by-step-implementation","title":"Step-by-Step Implementation","text":"<p>The following steps will walk you through the entire process, from provisioning infrastructure and setting up permissions to executing your first training job with comprehensive experiment tracking.</p>"},{"location":"tutorials/mlflow/dlc-with-mlflow/#step-1-ec2-instance-setup","title":"Step 1: EC2 Instance Setup","text":"<ol> <li>Log in to the AWS Management Console and navigate to EC2</li> <li>Click \"Launch Instance\" - Refer to the AWS documentation on how to launch a new instance</li> <li>Configure your instance:</li> <li>Name: <code>aws-dlc-training-instance</code></li> <li>AMI: Amazon Linux 2023</li> <li>Instance type: <code>t3.large</code> (recommended for ML workloads)</li> <li>Key pair: Create or select an existing key pair</li> <li>Network settings: Allow SSH traffic</li> <li>Storage: 20 GB gp3 volume</li> </ol>"},{"location":"tutorials/mlflow/dlc-with-mlflow/#step-2-iam-role-configuration","title":"Step 2: IAM Role Configuration","text":"<ol> <li>Navigate to IAM in the AWS Console</li> <li>Create a new role with the following policy:</li> </ol> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:DescribeMLflowTrackingServer\",\n                \"sagemaker-mlflow:CreateExperiment\",\n                \"sagemaker-mlflow:GetExperiment\",\n                \"sagemaker-mlflow:SearchExperiments\",\n                \"sagemaker-mlflow:CreateRun\",\n                \"sagemaker-mlflow:UpdateRun\",\n                \"sagemaker-mlflow:LogMetric\",\n                \"sagemaker-mlflow:LogParam\",\n                \"sagemaker-mlflow:LogModel\",\n                \"sagemaker-mlflow:CreateRegisteredModel\",\n                \"sagemaker-mlflow:GetExperimentByName\",\n                \"sagemaker-mlflow:GetRun\",\n                \"sagemaker-mlflow:LogOutputs\",\n                \"sagemaker-mlflow:FinalizeLoggedModel\",\n                \"sagemaker-mlflow:GetMetricHistory\",\n                \"sagemaker-mlflow:LogBatch\",\n                \"sagemaker-mlflow:GetLoggedModel\",\n                \"sagemaker-mlflow:CreateModelVersion\",\n                \"sagemaker-mlflow:SetLoggedModelTags\",\n                \"sagemaker-mlflow:LogInputs\",\n                \"sagemaker-mlflow:SetTag\"\n            ],\n            \"Resource\": \"arn:aws:sagemaker:us-east-1:{your-account-number}:mlflow-tracking-server/{your-tracking-server-name}\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"ecr:GetAuthorizationToken\",\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ecr:BatchCheckLayerAvailability\",\n                \"ecr:GetDownloadUrlForLayer\",\n                \"ecr:BatchGetImage\"\n            ],\n            \"Resource\": \"arn:aws:ecr:us-east-1:763104351884:repository/tensorflow-training\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:PutObject\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::{your-bucket}/{your-path}/*\"\n            ]\n        }\n    ]\n}\n</code></pre> <ol> <li>Attach a trust policy to the role:</li> </ol> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"ec2.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\n</code></pre> <ol> <li>Attach this role to your EC2 instance:</li> <li>Go to EC2 Dashboard \u2192 Instances \u2192 Select your instance</li> <li>Actions \u2192 Security \u2192 Modify IAM role</li> <li>Select the role you created and choose \"Update IAM role\"</li> </ol>"},{"location":"tutorials/mlflow/dlc-with-mlflow/#step-3-connect-to-your-ec2-instance-and-install-required-software","title":"Step 3: Connect to Your EC2 Instance and Install Required Software","text":"<p>Refer to the AWS documentation to connect to your EC2 instance.</p> <pre><code># Connect to your instance\nssh -i your-key.pem ec2-user@your-instance-public-dns\n\n# Update system packages\nsudo yum update -y\n\n# Install Docker\nsudo yum install -y docker\nsudo systemctl start docker\nsudo systemctl enable docker\nsudo usermod -aG docker $USER\n\n# Log out and log back in for group changes to take effect\nexit\n\n# Reconnect to your instance\nssh -i your-key.pem ec2-user@your-instance-public-dns\n\n# Install Git\nsudo yum install -y git\n\n# Install AWS CLI (if not already installed)\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\n</code></pre>"},{"location":"tutorials/mlflow/dlc-with-mlflow/#step-4-clone-the-repository","title":"Step 4: Clone the Repository","text":"<pre><code># Clone the repository\ngit clone https://github.com/aws-samples/sample-aws-deep-learning-containers.git \ncd mlflow/dlc-with-mlflow\n\n# Create required directories\nmkdir -p data\nmkdir -p output\n</code></pre>"},{"location":"tutorials/mlflow/dlc-with-mlflow/#step-5-verify-the-project-structure","title":"Step 5: Verify the Project Structure","text":"<pre><code># List files in the project\nls -la\n\n# Check the training script\ncat src/train.py\n\n# Make the training script executable\nchmod +x run_training.sh\n</code></pre> <p>Expected project structure: <pre><code>aws-dlc-with-mlflow/\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 train.py                    # Main training script\n\u251c\u2500\u2500 data/                           # Dataset directory\n\u251c\u2500\u2500 output/                         # Model output directory\n\u251c\u2500\u2500 requirements.txt                # Python dependencies\n\u251c\u2500\u2500 Dockerfile                      # Custom container definition\n\u251c\u2500\u2500 run_training.sh                # Training execution script\n\u251c\u2500\u2500 IAM_permissions.json           # Required AWS permissions\n\u251c\u2500\u2500 IAM_trust_relationship.json    # IAM role trust policy\n\u2514\u2500\u2500 README.md                      # This documentation\n</code></pre></p>"},{"location":"tutorials/mlflow/dlc-with-mlflow/#step-6-update-the-mlflow-arn-in-the-training-script","title":"Step 6: Update the MLflow ARN in the Training Script","text":"<p>Note: See Prerequisites section if you do not have a MLflow tracking server setup.</p> <pre><code># Open the file in a text editor\nnano src/train.py\n\n# Find and update this line with your tracking server name\ntracking_server_name = \"{your-tracking-server-name}\"\n</code></pre>"},{"location":"tutorials/mlflow/dlc-with-mlflow/#step-7-run-the-training-script","title":"Step 7: Run the Training Script","text":"<pre><code># Execute the training script\n./run_training.sh\n</code></pre> <p>This script will: 1. Check for the abalone dataset and download it if needed 2. Authenticate with AWS ECR 3. Pull the TensorFlow training container 4. Run the training script in the container</p>"},{"location":"tutorials/mlflow/dlc-with-mlflow/#step-8-monitor-the-training-process","title":"Step 8: Monitor the Training Process","text":"<p>The training output will show: - Dataset loading and preprocessing - Model architecture - Training progress with loss and metrics - Evaluation results - MLflow logging information</p> <p>Example output: <pre><code>TensorFlow version: 2.18.0\nMLflow version: 3.0.0\nLoading Abalone dataset...\nLoading dataset from /opt/ml/data/abalone.data\n...\nMLflow tracking server URL: arn:aws:sagemaker:us-east-1:{your-account-id}:mlflow-tracking-server/{your-tracking-server-name}\nUsing experiment: abalone-tensorflow-experiment\nMLflow configured - autolog disabled, using custom callback\nMLflow run ID: 6c1422c154964c139927a53dc242dcb6\n...\nTraining the model...\nStarting epoch 1\nEpoch 1/100\n59/84 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0s 2ms/step - loss: 110.2189 - mae: 9.8295 \nCompleted epoch 1 in 2.15s\n  loss: 82.3213\n  mae: 8.2671\n  val_loss: 74.2215\n  val_mae: 8.1098\n  learning_rate: 0.0010000000474974513\n...\nRegistered model 'abalone-tensorflow-custom-callback-model' already exists. Creating a new version of this model...\n2025/07/24 16:46:33 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: abalone-tensorflow-custom-callback-model, version 2\nCreated version '2' of model 'abalone-tensorflow-custom-callback-model'.\nModel and artifacts logged to MLflow\n\ud83c\udfc3 View run unique-cod-104 at: https://us-east-1.experiments.sagemaker.aws/#/experiments/1/runs/6c1422c154964c139927a53dc242dcb6\n\ud83e\uddea View experiment at: https://us-east-1.experiments.sagemaker.aws/#/experiments/1\nMLflow run 6c1422c154964c139927a53dc242dcb6 completed with custom callback\n</code></pre></p>"},{"location":"tutorials/mlflow/dlc-with-mlflow/#step-9-view-the-results-in-mlflow","title":"Step 9: View the Results in MLflow","text":"<p>After the training has completed, the model is registered in MLflow and within SageMaker AI. In the following steps you will access them to view the details, starting with SageMaker AI Studio console:</p> <ol> <li>Access the Amazon SageMaker AI Studio console and access the domain that was created earlier</li> <li>Navigate to MLflow \u2192 Tracking servers</li> <li>Select your MLflow tracking server</li> </ol> <p>Figure 2: Landing page for SageMaker Studio UI and access to MLflow</p> <p></p> <ol> <li>Click \"Open MLflow UI\"</li> <li>Browse to the \"abalone-tensorflow-experiment\" experiment</li> </ol> <p>The following image shows experiment \"abalone-tensorflow-experiment\" with a run name of \"unique-cod-104\".</p> <p>Figure 3: Experiment \"unique-cod-104\"</p> <p></p> <ol> <li>View the run details, metrics, and artifacts in MLflow</li> </ol> <p>The following images show details of the experiment run, the training loss and registered model within MLflow and SageMaker AI Studio.</p> <p>Figure 4: Details for run name \"unique-cod-104\". Reference the registered model \"abalone-tensorflow-custom-callback-model\" and model version of v2.</p> <p></p> <p>Figure 5: Details of loss by epochs, logged using a custom callback</p> <p></p> <p>Figure 6: Registered models of \"abalone-tensorflow-custom-callback-model\".</p> <p></p> <p>When logging a model in MLflow using <code>mlflow.tensorflow.log_model()</code> with the <code>registered_model_name</code> parameter, the model is automatically registered in Amazon SageMaker Model Registry without additional steps. This seamless integration creates a versioned model entry in SageMaker, complete with signatures and metadata, directly from your MLflow experiment. </p> <p>Within your SageMaker Studio UI, under Models, you can find the association under the details tab. The screenshot below illustrates how the model in SageMaker Model Registry ties back to the model logged in MLflow.</p> <p>Figure 7: Registered model in SageMaker Model Registry</p> <p></p> <p>Figure 8: Model artifacts uploaded to S3 after training completion.</p> <p></p>"},{"location":"tutorials/mlflow/dlc-with-mlflow/#resources","title":"Resources","text":"<ul> <li>AWS Deep Learning Containers Documentation</li> <li>AWS Deep Learning AMIs Documentation</li> <li>SageMaker MLflow Documentation</li> <li>SageMaker AI Studio Setup Guide</li> <li>Abalone Dataset Information</li> <li>TensorFlow Documentation</li> <li>TensorFlow Deep Learning Container Repository</li> </ul>"},{"location":"tutorials/training/eks/","title":"Distributed Training on Amazon EKS with AWS Deep Learning Containers","text":"<p>In this post, we show how to configure and verify a distributed training cluster using AWS Deep Learning Containers on Amazon Elastic Kubernetes Service (EKS). We demonstrate building a cost-effective, enterprise-scale distributed training environment for large language models using P4d instances, FSx for Lustre storage, and PyTorch FSDP (Fully Sharded Data Parallel), making sure the infrastructure meets production standards. We demonstrate this by setting up a distributed training system that fine-tunes Meta Llama 2 7B using a systematic approach to launch and verify all required components.</p> <p>This sample consists of the following components:</p> <p>Infrastructure Setup \u2013 Deploy EKS cluster with GPU-optimized P4d instances and EFA networking for high-performance distributed training.</p> <p>Container Building \u2013 Create custom Docker images based on AWS Deep Learning Containers with additional dependencies for training workloads.</p> <p>Plugin Installation \u2013 Configure NVIDIA GPU plugins, EFA networking, distributed training frameworks (etcd, Kubeflow Training Operator), and persistent storage drivers.</p> <p>Storage Configuration \u2013 Set up FSx for Lustre high-performance parallel filesystem for training data and model checkpoints.</p> <p>Validation &amp; Testing \u2013 Run comprehensive health checks including GPU validation, NCCL communication tests, and sample training workloads.</p> <p>Training Orchestration \u2013 Launch distributed PyTorch jobs using FSDP with proper worker coordination and fault handling.</p> <p>This repository is explained in detail in the AWS blog \"Configuring and Verifying a Distributed Training Cluster  with AWS Deep Learning Containers on Amazon Elastic Kubernetes Service\"</p>"},{"location":"tutorials/training/eks/#code-contains","title":"Code Contains:","text":"<p>The repository includes practical scripts and configurations demonstrating:</p> <ul> <li>Building custom Docker images from AWS Deep Learning Containers with PyTorch 2.7.1</li> <li>Deploying EKS clusters with GPU node groups and EFA-enabled networking using eksctl</li> <li>Installing and configuring NVIDIA device plugins, EFA plugins, and distributed training operators</li> <li>Setting up FSx for Lustre filesystem for high-throughput storage</li> <li>Running NCCL tests to validate multi-node GPU communication</li> <li>Launching distributed PyTorch training jobs with FSDP using Kubeflow Training Operator</li> </ul> <p></p>"},{"location":"tutorials/training/eks/#prerequisites","title":"Prerequisites","text":"<ul> <li>An AWS account with billing enabled</li> <li>AWS CLI configured with appropriate permissions</li> <li>Docker installed on build environment</li> <li>At least 100 GiB storage for building containers</li> <li>Hugging Face token for Llama 2 model access (gated model)</li> <li>Optional: EC2 Capacity Reservation for P4d instances</li> <li>Deep Learning AMI for container building</li> </ul>"},{"location":"tutorials/training/eks/#execution","title":"Execution","text":""},{"location":"tutorials/training/eks/#step-1-environment-setup","title":"Step 1: Environment Setup","text":"<p>Launch an EC2 instance with Deep Learning AMI and install dependencies:</p> <pre><code># Clone this repository\ngit clone &lt;repository-url&gt;\ncd &lt;repository-name&gt;\n\n# Install AWS CLI, kubectl, and eksctl\nsource ./setup_ec2.sh\n</code></pre>"},{"location":"tutorials/training/eks/#step-2-build-custom-training-container","title":"Step 2: Build Custom Training Container","text":"<pre><code># Build and push custom Docker image with training dependencies\nbash ./build.sh\n</code></pre>"},{"location":"tutorials/training/eks/#step-3-deploy-eks-cluster","title":"Step 3: Deploy EKS Cluster","text":"<pre><code># Create EKS cluster with GPU nodes and required add-ons\neksctl create cluster -f ./eks-p4d-odcr.yaml\n</code></pre>"},{"location":"tutorials/training/eks/#step-4-install-training-plugins","title":"Step 4: Install Training Plugins","text":"<pre><code># Deploy etcd for worker coordination\nkubectl apply -f etcd.yaml\n\n# Install Kubeflow Training Operator\nkubectl apply --server-side -k \"github.com/kubeflow/training-operator.git/manifests/overlays/standalone?ref=v1.9.3\"\n\n# Create FSx filesystem and storage\nbash ./fsx_create.sh\nkubectl apply -f ./fsx-pvc-static.yaml\n</code></pre>"},{"location":"tutorials/training/eks/#step-5-validate-environment","title":"Step 5: Validate Environment","text":"<pre><code># Verify GPU availability\nkubectl apply -f nvidia_smi.yaml\nkubectl logs nvidia-smi\n\n# Test NCCL communication\nkubectl apply -f nccl-tests.yaml\nkubectl get pods | grep nccl\n</code></pre>"},{"location":"tutorials/training/eks/#step-6-run-distributed-training","title":"Step 6: Run Distributed Training","text":"<pre><code># Configure Hugging Face token in fsdp.conf\n# Then launch training job\nbash ./fsdp.sh\nkubectl apply -f ./fsdp.yaml\n\n# Monitor training progress\nkubectl get pods | grep fsdp\nkubectl logs -f fsdp-worker-0\n</code></pre>"},{"location":"tutorials/training/eks/#project-structure","title":"Project Structure","text":"<pre><code>\u251c\u2500\u2500 setup_ec2.sh              # EC2 environment setup\n\u251c\u2500\u2500 build.sh                  # Docker image build script\n\u251c\u2500\u2500 Dockerfile.llama2-efa-dlc # Custom training container\n\u251c\u2500\u2500 .env                      # Environment variables\n\u2502\n\u251c\u2500\u2500 eks-p4d-odcr.yaml         # EKS cluster configuration\n\u251c\u2500\u2500 eks-p4d.yaml              # Alternative cluster config\n\u251c\u2500\u2500 eks-vpc-odcr-p4d.yaml     # VPC-specific cluster config\n\u2502\n\u251c\u2500\u2500 fsx_create.sh             # FSx filesystem creation\n\u251c\u2500\u2500 fsx_deploy.sh             # FSx deployment to EKS\n\u251c\u2500\u2500 fsx_delete.sh             # FSx cleanup\n\u251c\u2500\u2500 fsx-pvc-static.yaml       # FSx persistent volume claim\n\u251c\u2500\u2500 fsx.conf                  # FSx configuration\n\u2502\n\u251c\u2500\u2500 fsdp.yaml                 # PyTorch distributed training job\n\u251c\u2500\u2500 fsdp.yaml-template        # Training job template\n\u251c\u2500\u2500 fsdp.sh                   # Training job launcher\n\u251c\u2500\u2500 fsdp.conf                 # Training configuration\n\u2502\n\u251c\u2500\u2500 etcd.yaml                 # Worker coordination service\n\u251c\u2500\u2500 nccl-tests.yaml           # Network performance validation\n\u2514\u2500\u2500 nvidia_smi.yaml           # GPU validation job\n</code></pre>"},{"location":"tutorials/training/eks/#configuration-details","title":"Configuration Details","text":"<p>All the sample scripts can be adjusted to the needs of specific workloads.</p>"},{"location":"tutorials/training/eks/#cluster-configuration","title":"Cluster Configuration","text":"<ul> <li>System nodes: <code>c5.2xlarge</code> for cluster management</li> <li>GPU nodes: <code>p4d.24xlarge</code> with EFA networking (8 H100 GPUs per node)</li> <li>Storage: 500 GiB EBS volumes + FSx for Lustre</li> <li>Networking: EFA-enabled for high-performance communication</li> <li>Kubernetes: Version 1.33 with managed node groups</li> </ul>"},{"location":"tutorials/training/eks/#training-configuration","title":"Training Configuration","text":"<ul> <li>Model: Meta Llama 2 7B (gated model - requires HF token)</li> <li>Framework: PyTorch with FSDP (Fully Sharded Data Parallel)</li> <li>Communication: NCCL with AWS OFI backend for EFA</li> <li>Storage: FSx for Lustre for dataset and checkpoints</li> <li>Orchestration: Kubeflow Training Operator with etcd coordination</li> </ul>"},{"location":"tutorials/training/eks/#validation-and-testing","title":"Validation and Testing","text":"<p>The setup includes comprehensive validation steps:</p> <ul> <li>GPU Validation: Verify NVIDIA drivers and GPU visibility</li> <li>Network Testing: NCCL all-reduce and bandwidth tests</li> <li>Storage Verification: FSx mount and throughput validation</li> <li>Training Validation: Sample FSDP job with Llama 2 7B</li> </ul>"},{"location":"tutorials/training/eks/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/training/eks/#common-issues","title":"Common Issues","text":"<ul> <li>GPU not visible: Check NVIDIA device plugin installation</li> <li>EFA not working: Verify EFA plugin and instance type support</li> <li>Training fails: Ensure etcd is running and accessible</li> <li>Storage issues: Verify FSx filesystem is mounted correctly</li> </ul>"},{"location":"tutorials/training/eks/#debugging-commands","title":"Debugging Commands","text":"<pre><code># Check node status and GPU resources\nkubectl get nodes -o wide\nkubectl get nodes -o json | jq '.items[].status.capacity.\"nvidia.com/gpu\"'\n\n# Check EFA availability\nkubectl get nodes -o=custom-columns=NAME:.metadata.name,EFA:.status.allocatable.vpc\\\\.amazonaws\\\\.com/efa\n\n# Monitor training jobs\nkubectl describe -f ./fsdp.yaml\nkubectl logs &lt;pod-name&gt; -f\n</code></pre>"},{"location":"tutorials/training/eks/#cleanup","title":"Cleanup","text":"<pre><code># Stop training job and coordination services\nkubectl delete -f ./fsdp.yaml\nkubectl delete -f ./etcd.yaml\n\n# Delete FSx filesystem\nbash ./fsx_delete.sh\n\n# Delete EKS cluster\neksctl delete cluster -f ./eks-p4d-odcr.yaml\n</code></pre>"},{"location":"tutorials/training/eks/#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Use EC2 Capacity Reservations for predictable P4d availability</li> <li>Configure cluster autoscaling to scale down when not training</li> <li>Monitor FSx usage and adjust throughput based on workload needs</li> <li>Consider Spot instances for development/testing workloads</li> </ul>"},{"location":"tutorials/training/eks/#license","title":"License","text":"<p>This library is licensed under the MIT-0 License. See the LICENSE file.</p>"},{"location":"tutorials/training/eks/#additional-resources","title":"Additional Resources","text":"<ul> <li>Scripts for running FSDP distributed training on Amazon EKS</li> <li>AWSome Distributed Training repository</li> <li>AWS Deep Learning Containers Documentation</li> <li>Amazon EKS User Guide</li> <li>FSx for Lustre Documentation</li> </ul>"},{"location":"tutorials/vllm-samples/deepseek/eks/","title":"vLLM Deepseek Model on EKS with GPU Support, EFA, and FSx Lustre Integration","text":"<p>This repository contains scripts and configuration files to deploy a deepseek model using the AWS public vLLM deep learning container ECR image on an Amazon EKS cluster with GPU support (p4d.24xlarge instances with NVIDIA A100 GPUs), Elastic Fabric Adapter (EFA) for high-performance networking, and FSx Lustre for persistent model storage.</p> <p></p>"},{"location":"tutorials/vllm-samples/deepseek/eks/#advanced-example-ai-powered-fraud-detection","title":"\ud83d\ude80 Advanced Example: AI-Powered Fraud Detection","text":"<p>New! Check out our complete production-ready fraud detection demo that showcases: - \u2705 Real-time financial fraud detection using DeepSeek R1 32B - \u2705 AI agents with Model Context Protocol (MCP) microservices - \u2705 Interactive Streamlit UI for fraud analysis - \u2705 Full deployment on AWS (EKS + ECS + ALB) - \u2705 Sub-2 second inference latency - \u2705 Production-ready architecture with cost optimization</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/#go-to-fraud-detection-demo","title":"\ud83d\udc49 Go to Fraud Detection Demo \u2192","text":"<p>What's included: - Complete working production example - Step-by-step deployment guide - 6 MCP microservices (transaction-risk, identity-verifier, geolocation-checker, etc.) - Performance metrics and cost analysis - Troubleshooting guide</p> <p>Timeline: ~45 min for basic vLLM setup + ~30 min for full demo deployment</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/#two-ways-to-use-this-repository","title":"\ud83d\udcda Two Ways to Use This Repository","text":""},{"location":"tutorials/vllm-samples/deepseek/eks/#option-1-basic-vllm-deployment-this-page","title":"Option 1: Basic vLLM Deployment (This Page)","text":"<p>Deploy vLLM with DeepSeek R1 32B on Amazon EKS. Perfect for: - Learning vLLM deployment basics - Testing model inference - Building your own applications</p> <p>Continue reading below for the basic deployment guide.</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/#option-2-complete-fraud-detection-demo","title":"Option 2: Complete Fraud Detection Demo","text":"<p>Full production example with AI agents, microservices, and UI. Perfect for: - Understanding production AI architectures - Seeing vLLM in a real-world use case - Learning about MCP and AI agents</p> <p>Jump to: fraud-detection-demo/README.md</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/#prerequisites","title":"Prerequisites","text":"<p>Before getting started, ensure you have:</p> <ul> <li>An AWS account with access to amazon EC2 P4 instances (you may need to request a quota increase)</li> <li>Access to a terminal which has the below tools installed:</li> <li>AWS CLI version 2.11.0 or later</li> <li>eksctl version 0.150.0 or later</li> <li>kubectl version 1.27 or later</li> <li>Helm version 3.12.0 or later</li> <li>Configure a new aws cli profile (vllm-profile) with an IAM role/user that has the following permissions:</li> <li>Create, manage, and delete EKS clusters and node groups</li> <li>Create, manage, and delete EC2 resources including VPCs, subnets, security groups, and internet gateways</li> <li>Create and manage IAM roles</li> <li>Create, update, and delete CloudFormation stacks</li> <li>Create, delete, and describe FSx file systems</li> <li>Create and manage Elastic Load Balancers</li> </ul>"},{"location":"tutorials/vllm-samples/deepseek/eks/#regional-availability","title":"Regional Availability","text":"<p>This solution can be deployed in any region where Amazon EKS, P4d instances, and Amazon FSx for Lustre are available. This guide uses the us-west-2 (Oregon) region.</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/#setting-up-aws-profile-for-a-new-account","title":"Setting Up AWS Profile for a New Account","text":"<p>If you're deploying to a new AWS account, you'll need to create a new AWS profile:</p> <pre><code># Create a new AWS profile\naws configure --profile vllm-profile\n\n# For temporary credentials, also add the session token\necho -e \"\\naws_session_token = YOUR_SESSION_TOKEN_HERE\" &gt;&gt; ~/.aws/credentials\n</code></pre> <p>You'll need to provide: - AWS Access Key ID - AWS Secret Access Key - Default region (e.g., us-west-2) - Default output format (e.g., json, table) - Session token (if using temporary credentials)</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/#installation-instructions","title":"Installation Instructions","text":""},{"location":"tutorials/vllm-samples/deepseek/eks/#install-aws-cli","title":"Install AWS CLI","text":"<pre><code># For Linux\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\n\n# Configure AWS CLI\naws configure\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/#install-eksctl","title":"Install eksctl","text":"<pre><code># For Linux\ncurl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp\nsudo mv /tmp/eksctl /usr/local/bin\neksctl version\n\n# For macOS\nbrew tap weaveworks/tap\nbrew install weaveworks/tap/eksctl\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/#install-kubectl","title":"Install kubectl","text":"<pre><code># For Linux\ncurl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nchmod +x kubectl\nsudo mv kubectl /usr/local/bin/\n\n# For macOS\nbrew install kubectl\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/#install-helm","title":"Install Helm","text":"<pre><code># For Linux\ncurl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash\n\n# For macOS\nbrew install helm\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/#files","title":"Files","text":"<ul> <li><code>cleanup.sh</code>: Script for cleaning up all resources created by this deployment</li> <li><code>eks-cluster.yaml</code>: EKS cluster configuration</li> <li><code>fsx-inspect-pod.yaml</code>: Pod definition for inspecting FSx Lustre filesystem</li> <li><code>fsx-lustre-pv.yaml</code>: Persistent Volume for FSx Lustre</li> <li><code>fsx-lustre-pvc.yaml</code>: Persistent Volume Claim for FSx Lustre</li> <li><code>fsx-storage-class.yaml</code>: Storage class for FSx Lustre</li> <li><code>iam-policy.json</code>: IAM policy document for the AWS Load Balancer Controller</li> <li><code>large-model-nodegroup.yaml</code>: Nodegroup configuration for p4d.24xlarge instances with EFA support</li> <li><code>vllm-deepseek-32b-lws.yaml</code>: LeaderWorkerSet configuration for the vLLM server with deepseek model</li> <li><code>vllm-deepseek-32b-lws-ingress.yaml</code>: Kubernetes ingress for the vLLM server with ALB</li> </ul>"},{"location":"tutorials/vllm-samples/deepseek/eks/#setup","title":"Setup","text":"<p>This section provides step-by-step instructions to deploy the vLLM Deepseek model on Amazon EKS with GPU support, EFA, and FSx Lustre integration. Follow these steps in order to set up your environment.</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/#create-an-eks-cluster","title":"Create an EKS cluster","text":"<p>First, we create an EKS cluster in the us-west-2 Region using the provided configuration file:</p> <pre><code># Update the region in eks-cluster.yaml if needed\nsed -i \"s|region: us-east-1|region: us-west-2|g\" eks-cluster.yaml\n\n# Create the EKS cluster\neksctl create cluster -f eks-cluster.yaml --profile vllm-profile\n</code></pre> <p>Timeline: This will take approximately 15-20 minutes to complete. During this time, eksctl creates a CloudFormation stack that provisions the necessary resources for your EKS cluster.</p> <p>You can validate the cluster creation with:</p> <pre><code># Verify cluster creation\neksctl get cluster --profile vllm-profile\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/#create-a-node-group-with-efa-support","title":"Create a node group with EFA support","text":"<p>Next, we create a managed node group with P4d.24xlarge instances that have EFA enabled:</p> <pre><code># Get the VPC ID from the EKS cluster\nVPC_ID=$(aws --profile vllm-profile eks describe-cluster --name vllm-cluster \\\n  --query \"cluster.resourcesVpcConfig.vpcId\" --output text)\n\n# Find the one of private subnet's availability zone\nPRIVATE_AZ=$(aws --profile vllm-profile ec2 describe-subnets \\\n  --filters \"Name=vpc-id,Values=$VPC_ID\" \"Name=map-public-ip-on-launch,Values=false\" \\\n  --query \"Subnets[0].AvailabilityZone\" --output text)\necho \"Selected private subnet AZ: $PRIVATE_AZ\"\n\n# update the nodegroup_az section with the private AZ value\nsed -i \"s|availabilityZones: \\[nodegroup_az\\]|availabilityZones: \\[\\\"$PRIVATE_AZ\\\"\\]|g\" large-model-nodegroup.yaml\n\n# Verify the change\ngrep \"availabilityZones\" large-model-nodegroup.yaml\n\n# Create the node group with EFA support\neksctl create nodegroup -f large-model-nodegroup.yaml --profile vllm-profile\n</code></pre> <p>Timeline: This will take approximately 10-15 minutes to complete. The EFA configuration is particularly important for multi-node deployments.</p> <p>After the node group is created, configure kubectl to connect to the cluster:</p> <pre><code># Configure kubectl to connect to the cluster\naws eks update-kubeconfig --name vllm-cluster --region us-west-2 --profile vllm-profile\n</code></pre> <p>Verify that the nodes are ready:</p> <pre><code># Check node status\nkubectl get nodes\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/#check-nvidia-device-pods","title":"Check NVIDIA device pods","text":"<p>Verify that the NVIDIA device plugin is running:</p> <pre><code># Check NVIDIA device plugin pods\nkubectl get pods -n kube-system | grep nvidia\n</code></pre> <p>Verify that GPUs are available in the cluster:</p> <pre><code># Check available GPUs\nkubectl get nodes -o json | jq '.items[].status.capacity.\"nvidia.com/gpu\"'\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/#create-an-fsx-for-lustre-file-system","title":"Create an FSx for Lustre file system","text":"<p>For optimal performance, we create an FSx for Lustre file system to store our model weights:</p> <pre><code># Create a security group for FSx Lustre\nFSX_SG_ID=$(aws --profile vllm-profile ec2 create-security-group --group-name fsx-lustre-sg \\\n  --description \"Security group for FSx Lustre\" \\\n  --vpc-id $(aws --profile vllm-profile eks describe-cluster --name vllm-cluster \\\n  --query \"cluster.resourcesVpcConfig.vpcId\" --output text) \\\n  --query \"GroupId\" --output text)\n\necho \"Created security group: $FSX_SG_ID\"\n\n# Add inbound rules for FSx Lustre\naws --profile vllm-profile ec2 authorize-security-group-ingress --group-id $FSX_SG_ID \\\n  --protocol tcp --port 988-1023 \\\n  --source-group $(aws --profile vllm-profile eks describe-cluster --name vllm-cluster \\\n  --query \"cluster.resourcesVpcConfig.clusterSecurityGroupId\" --output text)\n\naws --profile vllm-profile ec2 authorize-security-group-ingress --group-id $FSX_SG_ID \\\n     --protocol tcp --port 988-1023 \\\n     --source-group $FSX_SG_ID\n\n# Create the FSx Lustre filesystem\nSUBNET_ID=$(aws --profile vllm-profile eks describe-cluster --name vllm-cluster \\\n  --query \"cluster.resourcesVpcConfig.subnetIds[0]\" --output text)\n\necho \"Using subnet: $SUBNET_ID\"\n\nFSX_ID=$(aws --profile vllm-profile fsx create-file-system --file-system-type LUSTRE \\\n  --storage-capacity 1200 --subnet-ids $SUBNET_ID \\\n  --security-group-ids $FSX_SG_ID --lustre-configuration DeploymentType=SCRATCH_2 \\\n  --tags Key=Name,Value=vllm-model-storage \\\n  --query \"FileSystem.FileSystemId\" --output text)\n\necho \"Created FSx filesystem: $FSX_ID\"\n\n# Wait for the filesystem to be available (typically takes 5-10 minutes)\necho \"Waiting for filesystem to become available...\"\naws --profile vllm-profile fsx describe-file-systems --file-system-id $FSX_ID \\\n  --query \"FileSystems[0].Lifecycle\" --output text\n\n# You can run the above command periodically until it returns \"AVAILABLE\"\n# Example: watch -n 30 \"aws --profile vllm-profile fsx describe-file-systems --file-system-id $FSX_ID --query FileSystems[0].Lifecycle --output text\"\n\n# Get the DNS name and mount name\nFSX_DNS=$(aws --profile vllm-profile fsx describe-file-systems --file-system-id $FSX_ID \\\n  --query \"FileSystems[0].DNSName\" --output text)\n\nFSX_MOUNT=$(aws --profile vllm-profile fsx describe-file-systems --file-system-id $FSX_ID \\\n  --query \"FileSystems[0].LustreConfiguration.MountName\" --output text)\n\necho \"FSx DNS: $FSX_DNS\"\necho \"FSx Mount Name: $FSX_MOUNT\"\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/#install-the-aws-fsx-csi-driver","title":"Install the AWS FSx CSI Driver","text":"<p>To mount the FSx for Lustre file system in our Kubernetes pods:</p> <pre><code># Add the AWS FSx CSI Driver Helm repository\nhelm repo add aws-fsx-csi-driver https://kubernetes-sigs.github.io/aws-fsx-csi-driver/\nhelm repo update\n\n# Install the AWS FSx CSI Driver\nhelm install aws-fsx-csi-driver aws-fsx-csi-driver/aws-fsx-csi-driver --namespace kube-system\n</code></pre> <p>Verify that the AWS FSx CSI Driver is running:</p> <pre><code># Check AWS FSx CSI Driver pods\nkubectl get pods -n kube-system | grep fsx\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/#create-kubernetes-resources-for-fsx-for-lustre","title":"Create Kubernetes resources for FSx for Lustre","text":"<p>We create the necessary Kubernetes resources to use our FSx for Lustre file system:</p> <pre><code># Update the storage class with your subnet and security group IDs\nsed -i \"s|&lt;subnet-id&gt;|$SUBNET_ID|g\" fsx-storage-class.yaml\nsed -i \"s|&lt;sg-id&gt;|$FSX_SG_ID|g\" fsx-storage-class.yaml\n\n# Update the PV with your FSx Lustre details\nsed -i \"s|&lt;fs-id&gt;|$FSX_ID|g\" fsx-lustre-pv.yaml\nsed -i \"s|&lt;fs-id&gt;.fsx.us-west-2.amazonaws.com|$FSX_DNS|g\" fsx-lustre-pv.yaml\nsed -i \"s|&lt;mount-name&gt;|$FSX_MOUNT|g\" fsx-lustre-pv.yaml\n\n# Apply the Kubernetes resources\nkubectl apply -f fsx-storage-class.yaml\nkubectl apply -f fsx-lustre-pv.yaml\nkubectl apply -f fsx-lustre-pvc.yaml\n</code></pre> <p>Verify that the resources were created successfully:</p> <pre><code># Check storage class\nkubectl get sc fsx-sc\n\n# Check persistent volume\nkubectl get pv fsx-lustre-pv\n\n# Check persistent volume claim\nkubectl get pvc fsx-lustre-pvc\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/#install-the-aws-load-balancer-controller","title":"Install the AWS Load Balancer Controller","text":"<p>To expose our vLLM service to the outside world:</p> <pre><code># Download the IAM policy document\ncurl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json\n\n# Create the IAM policy\naws --profile vllm-profile iam create-policy --policy-name AWSLoadBalancerControllerIAMPolicy --policy-document file://iam-policy.json\n\n# Create an IAM OIDC provider for the cluster\neksctl utils associate-iam-oidc-provider --profile vllm-profile --region=us-west-2 --cluster=vllm-cluster --approve\n\n# Create an IAM service account for the AWS Load Balancer Controller\nACCOUNT_ID=$(aws --profile vllm-profile sts get-caller-identity --query \"Account\" --output text)\neksctl create iamserviceaccount \\\n  --profile vllm-profile \\\n  --cluster=vllm-cluster \\\n  --namespace=kube-system \\\n  --name=aws-load-balancer-controller \\\n  --attach-policy-arn=arn:aws:iam::$ACCOUNT_ID:policy/AWSLoadBalancerControllerIAMPolicy \\\n  --override-existing-serviceaccounts \\\n  --approve\n\n# Install the AWS Load Balancer Controller using Helm\nhelm repo add eks https://aws.github.io/eks-charts\nhelm repo update\n\n# Install the CRDs\nkubectl apply -f https://raw.githubusercontent.com/aws/eks-charts/master/stable/aws-load-balancer-controller/crds/crds.yaml\n\n# Install the controller\nhelm install aws-load-balancer-controller eks/aws-load-balancer-controller \\\n  -n kube-system \\\n  --set clusterName=vllm-cluster \\\n  --set serviceAccount.create=false \\\n  --set serviceAccount.name=aws-load-balancer-controller\n\n# Install the LeaderWorkerSet controller\nhelm install lws oci://registry.k8s.io/lws/charts/lws \\\n  --version=0.6.1 \\\n  --namespace lws-system \\\n  --create-namespace \\\n  --wait --timeout 300s\n</code></pre> <p>Verify that the AWS Load Balancer Controller is running:</p> <pre><code># Check AWS Load Balancer Controller pods\nkubectl get pods -n kube-system | grep aws-load-balancer-controller\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/#configure-security-groups-for-the-alb","title":"Configure security groups for the ALB","text":"<p>We create a dedicated security group for the ALB:</p> <pre><code># Create security group for the ALB\nUSER_IP=$(curl -s https://checkip.amazonaws.com)\n\nVPC_ID=$(aws --profile vllm-profile eks describe-cluster --name vllm-cluster \\\n  --query \"cluster.resourcesVpcConfig.vpcId\" --output text)\n\nALB_SG=$(aws --profile vllm-profile ec2 create-security-group \\\n  --group-name vllm-alb-sg \\\n  --description \"Security group for vLLM ALB\" \\\n  --vpc-id $VPC_ID \\\n  --query \"GroupId\" --output text)\n\necho \"ALB security group: $ALB_SG\"\n\n# Allow inbound traffic on port 80 from your IP\naws --profile vllm-profile ec2 authorize-security-group-ingress \\\n  --group-id $ALB_SG \\\n  --protocol tcp \\\n  --port 80 \\\n  --cidr ${USER_IP}/32\n\n# Get the node group security group ID\nNODE_INSTANCE_ID=$(aws --profile vllm-profile ec2 describe-instances \\\n  --filters \"Name=tag:eks:nodegroup-name,Values=vllm-p4d-nodes-efa\" \\\n  --query \"Reservations[0].Instances[0].InstanceId\" --output text)\n\nNODE_SG=$(aws --profile vllm-profile ec2 describe-instances \\\n  --instance-ids $NODE_INSTANCE_ID \\\n  --query \"Reservations[0].Instances[0].SecurityGroups[0].GroupId\" --output text)\n\necho \"Node security group: $NODE_SG\"\n\n# Allow traffic from ALB security group to node security group on port 8000 (vLLM service port)\naws --profile vllm-profile ec2 authorize-security-group-ingress \\\n  --group-id $NODE_SG \\\n  --protocol tcp \\\n  --port 8000 \\\n  --source-group $ALB_SG\n\n# Update the security group in the ingress file\nsed -i \"s|&lt;sg-id&gt;|$ALB_SG|g\" vllm-deepseek-32b-lws-ingress.yaml\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/#deploy-the-vllm-server","title":"Deploy the vLLM server","text":"<p>Finally, we deploy the vLLM server using the LeaderWorkerSet pattern:</p> <pre><code># Deploy the vLLM server\n# First, verify that the AWS Load Balancer Controller is running\nkubectl get pods -n kube-system | grep aws-load-balancer-controller\n\n# Wait until the controller is in Running state\n# If it's not running, check the logs:\n# kubectl logs -n kube-system deployment/aws-load-balancer-controller\n\n# Apply the LeaderWorkerSet\nkubectl apply -f vllm-deepseek-32b-lws.yaml\n</code></pre> <p>Timeline: The deployment will start immediately, but the pod might remain in ContainerCreating state for several minutes (5-15 minutes) while it pulls the large GPU-enabled container image. After the container starts, it will take additional time (10-15 minutes) to download and load the DeepSeek model.</p> <p>You can monitor the progress with:</p> <pre><code># Monitor pod status\nkubectl get pods\n\n# Check pod logs\nkubectl logs -f &lt;pod-name&gt;\n</code></pre> <p>We also deploy an ingress resource that configures the ALB to route traffic to our vLLM service:</p> <pre><code># Apply the ingress (only after the controller is running)\nkubectl apply -f vllm-deepseek-32b-lws-ingress.yaml\n</code></pre> <p>You can check the status of the ingress with:</p> <pre><code># Check ingress status\nkubectl get ingress\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/#test-the-deployment","title":"Test the deployment","text":"<p>When the deployment is complete, we can test our vLLM server:</p> <pre><code># Test the vLLM server\n# Get the ALB endpoint\nexport VLLM_ENDPOINT=$(kubectl get ingress vllm-deepseek-32b-lws-ingress -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\necho \"vLLM endpoint: $VLLM_ENDPOINT\"\n\n# Test the completions API\ncurl -X POST http://$VLLM_ENDPOINT/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n      \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\",\n      \"prompt\": \"Hello, how are you?\",\n      \"max_tokens\": 100,\n      \"temperature\": 0.7\n  }'\n</code></pre> <p>You can also test the chat completions API:</p> <pre><code># Test the chat completions API\ncurl -X POST http://$VLLM_ENDPOINT/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n      \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\",\n      \"messages\": [{\"role\": \"user\", \"content\": \"What are the benefits of using FSx Lustre with EKS?\"}],\n      \"max_tokens\": 100,\n      \"temperature\": 0.7\n  }'\n</code></pre> <p>Note: The ALB creation typically takes 2-5 minutes to provision and become available with a DNS name.</p> <p>The vLLM server provides several API endpoints compatible with the OpenAI API: - <code>/v1/completions</code> - For text completions - <code>/v1/chat/completions</code> - For chat completions - <code>/v1/embeddings</code> - For generating embeddings - <code>/v1/models</code> - For listing available models</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/#cleanup","title":"Cleanup","text":"<p>You can clean up all resources created in this deployment using either the provided cleanup script or by following the detailed manual steps below.</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/#using-the-cleanup-script","title":"Using the Cleanup Script","text":"<p>For a simplified cleanup process, you can use the provided cleanup.sh script:</p> <pre><code># Make the script executable\nchmod +x cleanup.sh\n\n# Run the cleanup script\n./cleanup.sh\n</code></pre> <p>This script will automatically delete all resources in the correct order, with appropriate wait times between steps to ensure proper deletion. The script handles: - Kubernetes resources (ingress, LeaderWorkerSet, PVC, PV, AWS Load Balancer Controller) - IAM resources (service accounts and policies) - FSx for Lustre file system - Security groups - EKS node group - EKS cluster</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/#manual-cleanup-steps","title":"Manual Cleanup Steps","text":"<p>If you prefer to clean up resources manually or need more control over the process, follow these steps in order:</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/#1-delete-kubernetes-resources","title":"1. Delete Kubernetes Resources","text":"<pre><code># Delete the vLLM ingress\nkubectl delete -f vllm-deepseek-32b-lws-ingress.yaml\n\n# Delete the vLLM LeaderWorkerSet\nkubectl delete -f vllm-deepseek-32b-lws.yaml\n\n# Delete the FSx Lustre PVC and PV\nkubectl delete -f fsx-lustre-pvc.yaml\nkubectl delete -f fsx-lustre-pv.yaml\n\n# Delete the storage class\nkubectl delete -f fsx-storage-class.yaml\n\n# Delete the AWS Load Balancer Controller\nhelm uninstall aws-load-balancer-controller -n kube-system\n\n# Verify all Kubernetes resources are deleted\nkubectl get pods,svc,ingress,pv,pvc\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/#2-delete-the-iam-service-account-cloudformation-stack","title":"2. Delete the IAM Service Account CloudFormation Stack","text":"<pre><code># Delete the CloudFormation stack for the IAM service account\naws --profile vllm-profile cloudformation delete-stack --stack-name eksctl-vllm-cluster-addon-iamserviceaccount-kube-system-aws-load-balancer-controller\n\n# Verify the stack is being deleted\naws --profile vllm-profile cloudformation describe-stacks --stack-name eksctl-vllm-cluster-addon-iamserviceaccount-kube-system-aws-load-balancer-controller --query \"Stacks[0].StackStatus\" --output text || echo \"Stack deleted\"\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/#3-delete-the-iam-policy","title":"3. Delete the IAM Policy","text":"<pre><code># Get the ARN of the IAM policy\nPOLICY_ARN=$(aws --profile vllm-profile iam list-policies --query \"Policies[?PolicyName=='AWSLoadBalancerControllerIAMPolicy'].Arn\" --output text)\n\n# Delete the IAM policy\naws --profile vllm-profile iam delete-policy --policy-arn $POLICY_ARN\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/#4-delete-the-fsx-lustre-filesystem","title":"4. Delete the FSx Lustre Filesystem","text":"<pre><code># Get the FSx Lustre filesystem ID if not already set\nif [ -z \"$FSX_ID\" ]; then\n  FSX_ID=$(aws --profile vllm-profile fsx describe-file-systems --query \"FileSystems[?Tags[?Key=='Name' &amp;&amp; Value=='vllm-model-storage']].FileSystemId\" --output text)\n  echo \"Found FSx Lustre filesystem ID: $FSX_ID\"\nfi\n\n# Delete the FSx Lustre filesystem\n# Timeline: FSx Lustre deletion takes 5-10 minutes\naws --profile vllm-profile fsx delete-file-system --file-system-id $FSX_ID\n\n# Verify the filesystem is deleted\naws --profile vllm-profile fsx describe-file-systems --file-system-id $FSX_ID || echo \"Filesystem deleted\"\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/#5-check-for-any-remaining-load-balancers","title":"5. Check for Any Remaining Load Balancers","text":"<pre><code># Check for any remaining ALBs or NLBs\naws --profile vllm-profile elbv2 describe-load-balancers\n\n# Check for any remaining Classic ELBs\naws --profile vllm-profile elb describe-load-balancers\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/#6-delete-the-node-group","title":"6. Delete the Node Group","text":"<p>It's recommended to delete the node group separately before deleting the cluster, especially when using p4d.24xlarge instances with EFA and system pods that may be difficult to evict:</p> <pre><code># First, check the status of the node group\neksctl get nodegroup --cluster=vllm-cluster --region=us-west-2 --profile=vllm-profile\n\n# Delete the node group without draining (recommended approach)\n# This skips the pod eviction process which can get stuck with system pods\neksctl delete nodegroup --cluster=vllm-cluster --name=vllm-p4d-nodes-efa --region=us-west-2 --profile=vllm-profile --drain=false\n\n# Verify the node group deletion is in progress\naws --profile vllm-profile cloudformation list-stacks --stack-status-filter DELETE_IN_PROGRESS --query \"StackSummaries[?contains(StackName, 'vllm-p4d-nodes-efa')].{Name:StackName, Status:StackStatus}\" --output table\n\n# Verify the node group is deleted (this command should return \"Node group deleted\" when complete)\neksctl get nodegroup --cluster=vllm-cluster --name=vllm-p4d-nodes-efa --region=us-west-2 --profile=vllm-profile 2&gt;&amp;1 | grep -q \"not found\" &amp;&amp; echo \"Node group deleted\"\n</code></pre> <p>Note: Using the <code>--drain=false</code> flag is important when deleting node groups with p4d.24xlarge instances, as system pods like EFA device plugins, NVIDIA device plugins, and CSI drivers may prevent normal draining operations.</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/#7-delete-the-security-groups","title":"7. Delete the Security Groups","text":"<pre><code># Delete the security group created for the ALB\naws --profile vllm-profile ec2 delete-security-group --group-id $ALB_SG\n\n# Verify the ALB security group is deleted\naws --profile vllm-profile ec2 describe-security-groups --group-ids $ALB_SG 2&gt;&amp;1 | grep -q \"InvalidGroup.NotFound\" &amp;&amp; echo \"ALB security group deleted\"\n\n# Delete the security group created for FSx Lustre\naws --profile vllm-profile ec2 delete-security-group --group-id $SG_ID\n\n# Verify the FSx security group is deleted\naws --profile vllm-profile ec2 describe-security-groups --group-ids $SG_ID 2&gt;&amp;1 | grep -q \"InvalidGroup.NotFound\" &amp;&amp; echo \"FSx security group deleted\"\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/#8-delete-the-eks-cluster","title":"8. Delete the EKS Cluster","text":"<pre><code># Delete the EKS cluster\n# Timeline: EKS cluster deletion takes 10-15 minutes\neksctl delete cluster --name=vllm-cluster --region=us-west-2 --profile=vllm-profile\n\n# Verify the cluster is deleted\naws --profile vllm-profile eks describe-cluster --name vllm-cluster 2&gt;&amp;1 | grep -q \"ResourceNotFoundException\" &amp;&amp; echo \"Cluster deleted\"\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/#9-troubleshooting-cloudformation-stack-deletion-failures","title":"9. Troubleshooting CloudFormation Stack Deletion Failures","text":"<p>EKS cluster CloudFormation stack deletion often fails on the VPC deletion step. Common issues include pending default security group cleanup and route table dependencies. Follow these steps to troubleshoot and resolve these issues:</p> <pre><code># Check the status of the CloudFormation stacks\naws --profile vllm-profile cloudformation list-stacks --stack-status-filter DELETE_FAILED --query \"StackSummaries[?contains(StackName, 'vllm-cluster')].{Name:StackName, Status:StackStatus}\" --output table\n\n# Identify the resources that failed to delete\naws --profile vllm-profile cloudformation describe-stacks --stack-name &lt;stack-name&gt; --query \"Stacks[0].StackStatusReason\" --output text\n\n# Get the VPC ID\nVPC_ID=$(aws --profile vllm-profile ec2 describe-vpcs --filters \"Name=tag:Name,Values=*vllm-cluster*\" --query \"Vpcs[0].VpcId\" --output text)\n\n# Check for network interfaces still attached to the VPC (these prevent deletion)\naws --profile vllm-profile ec2 describe-network-interfaces --filters \"Name=vpc-id,Values=$VPC_ID\" --query \"NetworkInterfaces[*].[NetworkInterfaceId, SubnetId, Description]\" --output table\n\n# For VPC endpoint issues, identify and delete VPC endpoints\nENDPOINT_IDS=$(aws --profile vllm-profile ec2 describe-vpc-endpoints --filters \"Name=vpc-id,Values=$VPC_ID\" --query \"VpcEndpoints[*].VpcEndpointId\" --output text)\n\n# Delete VPC endpoints\nfor endpoint in $ENDPOINT_IDS; do\n  aws --profile vllm-profile ec2 delete-vpc-endpoints --vpc-endpoint-ids $endpoint\ndone\n\n# Check for default security group rules that need to be removed\nDEFAULT_SG=$(aws --profile vllm-profile ec2 describe-security-groups --filters \"Name=vpc-id,Values=$VPC_ID\" \"Name=group-name,Values=default\" --query \"SecurityGroups[0].GroupId\" --output text)\n\n# Remove all ingress and egress rules from the default security group\naws --profile vllm-profile ec2 revoke-security-group-ingress --group-id $DEFAULT_SG --protocol all --source-group $DEFAULT_SG || echo \"No ingress rules to delete\"\naws --profile vllm-profile ec2 revoke-security-group-egress --group-id $DEFAULT_SG --protocol all --cidr 0.0.0.0/0 || echo \"No egress rules to delete\"\n\n# Check for route tables with dependencies\naws --profile vllm-profile ec2 describe-route-tables --filters \"Name=vpc-id,Values=$VPC_ID\" --query \"RouteTables[*].[RouteTableId,Associations[*].RouteTableAssociationId]\" --output table\n\n# Disassociate route tables if needed\n# aws --profile vllm-profile ec2 disassociate-route-table --association-id rtbassoc-XXXXXXXXXXXXXXXXX\n\n# Retry the CloudFormation stack deletion\naws --profile vllm-profile cloudformation delete-stack --stack-name &lt;stack-name&gt;\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/#10-verify-all-resources-are-deleted","title":"10. Verify All Resources Are Deleted","text":"<pre><code># Verify no EKS clusters remain\naws --profile vllm-profile eks list-clusters\n\n# Verify no FSx Lustre filesystems remain\naws --profile vllm-profile fsx describe-file-systems\n\n# Verify no CloudFormation stacks remain\naws --profile vllm-profile cloudformation list-stacks --stack-status-filter CREATE_COMPLETE UPDATE_COMPLETE DELETE_FAILED --query \"StackSummaries[?contains(StackName, 'vllm-cluster')].{Name:StackName, Status:StackStatus}\" --output table\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/#monitoring","title":"Monitoring","text":"<p>You can monitor the deployment with:</p> <pre><code># Check if the pods are running\nkubectl get pods\n\n# Check the logs of the vLLM pod\nkubectl logs -f daemonset/vllm-deepseek-32b\n\n# Get the ALB endpoint\nkubectl get ingress vllm-deepseek-32b-ingress\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/#notes","title":"Notes","text":"<ul> <li>The deployment uses p4d.24xlarge instances with NVIDIA A100 GPUs</li> <li>The nodes are configured with Elastic Fabric Adapter (EFA) for high-performance networking between nodes</li> <li>The vLLM server is configured to use 85% of the GPU memory. You can adjust this in the <code>vllm-deepseek-32b-lws.yaml</code> file.</li> <li>The service is exposed using an Application Load Balancer (ALB) through the AWS Load Balancer Controller. This provides better integration with AWS services and supports features like path-based routing and SSL termination.</li> <li>The max model length is set to 4096 tokens to fit in the GPU memory. You can adjust this in the <code>vllm-deepseek-32b-lws.yaml</code> file.</li> <li>The FSx Lustre filesystem is used to store the model weights, which provides faster loading times and persistent storage.</li> <li>The nodegroup uses an EKS-optimized AMI with GPU support which already includes the NVIDIA device plugin, eliminating the need for a separate Helm installation.</li> </ul>"},{"location":"tutorials/vllm-samples/deepseek/eks/#leaderworkerset-implementation","title":"LeaderWorkerSet Implementation","text":"<p>We've implemented the LeaderWorkerSet (LWS) pattern to fix the vLLM node parallelism issue. This approach provides several advantages over the DaemonSet approach:</p> <ol> <li>Better Control Over Pod Placement: LWS allows us to explicitly define which nodes should run the leader and worker pods.</li> <li>Improved Ray Cluster Formation: The leader pod starts the Ray head node, and worker pods connect to it using the leader's address.</li> <li>Proper Pipeline Parallelism: With LWS, we can configure tensor parallelism (TP=8) and pipeline parallelism (PP=2) to utilize both nodes effectively.</li> </ol>"},{"location":"tutorials/vllm-samples/deepseek/eks/#key-configuration-files","title":"Key Configuration Files","text":"<ul> <li><code>vllm-deepseek-32b-lws.yaml</code>: Contains the LeaderWorkerSet configuration with leader and worker templates</li> <li><code>vllm-deepseek-32b-lws-ingress.yaml</code>: Contains the ingress configuration for the ALB</li> </ul>"},{"location":"tutorials/vllm-samples/deepseek/eks/#ray-cluster-configuration","title":"Ray Cluster Configuration","text":"<p>The Ray cluster is configured as follows: - Leader node: Starts Ray with <code>--head --port=6379 --num-cpus=48 --num-gpus=8</code> - Worker node: Connects to the leader with <code>--address=$(LWS_LEADER_ADDRESS):6379 --num-cpus=48 --num-gpus=8</code></p>"},{"location":"tutorials/vllm-samples/deepseek/eks/#vllm-server-configuration","title":"vLLM Server Configuration","text":"<p>The vLLM server is configured with: - Tensor parallelism: 8 (utilizing all GPUs on a single node) - Pipeline parallelism: 2 (distributing the model across 2 nodes) - GPU memory utilization: 85% - Max model length: 4096 tokens</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/#troubleshooting-aws-load-balancer-controller","title":"Troubleshooting AWS Load Balancer Controller","text":"<p>If you encounter permission issues with the AWS Load Balancer Controller, check: 1. The IAM role ARN in the service account annotation 2. The IAM role ARN in the pod environment variables 3. The trust relationship policy for the IAM role</p> <p>If there's a mismatch, restart the AWS Load Balancer Controller pods: <pre><code>kubectl rollout restart deployment -n kube-system aws-load-balancer-controller\n</code></pre></p>"},{"location":"tutorials/vllm-samples/deepseek/eks/#elastic-fabric-adapter-efa-configuration","title":"Elastic Fabric Adapter (EFA) Configuration","text":"<p>This deployment uses Amazon's Elastic Fabric Adapter (EFA) technology, which is a network interface for Amazon EC2 instances that enables high-performance computing and machine learning applications to scale efficiently. EFA provides lower and more consistent latency and higher throughput than the TCP transport traditionally used in cloud-based HPC systems.</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/#efa-setup-in-the-nodegroup","title":"EFA Setup in the Nodegroup","text":"<p>The <code>large-model-nodegroup.yaml</code> file (included in this repository) configures the p4d.24xlarge instances with EFA support:</p> <pre><code># Enable EFA interfaces\nefaEnabled: true\n\n# Add bootstrap commands to install EFA drivers\npreBootstrapCommands:\n  - |\n    #!/bin/bash\n    set -ex\n\n    # Install EFA driver and related packages\n    curl -O https://efa-installer.amazonaws.com/aws-efa-installer-latest.tar.gz\n    tar -xf aws-efa-installer-latest.tar.gz\n    cd aws-efa-installer\n    ./efa_installer.sh -y\n\n    # Configure NCCL to use EFA\n    echo \"export FI_PROVIDER=efa\" &gt;&gt; /etc/environment\n    echo \"export FI_EFA_USE_DEVICE_RDMA=1\" &gt;&gt; /etc/environment\n    echo \"export NCCL_DEBUG=INFO\" &gt;&gt; /etc/environment\n</code></pre> <p>The EFA installation process: 1. Downloads and installs the EFA installer 2. Runs the installer with the <code>-y</code> flag to automatically accept all prompts 3. Configures environment variables to enable NCCL (NVIDIA Collective Communications Library) to use EFA for GPU-to-GPU communication</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/#benefits-of-efa-for-large-language-models","title":"Benefits of EFA for Large Language Models","text":"<ul> <li>Reduced latency: EFA provides lower and more consistent latency for communication between GPUs across nodes</li> <li>Higher throughput: EFA enables higher throughput for data transfer between nodes</li> <li>Improved scaling: EFA allows large language models to scale more efficiently across multiple nodes</li> <li>Better performance: EFA can significantly improve the performance of distributed training and inference workloads</li> </ul>"},{"location":"tutorials/vllm-samples/deepseek/eks/#advanced-configuration-using-application-load-balancer-alb","title":"Advanced Configuration: Using Application Load Balancer (ALB)","text":"<p>We've configured the vLLM service to use an Application Load Balancer (ALB) through the AWS Load Balancer Controller. This provides several advantages over a standard Kubernetes LoadBalancer service:</p> <ol> <li>Path-based routing: ALB supports routing traffic to different services based on the URL path</li> <li>SSL/TLS termination: ALB can handle SSL/TLS termination, reducing the load on your pods</li> <li>Authentication: ALB supports authentication through Amazon Cognito or OIDC</li> <li>Web Application Firewall (WAF): ALB can be integrated with AWS WAF for additional security</li> <li>Access Logs: ALB can log all requests to an S3 bucket for auditing and analysis</li> </ol> <p>The ALB is configured through the <code>vllm-deepseek-32b-lws-ingress</code> file, which includes annotations for the AWS Load Balancer Controller. You can customize the ALB configuration by modifying these annotations.</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/#security-group-configuration","title":"Security Group Configuration","text":"<p>The ALB security group must allow inbound traffic on port 80 (and 443 if using HTTPS) from the clients that will access the vLLM service. You can update the security group configuration in the <code>vllm-deepseek-32b-lws-ingress</code> file:</p> <pre><code>alb.ingress.kubernetes.io/security-groups: &lt;sg-id&gt;\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/#ssltls-configuration","title":"SSL/TLS Configuration","text":"<p>To enable HTTPS, you can add the following annotations to the <code>vllm-deepseek-32b-lws-ingress</code> file:</p> <pre><code>alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}, {\"HTTPS\": 443}]'\nalb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:region:account-id:certificate/certificate-id\nalb.ingress.kubernetes.io/ssl-policy: ELBSecurityPolicy-2016-08\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/#access-logs","title":"Access Logs","text":"<p>To enable access logs, add the following annotation to the <code>vllm-deepseek-32b-lws-ingress</code> file:</p> <pre><code>alb.ingress.kubernetes.io/load-balancer-attributes: access_logs.s3.enabled=true,access_logs.s3.bucket=your-bucket-name,access_logs.s3.prefix=your-prefix\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/#fsx-lustre-integration-benefits","title":"FSx Lustre Integration Benefits","text":"<ol> <li>Persistent Storage: Model weights are stored on the FSx Lustre filesystem and persist across pod restarts</li> <li>Faster Loading: After the initial download, model loading will be much faster</li> <li>Shared Storage: Multiple pods can access the same model weights</li> <li>High Performance: FSx Lustre provides high-throughput, low-latency access to the model weights</li> </ol>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/","title":"\ud83d\ude80 AI-Powered Fraud Detection with vLLM on EKS","text":""},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#financial-fraud-detection-sample-application","title":"Financial Fraud Detection Sample Application","text":"<p>This is a complete working sample demonstrating real-time financial fraud detection using vLLM on Amazon EKS with AI agents and microservices.</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#overview","title":"\ud83d\udccb Overview","text":"<p>This sample showcases an AI agent system for real-time financial fraud detection, combining:</p> <ul> <li>\ud83e\udd16 DeepSeek R1 32B - Advanced reasoning model deployed via vLLM on Amazon EKS</li> <li>\u26a1 AWS Deep Learning Containers - Pre-optimized vLLM container images</li> <li>\ud83d\udd27 MCP (Model Context Protocol) - Microservices architecture providing AI tools</li> <li>\ud83c\udfa8 Streamlit UI - Interactive web interface for fraud analysis</li> <li>\u2601\ufe0f Amazon EKS - Scalable GPU infrastructure (p4d.24xlarge with EFA)</li> <li>\ud83d\udce6 Amazon ECS Fargate - Serverless microservices orchestration</li> </ul> <p></p>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#use-case-real-time-transaction-fraud-detection","title":"\ud83c\udfaf Use Case: Real-Time Transaction Fraud Detection","text":""},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#business-problem","title":"Business Problem","text":"<p>Financial institutions process millions of transactions daily and need: - Real-time fraud detection at scale - Complex decision-making requiring multiple data sources - Explainable AI for regulatory compliance - Low latency inference (&lt;2 seconds per transaction)</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#demo-scenario","title":"Demo Scenario","text":"<p>Suspicious Transaction: <pre><code>Transaction ID: TXN-20251024-191354\nAmount: $4,500\nMerchant: CRYPTO-EXCHANGE-XX\nLocation: Moscow, Russia\nPrevious Location: New York, NY (30 minutes ago)\nCard Present: No\nDevice: Unknown device fingerprint\n</code></pre></p> <p>AI Agent Analysis: 1. \u2705 Transaction Risk Assessment - Analyzes amount, merchant, location 2. \u2705 Identity Verification - Validates device fingerprint &amp; customer history 3. \u2705 Geolocation Check - Detects impossible travel patterns 4. \u2705 Real-time Alerts - Sends notifications to fraud team 5. \u2705 Case Logging - Records incident for investigation 6. \u2705 Report Generation - Creates compliance documentation</p> <p>Result: High-risk transaction blocked in &lt;2 seconds with full audit trail</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#architecture-components","title":"\ud83c\udfd7\ufe0f Architecture Components","text":""},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#1-vllm-on-amazon-eks","title":"1. vLLM on Amazon EKS","text":"<ul> <li>Model: DeepSeek-R1-Distill-Qwen-32B</li> <li>Infrastructure: 2x p4d.24xlarge (16 GPUs total)</li> <li>Configuration: Tensor Parallelism (TP=8), Pipeline Parallelism (PP=2)</li> <li>Networking: EFA for low-latency multi-node communication</li> <li>Storage: FSx Lustre for fast model loading</li> <li>Container: AWS DLC vLLM 0.8.5 GPU optimized</li> </ul>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#2-mcp-microservices-6-tools","title":"2. MCP Microservices (6 Tools)","text":"<p>Each service runs as a serverless container on Amazon ECS Fargate:</p> <ol> <li>transaction-risk - Risk scoring algorithms</li> <li>identity-verifier - Biometric/device verification</li> <li>geolocation-checker - Location intelligence &amp; travel analysis</li> <li>email-alerts - Real-time notification system</li> <li>fraud-logger - Case management &amp; audit trail</li> <li>report-generator - Compliance &amp; analytics reports</li> </ol>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#3-streamlit-ui","title":"3. Streamlit UI","text":"<ul> <li>Interactive web interface for fraud analysts</li> <li>Real-time visualization of AI reasoning</li> <li>Transaction submission and analysis</li> <li>Results dashboard with risk scores</li> </ul>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#performance-metrics","title":"\ud83d\udcca Performance Metrics","text":"<p>From the live demo:</p> Metric Result Simple Question Latency 1.28s Fraud Detection Latency 1.62s Complex Analysis Latency 2.34s Throughput 1000+ TPS GPU Utilization 75-85% Cost per 1M Transactions ~$50 <p>Business Impact: - 95% detection rate (vs 70% rule-based) - 2% false positive rate (vs 15% traditional) - 80% reduction in customer friction - $13M annual net benefit</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#prerequisites","title":"Prerequisites","text":"<p>Complete the basic vLLM deployment first: \ud83d\udc49 Follow the main EKS README to deploy: - EKS cluster with GPU nodes - vLLM server with DeepSeek R1 32B - ALB endpoint</p> <p>Time Required: ~45 minutes for basic setup + 30 minutes for fraud detection demo</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#step-1-verify-vllm-is-running","title":"Step 1: Verify vLLM is Running","text":"<pre><code># Get your vLLM endpoint\nexport VLLM_ENDPOINT=$(kubectl get ingress vllm-deepseek-32b-lws-ingress -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\necho \"vLLM endpoint: $VLLM_ENDPOINT\"\n\n# Test the endpoint\ncurl -X POST http://$VLLM_ENDPOINT/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n      \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\",\n      \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}],\n      \"max_tokens\": 50\n  }'\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#deploy-mcp-microservices-on-ecs","title":"\ud83d\udce6 Deploy MCP Microservices on ECS","text":""},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#step-2-set-up-aws-environment","title":"Step 2: Set Up AWS Environment","text":"<pre><code># Set your AWS profile\nexport AWS_PROFILE=vllm-profile\nexport AWS_REGION=us-west-2\n\n# Get your AWS account ID\nexport AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\necho \"AWS Account: $AWS_ACCOUNT_ID\"\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#step-3-create-ecr-repositories","title":"Step 3: Create ECR Repositories","text":"<pre><code># Create ECR repository for MCP servers\naws ecr create-repository --repository-name mcp-fraud-detection --region $AWS_REGION\n\n# Create ECR repository for UI\naws ecr create-repository --repository-name fraud-detection-ui --region $AWS_REGION\n\n# Login to ECR\naws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#step-4-build-and-push-docker-images","title":"Step 4: Build and Push Docker Images","text":"<pre><code>cd fraud-detection-demo\n\n# Build MCP servers image\ndocker build -t mcp-fraud-detection:latest -f mcp-servers/Dockerfile mcp-servers/\n\n# Tag and push\ndocker tag mcp-fraud-detection:latest $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/mcp-fraud-detection:latest\ndocker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/mcp-fraud-detection:latest\n\n# Build UI image\ndocker build -t fraud-detection-ui:latest -f ui/Dockerfile ui/\n\n# Tag and push\ndocker tag fraud-detection-ui:latest $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/fraud-detection-ui:latest\ndocker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/fraud-detection-ui:latest\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#step-5-deploy-ecs-services","title":"Step 5: Deploy ECS Services","text":"<pre><code># Update the deployment script with your details\nexport VLLM_ENDPOINT=\"http://$(kubectl get ingress vllm-deepseek-32b-lws-ingress -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\"\n\n# Deploy all MCP services and UI\ncd scripts\nchmod +x deploy-ecs-fraud-detection.sh\n./deploy-ecs-fraud-detection.sh\n</code></pre> <p>This script will: 1. Create ECS cluster 2. Create task definitions for all 6 MCP servers 3. Create task definition for Streamlit UI 4. Deploy all services on Fargate 5. Set up Application Load Balancer 6. Configure security groups</p> <p>Timeline: ~10-15 minutes</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#step-6-get-the-ui-endpoint","title":"Step 6: Get the UI Endpoint","text":"<pre><code># Get the UI ALB endpoint\naws elbv2 describe-load-balancers \\\n  --names fraud-detection-ui-alb \\\n  --query 'LoadBalancers[0].DNSName' \\\n  --output text\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#testing-the-demo","title":"\ud83c\udfae Testing the Demo","text":""},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#access-the-ui","title":"Access the UI","text":"<ol> <li>Open your browser to the ALB endpoint from Step 6</li> <li>You should see the Financial Fraud Detection AI Agent interface</li> </ol>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#test-scenario-1-low-risk-transaction","title":"Test Scenario 1: Low-Risk Transaction","text":"<pre><code>{\n  \"transaction_id\": \"TXN-20251201-100000\",\n  \"customer_id\": \"C-12345\",\n  \"amount\": 250.00,\n  \"merchant\": \"Amazon.com\",\n  \"merchant_category\": \"E-commerce\",\n  \"location\": \"Seattle, WA\",\n  \"card_present\": false,\n  \"device_id\": \"dev-abc123\",\n  \"ip_address\": \"192.168.1.100\",\n  \"previous_location\": \"Seattle, WA\",\n  \"time_since_last_transaction\": \"2 hours ago\"\n}\n</code></pre> <p>Expected: \u2705 Low risk, transaction approved</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#test-scenario-2-high-risk-transaction-demo-scenario","title":"Test Scenario 2: High-Risk Transaction (Demo Scenario)","text":"<pre><code>{\n  \"transaction_id\": \"TXN-20251201-191354\",\n  \"customer_id\": \"C-12345\",\n  \"amount\": 4500.00,\n  \"merchant\": \"CRYPTO-EXCHANGE-XX\",\n  \"merchant_category\": \"Cryptocurrency\",\n  \"location\": \"Moscow, Russia\",\n  \"card_present\": false,\n  \"device_id\": \"dev-unknown\",\n  \"ip_address\": \"185.220.101.5\",\n  \"previous_location\": \"New York, NY\",\n  \"time_since_last_transaction\": \"30 minutes ago\"\n}\n</code></pre> <p>Expected: \ud83d\udea8 High risk (95/100), transaction blocked, alerts sent</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#test-scenario-3-medium-risk-transaction","title":"Test Scenario 3: Medium-Risk Transaction","text":"<pre><code>{\n  \"transaction_id\": \"TXN-20251201-150000\",\n  \"customer_id\": \"C-67890\",\n  \"amount\": 1200.00,\n  \"merchant\": \"Best Buy\",\n  \"merchant_category\": \"Electronics\",\n  \"location\": \"Los Angeles, CA\",\n  \"card_present\": true,\n  \"device_id\": \"dev-xyz789\",\n  \"ip_address\": \"192.168.1.50\",\n  \"previous_location\": \"Los Angeles, CA\",\n  \"time_since_last_transaction\": \"1 day ago\"\n}\n</code></pre> <p>Expected: \u26a0\ufe0f Medium risk, manual review recommended</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#understanding-the-ai-agent-flow","title":"\ud83d\udd0d Understanding the AI Agent Flow","text":"<p>When you submit a transaction, watch the Reasoning Process section in the UI:</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#step-1-initial-analysis-deepseek-r1-reasoning","title":"Step 1: Initial Analysis (DeepSeek R1 Reasoning)","text":"<pre><code>&lt;think&gt;\nAnalyzing transaction for customer C-12345...\nAmount: $4,500 - Higher than typical ($500-1000)\nMerchant: Cryptocurrency exchange - High-risk category\nLocation: Moscow, Russia\nPrevious location: New York, NY (30 min ago)\nRed flag: Impossible travel detected\n&lt;/think&gt;\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#step-2-tool-execution","title":"Step 2: Tool Execution","text":"<p>The AI agent automatically calls MCP tools:</p> <ol> <li>transaction-risk \u2192 Risk Score: 85/100</li> <li>identity-verifier \u2192 Device: UNKNOWN (Risk +10)</li> <li>geolocation-checker \u2192 Impossible travel: 4,600 miles in 30 min</li> <li>email-alerts \u2192 Alert sent to fraud@company.com</li> <li>fraud-logger \u2192 Case #FRD-2025-0001 created</li> <li>report-generator \u2192 Compliance report generated</li> </ol>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#step-3-final-decision","title":"Step 3: Final Decision","text":"<pre><code>Risk Score: 95/100\nDecision: BLOCK TRANSACTION\nReason: Impossible travel + Unknown device + High-risk merchant\nActions Taken:\n  \u2713 Transaction blocked\n  \u2713 Customer SMS sent\n  \u2713 Account frozen for 24h\n  \u2713 Fraud team alerted\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#additional-documentation","title":"\ud83d\udcda Additional Documentation","text":"<ul> <li>ECS Deployment Guide - Detailed ECS setup instructions</li> </ul>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#recorded-demo","title":"\ud83c\udfa5 Recorded Demo","text":"<p>Watch the full Re:Invent 2025 presentation: \ud83d\udc49 [Link to be added after event]</p> <p>Key Timestamps: - 00:00 - Introduction &amp; Business Problem - 05:00 - Architecture Overview - 10:00 - Live Demo Walkthrough - 20:00 - Performance Metrics - 25:00 - Production Considerations - 30:00 - Q&amp;A</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#customization-guide","title":"\ud83d\udee0\ufe0f Customization Guide","text":""},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#modify-risk-scoring-logic","title":"Modify Risk Scoring Logic","text":"<p>Edit <code>mcp-servers/transaction-risk/server.py</code>:</p> <pre><code>def calculate_risk_score(transaction):\n    risk_score = 0\n\n    # Amount-based risk\n    if transaction[\"amount\"] &gt; 5000:\n        risk_score += 30\n    elif transaction[\"amount\"] &gt; 2000:\n        risk_score += 20\n\n    # Add your custom logic here\n    if transaction[\"merchant_category\"] == \"Cryptocurrency\":\n        risk_score += 25\n\n    return risk_score\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#add-new-mcp-tools","title":"Add New MCP Tools","text":"<ol> <li>Create new tool in <code>mcp-servers/your-tool/server.py</code></li> <li>Update <code>scripts/deploy-ecs-services.sh</code></li> <li>Rebuild and redeploy</li> </ol>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#change-the-llm-model","title":"Change the LLM Model","text":"<p>Edit <code>ui/app.py</code> to use a different model:</p> <pre><code># Current: DeepSeek R1 32B\nmodel = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\"\n\n# Alternative: Llama 3\nmodel = \"meta-llama/Llama-3-70b-chat-hf\"\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":""},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#issue-ui-cant-connect-to-vllm","title":"Issue: UI Can't Connect to vLLM","text":"<p>Solution: <pre><code># Check vLLM endpoint\nkubectl get ingress vllm-deepseek-32b-lws-ingress\n\n# Test connectivity\ncurl -X POST http://$VLLM_ENDPOINT/v1/models\n\n# Check UI logs\naws ecs describe-tasks --cluster fraud-detection-cluster \\\n  --tasks $(aws ecs list-tasks --cluster fraud-detection-cluster \\\n  --service-name fraud-detection-ui --query 'taskArns[0]' --output text)\n</code></pre></p>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#issue-mcp-services-not-responding","title":"Issue: MCP Services Not Responding","text":"<p>Solution: <pre><code># Check service status\naws ecs describe-services --cluster fraud-detection-cluster \\\n  --services transaction-risk identity-verifier geolocation-checker\n\n# Check CloudWatch logs\naws logs tail /ecs/mcp-fraud-detection --follow\n</code></pre></p>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#issue-high-latency-5-seconds","title":"Issue: High Latency (&gt;5 seconds)","text":"<p>Possible Causes: 1. Cold start - First request takes longer (warmup) 2. Network latency - Check ALB \u2192 EKS connectivity 3. GPU memory - Model might be swapping</p> <p>Solutions: <pre><code># Check GPU utilization\nkubectl exec -it vllm-deepseek-32b-lws-0 -- nvidia-smi\n\n# Check vLLM logs for OOM errors\nkubectl logs vllm-deepseek-32b-lws-0 | grep -i \"memory\"\n\n# Reduce batch size in vLLM config\nkubectl edit statefulset vllm-deepseek-32b-lws\n</code></pre></p>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#cost-optimization","title":"\ud83d\udcb0 Cost Optimization","text":""},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#current-demo-configuration","title":"Current Demo Configuration","text":"Resource Type Qty Cost/Hour Daily Cost EKS Control Plane Managed 1 $0.10 $2.40 p4d.24xlarge GPU Node 2 $32.77 $1,572.96 FSx Lustre Storage 1.2TB $0.14 $3.36 ECS Fargate vCPU/Memory 7 tasks ~$0.15 ~$3.60 ALB Load Balancer 2 $0.023 $1.10 Data Transfer Regional - ~$0.02 ~$0.50 Total ~$1,584/day"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#cost-reduction-strategies","title":"Cost Reduction Strategies","text":"<ol> <li>Use g6.12xlarge instead of p4d.24xlarge</li> <li>Cost: $5.67/hr vs $32.77/hr (83% reduction)</li> <li>Trade-off: 4 GPUs vs 8 GPUs per node</li> <li> <p>Suitable for: Single-node deployments</p> </li> <li> <p>Use Spot Instances</p> </li> <li>Savings: Up to 70%</li> <li> <p>Trade-off: Potential interruptions</p> </li> <li> <p>Scale Down When Not in Use <pre><code># Scale nodegroup to 0\neksctl scale nodegroup --cluster=vllm-cluster \\\n  --name=vllm-p4d-nodes-efa --nodes=0\n\n# Stop ECS services\naws ecs update-service --cluster fraud-detection-cluster \\\n  --service fraud-detection-ui --desired-count 0\n</code></pre></p> </li> <li> <p>Use Savings Plans</p> </li> <li>1-year commitment: 40% savings</li> <li>3-year commitment: 60% savings</li> </ol>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#cleanup","title":"\ud83e\uddf9 Cleanup","text":""},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#option-1-keep-vllm-remove-demo-only","title":"Option 1: Keep vLLM, Remove Demo Only","text":"<pre><code>cd fraud-detection-demo/scripts\n./cleanup-ecs-only.sh\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#option-2-complete-cleanup-everything","title":"Option 2: Complete Cleanup (Everything)","text":"<pre><code># Delete ECS resources\ncd fraud-detection-demo/scripts\n./cleanup.sh\n\n# Delete EKS cluster (from parent directory)\ncd ../../\n./cleanup.sh\n</code></pre> <p>Estimated Time: 15-20 minutes</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#contributing","title":"\ud83e\udd1d Contributing","text":"<p>This demo is part of the AWS samples repository. To contribute:</p> <ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Make your changes</li> <li>Submit a pull request</li> </ol>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#license","title":"\ud83d\udcc4 License","text":"<p>This sample code is made available under the MIT-0 license. See the LICENSE file.</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#support-questions","title":"\ud83d\ude4b Support &amp; Questions","text":"<ul> <li>GitHub Issues: aws-samples/sample-aws-deep-learning-containers</li> <li>AWS Support: For production deployments, consider AWS Enterprise Support</li> <li>Community: Join the AWS ML Community</li> </ul>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#additional-resources","title":"\ud83d\udcda Additional Resources","text":""},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#aws-documentation","title":"AWS Documentation","text":"<ul> <li>Amazon EKS User Guide</li> <li>AWS Deep Learning Containers</li> <li>Amazon ECS Developer Guide</li> <li>FSx for Lustre</li> </ul>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#vllm-resources","title":"vLLM Resources","text":"<ul> <li>vLLM Official Documentation</li> <li>vLLM GitHub Repository</li> <li>Model Context Protocol</li> </ul>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#blog-posts-tutorials","title":"Blog Posts &amp; Tutorials","text":"<ul> <li>Deploying vLLM on Amazon EKS</li> <li>Building AI Agents with MCP</li> <li>GPU Optimization Best Practices</li> </ul>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#whats-next","title":"\u2728 What's Next?","text":""},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#production-enhancements","title":"Production Enhancements","text":"<ol> <li>Multi-Model Ensemble - Combine multiple models for better accuracy</li> <li>Real-time Streaming - Integrate with Amazon Kinesis</li> <li>Auto-Scaling - Dynamic scaling based on transaction volume</li> <li>Multi-Region - Deploy across regions for disaster recovery</li> <li>Advanced Security - Add AWS WAF, encryption at rest/transit</li> </ol>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/#extended-use-cases","title":"Extended Use Cases","text":"<ul> <li>Credit Card Fraud Detection - Real-time transaction monitoring</li> <li>Insurance Claims Fraud - Automated claim verification</li> <li>Healthcare Billing - Detect anomalous billing patterns</li> <li>E-commerce Fraud - Protect online marketplaces</li> <li>Identity Theft Prevention - Monitor account takeovers</li> </ul> <p>Built with \u2764\ufe0f for Re:Invent 2025</p> <p>Questions? Reach out to the AWS ML team or open an issue!</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/ecs/","title":"ECS Deployment Guide - Financial Fraud Detection System","text":""},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/ecs/#architecture-overview","title":"Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  Internet Gateway                        \u2502\n\u2502                         \u2502                                \u2502\n\u2502                    \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510                          \u2502\n\u2502                    \u2502   ALB   \u2502 (Public subnets)         \u2502\n\u2502                    \u2502         \u2502 (Your IP only)           \u2502\n\u2502                    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518                          \u2502\n\u2502                         \u2502                                \u2502\n\u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502\n\u2502              \u2502                     \u2502                    \u2502\n\u2502         \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510              \u2502\n\u2502         \u2502 ECS Task\u2502         \u2502 ECS Task \u2502              \u2502\n\u2502         \u2502   UI    \u2502         \u2502 MCP Srvs \u2502              \u2502\n\u2502         \u2502(Private)\u2502         \u2502(Private) \u2502              \u2502\n\u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n\u2502                                                          \u2502\n\u2502      Private Subnets (us-west-2a, us-west-2b)          \u2502\n\u2502      Service Discovery (Cloud Map)                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/ecs/#prerequisites","title":"Prerequisites","text":"<ol> <li>VPC Setup</li> <li>VPC with public and private subnets in 2 AZs</li> <li>NAT Gateway in public subnets</li> <li> <p>Internet Gateway attached</p> </li> <li> <p>AWS Resources</p> </li> <li>ECS Cluster</li> <li>ECR repositories for all containers</li> <li>Application Load Balancer</li> <li> <p>AWS Cloud Map namespace for service discovery</p> </li> <li> <p>IAM Roles</p> </li> <li>ECS Task Execution Role (for pulling images, secrets)</li> <li> <p>ECS Task Role (for Bedrock, SES, CloudWatch)</p> </li> <li> <p>Security Requirements</p> </li> <li>CRITICAL: NO 0.0.0.0/0 rules allowed</li> <li>ALB security group: Allow YOUR IP only</li> <li>ECS security groups: Allow from ALB/other ECS tasks only</li> </ol>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/ecs/#deployment-steps","title":"Deployment Steps","text":""},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/ecs/#1-create-ecr-repositories","title":"1. Create ECR Repositories","text":"<pre><code># Create ECR repos for all MCP servers\nfor service in transaction-risk identity-verifier email-alerts fraud-logger geolocation-checker report-generator ui; do\n  aws ecr create-repository \\\n    --repository-name fraud-detection/$service \\\n    --region us-west-2 \\\n    --profile non-prod-profile\ndone\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/ecs/#2-build-and-push-docker-images","title":"2. Build and Push Docker Images","text":"<pre><code># Set your account ID\nACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text --profile non-prod-profile)\nREGION=us-west-2\nREGISTRY=$ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com\n\n# Login to ECR\naws ecr get-login-password --region $REGION --profile non-prod-profile | \\\n  docker login --username AWS --password-stdin $REGISTRY\n\n# Build and push MCP servers\ncd mcp-servers\nfor service in transaction-risk identity-verifier email-alerts fraud-logger geolocation-checker report-generator; do\n  echo \"Building $service...\"\n  docker build -t fraud-detection/$service:latest \\\n    --build-arg SERVICE_DIR=$service \\\n    -f Dockerfile \\\n    --build-context server=./$service \\\n    .\n\n  docker tag fraud-detection/$service:latest $REGISTRY/fraud-detection/$service:latest\n  docker push $REGISTRY/fraud-detection/$service:latest\ndone\n\n# Build and push UI\ncd ../ui\ndocker build -t fraud-detection/ui:latest .\ndocker tag fraud-detection/ui:latest $REGISTRY/fraud-detection/ui:latest\ndocker push $REGISTRY/fraud-detection/ui:latest\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/ecs/#3-create-service-discovery-namespace","title":"3. Create Service Discovery Namespace","text":"<pre><code>aws servicediscovery create-private-dns-namespace \\\n  --name fraud-detection.local \\\n  --vpc vpc-xxxxx \\\n  --region us-west-2 \\\n  --profile non-prod-profile\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/ecs/#4-deploy-ecs-resources","title":"4. Deploy ECS Resources","text":"<p>See the <code>deploy.sh</code> script for automated deployment.</p>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/ecs/#security-groups-configuration","title":"Security Groups Configuration","text":""},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/ecs/#alb-security-group","title":"ALB Security Group","text":"<pre><code>Inbound:\n- Port 80: YOUR_IP/32 ONLY (no 0.0.0.0/0!)\nOutbound:\n- Port 8501: ECS UI security group\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/ecs/#ecs-ui-security-group","title":"ECS UI Security Group","text":"<pre><code>Inbound:\n- Port 8501: From ALB security group\nOutbound:\n- Port 8080: To MCP servers security group\n- Port 443: To AWS services (Bedrock, SES)\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/ecs/#mcp-servers-security-group","title":"MCP Servers Security Group","text":"<pre><code>Inbound:\n- Port 8080: From ECS UI security group\nOutbound:\n- Port 443: To AWS services\n</code></pre>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/ecs/#environment-variables","title":"Environment Variables","text":""},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/ecs/#ui-task","title":"UI Task","text":"<ul> <li><code>MCP_TRANSACTION_RISK_URL</code>: http://transaction-risk.fraud-detection.local:8080/sse</li> <li><code>MCP_IDENTITY_VERIFIER_URL</code>: http://identity-verifier.fraud-detection.local:8080/sse</li> <li><code>MCP_EMAIL_ALERTS_URL</code>: http://email-alerts.fraud-detection.local:8080/sse</li> <li><code>MCP_FRAUD_LOGGER_URL</code>: http://fraud-logger.fraud-detection.local:8080/sse</li> <li><code>MCP_GEOLOCATION_URL</code>: http://geolocation-checker.fraud-detection.local:8080/sse</li> <li><code>MCP_REPORT_GENERATOR_URL</code>: http://report-generator.fraud-detection.local:8080/sse</li> <li><code>AWS_REGION</code>: us-west-2</li> </ul>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/ecs/#email-alerts-task","title":"Email Alerts Task","text":"<ul> <li><code>SENDER_EMAIL</code>: fraud-alerts@yourdomain.com</li> <li><code>ALERT_RECIPIENTS</code>: security-team@yourdomain.com</li> <li><code>AWS_REGION</code>: us-west-2</li> </ul>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/ecs/#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Use Fargate Spot for MCP servers (cheaper)</li> <li>Use regular Fargate for UI (reliability)</li> <li>Set appropriate task sizes:</li> <li>MCP servers: 0.5 vCPU, 1 GB RAM</li> <li>UI: 1 vCPU, 2 GB RAM</li> </ul>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/ecs/#monitoring","title":"Monitoring","text":"<ul> <li>CloudWatch Container Insights enabled</li> <li>ALB access logs to S3</li> <li>ECS task logs to CloudWatch Logs</li> </ul>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/ecs/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/ecs/#mcp-servers-not-reachable","title":"MCP Servers Not Reachable","text":"<ol> <li>Check service discovery: <code>dig transaction-risk.fraud-detection.local</code></li> <li>Verify security groups allow port 8080</li> <li>Check ECS task logs</li> </ol>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/ecs/#ui-cant-connect-to-mcp-servers","title":"UI Can't Connect to MCP Servers","text":"<ol> <li>Verify environment variables</li> <li>Check network connectivity from UI task</li> <li>Verify Cloud Map namespace</li> </ol>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/ecs/#alb-returns-503","title":"ALB Returns 503","text":"<ol> <li>Check target health in ALB console</li> <li>Verify UI task is running</li> <li>Check security groups</li> </ol>"},{"location":"tutorials/vllm-samples/deepseek/eks/fraud-detection-demo/ecs/#next-steps","title":"Next Steps","text":"<p>After deployment: 1. Access UI via ALB DNS name 2. Test with sample transactions 3. When DeepSeek EKS is ready, update UI env var 4. Monitor CloudWatch metrics</p>"},{"location":"tutorials/vllm-samples/sagemaker/","title":"AWS SageMaker vLLM Inference","text":"<p>Deploy and run inference on vLLM models using AWS SageMaker and vLLM DLC.</p>"},{"location":"tutorials/vllm-samples/sagemaker/#files","title":"Files","text":"<ul> <li><code>deploy_and_test_sm_endpoint.py</code> - Complete workflow: deploy, inference, and cleanup</li> <li><code>testNixlConnector.sh</code> - Multi-GPU NixlConnector test script</li> </ul>"},{"location":"tutorials/vllm-samples/sagemaker/#prerequisites","title":"Prerequisites","text":"<ul> <li>AWS CLI configured with appropriate permissions</li> <li>HuggingFace token for model access (if required)</li> </ul>"},{"location":"tutorials/vllm-samples/sagemaker/#setup","title":"Setup","text":""},{"location":"tutorials/vllm-samples/sagemaker/#create-iam-role","title":"Create IAM Role","text":"<pre><code># Create role\naws iam create-role --role-name SageMakerExecutionRole\n\n# Attach policies\naws iam attach-role-policy --role-name SageMakerExecutionRole --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\naws iam attach-role-policy --role-name SageMakerExecutionRole --policy-arn arn:aws:iam::aws:policy/AmazonElasticContainerRegistryPublicFullAccess\n</code></pre>"},{"location":"tutorials/vllm-samples/sagemaker/#quick-start","title":"Quick Start","text":""},{"location":"tutorials/vllm-samples/sagemaker/#1-set-environment-variables","title":"1. Set Environment Variables","text":"<pre><code># Note: Using a Public Gallery image to create an SM endpoint is currently not supported\nexport CONTAINER_URI=\"763104351884.dkr.ecr.us-east-1.amazonaws.com/vllm:0.11.2-gpu-py312\"\nexport IAM_ROLE=\"SageMakerExecutionRole\"\nexport HF_TOKEN=\"your-huggingface-token\" \n</code></pre>"},{"location":"tutorials/vllm-samples/sagemaker/#2-run-complete-workflow","title":"2. Run Complete Workflow","text":"<pre><code># Deploy, run inference, and cleanup automatically\npython deploy_and_test_sm_endpoint.py --endpoint-name vllm-test-$(date +%s) --prompt \"Write a Python function to calculate fibonacci numbers\"\n\n# Alternate with custom parameters\npython deploy_and_test_sm_endpoint.py \\\n  --endpoint-name my-vllm-endpoint \\\n  --model-id deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B \\\n  --instance-type ml.g5.12xlarge \\\n  --prompt \"Explain machine learning\" \\\n  --max-tokens 1000 \\\n  --temperature 0.7\n</code></pre>"},{"location":"tutorials/vllm-samples/sagemaker/#command-line-options","title":"Command Line Options","text":"<ul> <li><code>--endpoint-name</code> - SageMaker endpoint name (required)</li> <li><code>--container-uri</code> - DLC image URI (default from env)</li> <li><code>--iam-role</code> - IAM role ARN (default from env)</li> <li><code>--instance-type</code> - Instance type (default: ml.g5.12xlarge)</li> <li><code>--model-id</code> - HuggingFace model ID (default: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)</li> <li><code>--hf-token</code> - HuggingFace token (default from env)</li> <li><code>--prompt</code> - Inference prompt (default: code generation example)</li> <li><code>--max-tokens</code> - Maximum response length (default: 2400)</li> <li><code>--temperature</code> - Sampling randomness 0-1 (default: 0.01)</li> </ul>"},{"location":"tutorials/vllm-samples/sagemaker/#instance-types","title":"Instance Types","text":"<p>Recommended GPU instances: - <code>ml.g5.12xlarge</code> - 4 A10G GPUs, 48 vCPUs, 192 GB RAM - <code>ml.g5.24xlarge</code> - 4 A10G GPUs, 96 vCPUs, 384 GB RAM - <code>ml.p4d.24xlarge</code> - 8 A100 GPUs, 96 vCPUs, 1152 GB RAM</p>"},{"location":"tutorials/vllm-samples/sagemaker/#test-nixlconnector","title":"Test NixlConnector","text":"<p>Test NixlConnector locally - NixlConnector Documentation</p> <pre><code># Login to aws ecr\naws ecr get-login-password --region us-west-2 | docker login \\\n--username AWS --password-stdin 763104351884.dkr.ecr.us-east-1.amazonaws.com\n\n# Pull latest vLLM DLC for EC2\n# Note: Using a Public Gallery image to create an SM endpoint is currently not supported\ndocker pull 763104351884.dkr.ecr.us-east-1.amazonaws.com/vllm:0.11.2-gpu-py312\n\n# Run container with GPU access\ndocker run -it --entrypoint=/bin/bash --gpus=all \\\n  -v $(pwd):/workspace \\\n  763104351884.dkr.ecr.us-east-1.amazonaws.com/vllm:0.11.2-gpu-py312\n\n# Inside container, run the NixlConnector test\nexport HF_TOKEN= \"&lt;TOKEN&gt;\"\n./testNixlConnector.sh\n</code></pre>"},{"location":"tutorials/vllm-samples/sagemaker/#notes","title":"Notes","text":"<ul> <li>The script automatically cleans up resources after inference to avoid ongoing costs</li> <li>Deployment waits for endpoint to be ready before running inference</li> <li>All parameters can be set via environment variables or command line arguments</li> </ul>"}]}