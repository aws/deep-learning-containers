ARG CUDA_VERSION=12.8.1
ARG IMAGE_DISTRO=ubuntu22.04
ARG PYTHON_VERSION=3.12

##############################################################################
# Base Image
##############################################################################
FROM nvcr.io/nvidia/cuda:${CUDA_VERSION}-devel-${IMAGE_DISTRO} AS base

# CUDA Configuration
ARG TORCH_CUDA_ARCH_LIST="7.5"
ARG VLLM_FA_CMAKE_GPU_ARCHES="75"
ENV TORCH_CUDA_ARCH_LIST=${TORCH_CUDA_ARCH_LIST} \
    UV_TORCH_BACKEND=cu128 \
    VLLM_FA_CMAKE_GPU_ARCHES=${VLLM_FA_CMAKE_GPU_ARCHES}

# System Dependencies
ENV DEBIAN_FRONTEND=noninteractive
RUN apt update && apt upgrade -y && \
    apt install -y --no-install-recommends \
        curl \
        git \
        libibverbs-dev \
        zlib1g-dev \
        libnuma-dev \
        wget && \
    apt clean && \
    rm -rf /var/lib/apt/lists/* /var/cache/apt/archives

# Environment Setup
ENV CC=/usr/bin/gcc \
    CXX=/usr/bin/g++
RUN curl -LsSf https://astral.sh/uv/install.sh | env UV_INSTALL_DIR=/usr/local/bin sh

WORKDIR /workspace
ARG PYTHON_VERSION
RUN uv venv -p ${PYTHON_VERSION} --seed --python-preference only-managed
ENV VIRTUAL_ENV=/workspace/.venv \
    PATH=${VIRTUAL_ENV}/bin:${PATH} \
    CUDA_HOME=/usr/local/cuda \
    LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}

# PyTorch Installation
ARG TORCH_URL=https://framework-binaries.s3.us-west-2.amazonaws.com/pytorch/v2.7.0/arm64/cu128/torch-2.7.0%2Bcu128-cp312-cp312-manylinux_2_28_aarch64.whl
ARG TORCHVISION_URL=https://framework-binaries.s3.us-west-2.amazonaws.com/pytorch/v2.7.0/arm64/cu128/torchvision-0.22.0%2Bcu128-cp312-cp312-linux_aarch64.whl
ARG TORCHAUDIO_URL=https://framework-binaries.s3.us-west-2.amazonaws.com/pytorch/v2.7.0/arm64/cu128/torchaudio-2.7.0%2Bcu128-cp312-cp312-linux_aarch64.whl

RUN uv pip install --no-cache-dir -U \
    ${TORCH_URL} \
    ${TORCHVISION_URL} \
    ${TORCHAUDIO_URL} && \
    uv pip install --extra-index-url https://download.pytorch.org/whl/nightly/pytorch-triton/ pytorch_triton==3.3.0

##############################################################################
# Build Base
##############################################################################
FROM base AS build-base
RUN mkdir /wheels && \
    uv pip install -U build cmake ninja pybind11 setuptools wheel requests numpy && \
    export MAX_JOBS=15

##############################################################################
# Build xformers
##############################################################################
FROM build-base AS build-xformers
ARG XFORMERS_REF=v0.0.30
ARG XFORMERS_BUILD_VERSION=0.0.30+cu128
ENV BUILD_VERSION=${XFORMERS_BUILD_VERSION}
RUN git clone https://github.com/facebookresearch/xformers.git && \
    cd xformers && \
    git checkout ${XFORMERS_REF} && \
    git submodule sync && \
    git submodule update --init --recursive -j 8 && \
    uv build --wheel --no-build-isolation -o /wheels

##############################################################################
# Build vllm
##############################################################################
FROM build-base AS build-vllm
RUN git clone https://github.com/vllm-project/vllm.git && \
    cd vllm && \
    git checkout v0.10.2rc1 && \
    git submodule sync && \
    git submodule update --init --recursive -j 8 && \
    python use_existing_torch.py && \
    uv pip install -r requirements/build.txt && \
    MAX_JOBS=16 uv build --wheel --no-build-isolation -o /wheels

##############################################################################
# Final Image
##############################################################################
FROM base AS vllm-openai

# Copy wheels from previous stages
COPY --from=build-vllm /wheels/* wheels/
COPY --from=build-xformers /wheels/* wheels/

# Install flashinfer
RUN git clone https://github.com/flashinfer-ai/flashinfer.git --recursive && \
    cd flashinfer && \
    python -c "import torch; print(torch.__version__, torch.version.cuda)" && \
    git checkout v0.2.6.post1 && \
    export FLASHINFER_CUDA_ARCH_LIST="7.5" && \
    python -m flashinfer.aot && \
    MAX_JOBS=16 uv pip install --system --no-build-isolation . && \
    python3 -m flashinfer --download-cubin || echo "WARNING: Failed to download flashinfer cubins."

RUN uv pip install wheels/* && \
    rm -r wheels && \
    uv pip install accelerate hf_transfer modelscope bitsandbytes timm boto3 \
    runai-model-streamer runai-model-streamer[s3] tensorizer && \
    uv clean

ENV PATH="$(dirname $(realpath .venv/bin/python)):$PATH" \
    HF_HUB_ENABLE_HF_TRANSFER=1

RUN uv pip install -U build cmake ninja pybind11 setuptools==79.0.1 wheel && \
    uv pip install datasets aiohttp

ARG NSYS_URL=https://developer.nvidia.com/downloads/assets/tools/secure/nsight-systems/2025_3/
ARG NSYS_PKG=nsight-systems-cli-2025.3.1_2025.3.1.90-1_arm64.deb

RUN apt-get update && \
    apt install -y libglib2.0-0 tmux cmake && \
    wget ${NSYS_URL}${NSYS_PKG} && \
    dpkg -i $NSYS_PKG && \
    rm $NSYS_PKG

ARG PYTHON="python3"
ARG EFA_VERSION="1.43.2"

ENV DEBIAN_FRONTEND=noninteractive \
    LANG=C.UTF-8 \
    LC_ALL=C.UTF-8 \
    DLC_CONTAINER_TYPE=general \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PYTHONIOENCODING=UTF-8 \
    LD_LIBRARY_PATH="/usr/local/lib:/opt/amazon/ofi-nccl/lib/aarch64-linux-gnu:/opt/amazon/openmpi/lib:/opt/amazon/efa/lib:/usr/local/cuda/lib64:${LD_LIBRARY_PATH}" \
    PATH="/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/usr/local/cuda/bin:${PATH}"

# Copy and setup scripts
WORKDIR /
COPY install_efa.sh install_efa.sh
COPY deep_learning_container.py /usr/local/bin/deep_learning_container.py
COPY bash_telemetry.sh /usr/local/bin/bash_telemetry.sh
COPY dockerd_entrypoint.sh /usr/local/bin/dockerd_entrypoint.sh

RUN chmod +x /usr/local/bin/deep_learning_container.py && \
    chmod +x /usr/local/bin/bash_telemetry.sh && \
    chmod +x /usr/local/bin/dockerd_entrypoint.sh && \
    echo 'source /usr/local/bin/bash_telemetry.sh' >> /etc/bash.bashrc && \
    bash install_efa.sh ${EFA_VERSION} && \
    rm install_efa.sh

RUN mkdir -p /tmp/nvjpeg && \
    cd /tmp/nvjpeg && \
    wget https://developer.download.nvidia.com/compute/cuda/redist/libnvjpeg/linux-aarch64/libnvjpeg-linux-aarch64-12.4.0.76-archive.tar.xz && \
    tar -xvf libnvjpeg-linux-aarch64-12.4.0.76-archive.tar.xz && \
    rm -rf /usr/local/cuda/targets/sbsa-linux/lib/libnvjpeg* && \
    rm -rf /usr/local/cuda/targets/sbsa-linux/include/nvjpeg.h && \
    cp libnvjpeg-linux-aarch64-12.4.0.76-archive/lib/libnvjpeg* /usr/local/cuda/targets/sbsa-linux/lib/ && \
    cp libnvjpeg-linux-aarch64-12.4.0.76-archive/include/* /usr/local/cuda/targets/sbsa-linux/include/ && \
    rm -rf /tmp/nvjpeg && \
    rm -rf /usr/local/cuda/bin/cuobjdump* && \
    rm -rf /usr/local/cuda/bin/nvdisasm*

ENTRYPOINT ["/usr/local/bin/dockerd_entrypoint.sh"]
