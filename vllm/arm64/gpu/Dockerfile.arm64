# ARG CUDA_VERSION=12.8.1
# ARG PYTHON_VERSION=3.12

# ARG BUILD_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04
# ARG FINAL_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04

# ARG DEADSNAKES_MIRROR_URL
# ARG DEADSNAKES_GPGKEY_URL

# ARG GET_PIP_URL="https://bootstrap.pypa.io/get-pip.py"

# ARG PYTORCH_CUDA_INDEX_BASE_URL=https://download.pytorch.org/whl
# ARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL=https://download.pytorch.org/whl/nightly

# ARG INSTALL_KV_CONNECTORS=false

# #################### BASE BUILD IMAGE ####################
# FROM ${BUILD_BASE_IMAGE} AS base
# ARG CUDA_VERSION
# ARG PYTHON_VERSION
# ARG INSTALL_KV_CONNECTORS=false
# ENV DEBIAN_FRONTEND=noninteractive

# ARG DEADSNAKES_MIRROR_URL
# ARG DEADSNAKES_GPGKEY_URL
# ARG GET_PIP_URL

# # Install Python and other dependencies
# RUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \
#     && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \
#     && apt-get update -y \
#     && apt-get install -y ccache software-properties-common git curl sudo \
#     && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \
#         if [ ! -z "${DEADSNAKES_GPGKEY_URL}" ] ; then \
#             mkdir -p -m 0755 /etc/apt/keyrings \
#             && curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg \
#             && sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg \
#             && echo "deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main" > /etc/apt/sources.list.d/deadsnakes.list; \
#         fi ; \
#     else \
#         for i in 1 2 3; do \
#             add-apt-repository -y ppa:deadsnakes/ppa && break || \
#             { echo "Attempt $i failed, retrying in 5s..."; sleep 5; }; \
#         done ; \
#     fi \
#     && apt-get update -y \
#     && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \
#     && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \
#     && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \
#     && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \
#     && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \
#     && ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/ \
#     && python3 --version && python3 -m pip --version

# ARG PYTORCH_CUDA_INDEX_BASE_URL
# ARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL


# WORKDIR /workspace


# #################### WHEEL BUILD IMAGE ####################
# FROM base AS build
# ARG PYTORCH_CUDA_INDEX_BASE_URL

# WORKDIR /workspace/vllm

# ARG VLLM_VERSION="v0.10.1.1"
# ARG max_jobs=20
# ENV MAX_JOBS=${max_jobs}

# ARG nvcc_threads=10
# ENV NVCC_THREADS=$nvcc_threads

# # cuda arch list used by torch
# ARG torch_cuda_arch_list='7.5'
# ENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}

# RUN python3 -m pip install -r https://raw.githubusercontent.com/vllm-project/vllm/${VLLM_VERSION}/requirements/build.txt \
#     --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \
#     && mkdir -p /workspace/dist \
#     && git clone https://github.com/vllm-project/vllm.git . \
#     && git checkout ${VLLM_VERSION} \
#     && python3 use_existing_torch.py 

# ARG vllm_target_device="cuda"
# ENV VLLM_TARGET_DEVICE=${vllm_target_device}
# ENV VLLM_USE_PRECOMPILED=True
# RUN rm -rf .deps && \
#     mkdir -p .deps && \
#     export VLLM_USE_PRECOMPILED=True && \
#     export VLLM_DOCKER_BUILD_CONTEXT=1 && \
#     python3 setup.py bdist_wheel --dist-dir=/workspace/dist --py-limited-api=cp38

# #################### vLLM installation IMAGE ####################
# FROM ${FINAL_BASE_IMAGE} AS vllm-base
# ARG CUDA_VERSION
# ARG PYTHON_VERSION
# ARG INSTALL_KV_CONNECTORS=false
# ENV DEBIAN_FRONTEND=noninteractive

# SHELL ["/bin/bash", "-c"]

# ARG DEADSNAKES_MIRROR_URL
# ARG DEADSNAKES_GPGKEY_URL
# ARG GET_PIP_URL

# WORKDIR /vllm-workspace

# RUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\.//g') && \
#     echo "export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}" >> /etc/environment

# # Install Python and other dependencies
# RUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \
#     && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \
#     && apt-get update -y \
#     && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \
#     && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \
#     && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \
#         if [ ! -z "${DEADSNAKES_GPGKEY_URL}" ] ; then \
#             mkdir -p -m 0755 /etc/apt/keyrings ; \
#             curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg ; \
#             sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg ; \
#             echo "deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main" > /etc/apt/sources.list.d/deadsnakes.list ; \
#         fi ; \
#     else \
#         for i in 1 2 3; do \
#             add-apt-repository -y ppa:deadsnakes/ppa && break || \
#             { echo "Attempt $i failed, retrying in 5s..."; sleep 5; }; \
#         done ; \
#     fi \
#     && apt-get update -y \
#     && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \
#     && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \
#     && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \
#     && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \
#     && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \
#     && python3 --version && python3 -m pip --version

# ARG PYTORCH_CUDA_INDEX_BASE_URL
# ARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL

# RUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/

# RUN python3 -m pip install -r https://raw.githubusercontent.com/vllm-project/vllm/v0.10.1.1/requirements/build.txt \
#     --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')

# # Install vllm wheel
# COPY --from=build /workspace/dist /vllm-workspace/dist
# RUN python3 -m pip install /vllm-workspace/dist/*.whl --verbose \
#     --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')

# # Install FlashInfer
# ARG FLASHINFER_GIT_REPO="https://github.com/flashinfer-ai/flashinfer.git"
# ARG FLASHINFER_GIT_REF="v0.2.12"
# ARG FLASHINFER_AOT_COMPILE="true"

# RUN bash -c '. /etc/environment && \
#     git clone --depth 1 --recursive --shallow-submodules \
#         --branch ${FLASHINFER_GIT_REF} \
#         ${FLASHINFER_GIT_REPO} flashinfer && \
#     cd flashinfer && \
#     if [ "${FLASHINFER_AOT_COMPILE}" = "true" ]; then \
#         if [[ "${CUDA_VERSION}" == 11.* ]]; then \
#             FI_TORCH_CUDA_ARCH_LIST="7.5 8.0 8.9"; \
#         elif [[ "${CUDA_VERSION}" == 12.[0-7]* ]]; then \
#             FI_TORCH_CUDA_ARCH_LIST="7.5 8.0 8.9 9.0a"; \
#         else \
#             FI_TORCH_CUDA_ARCH_LIST="7.5 8.0 8.9 9.0a 10.0a 12.0"; \
#         fi && \
#         echo "🏗️  Installing FlashInfer with AOT compilation for arches: ${FI_TORCH_CUDA_ARCH_LIST}" && \
#         TORCH_CUDA_ARCH_LIST="${FI_TORCH_CUDA_ARCH_LIST}" python3 -m flashinfer.aot && \
#         TORCH_CUDA_ARCH_LIST="${FI_TORCH_CUDA_ARCH_LIST}" python3 -m pip install --no-build-isolation . \
#             --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d ".") && \
#         TORCH_CUDA_ARCH_LIST="${FI_TORCH_CUDA_ARCH_LIST}" python3 -m flashinfer --download-cubin || \
#             echo "WARNING: Failed to download flashinfer cubins."; \
#     else \
#         echo "🏗️  Installing FlashInfer without AOT compilation in JIT mode" && \
#         python3 -m pip install . \
#             --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d "."); \
#     fi && \
#     cd .. && \
#     rm -rf flashinfer'

# #################### OPENAI API SERVER ####################
# FROM vllm-base AS vllm-openai-base
# ARG INSTALL_KV_CONNECTORS=false

# # install additional dependencies for openai api server
# RUN if [ "$INSTALL_KV_CONNECTORS" = "true" ]; then \
#         python3 -m pip install lmcache; \
#     fi; \
#     python3 -m pip install accelerate hf_transfer modelscope "bitsandbytes>=0.42.0" 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]

# ENV VLLM_USAGE_SOURCE production-docker-image

# FROM vllm-openai-base AS final
# ARG PYTHON="python3"
# LABEL maintainer="Amazon AI"
# LABEL dlc_major_version="1"
# ENV DEBIAN_FRONTEND=noninteractive \
#     LANG=C.UTF-8 \
#     LC_ALL=C.UTF-8 \
#     DLC_CONTAINER_TYPE=base \
#     PYTHONDONTWRITEBYTECODE=1 \
#     PYTHONUNBUFFERED=1 \
#     PYTHONIOENCODING=UTF-8 \
#     LD_LIBRARY_PATH="/usr/local/lib:/opt/amazon/ofi-nccl/lib/aarch64-linux-gnu:/opt/amazon/openmpi/lib:/opt/amazon/efa/lib:/usr/local/cuda/lib64:${LD_LIBRARY_PATH}" \
#     PATH="/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/usr/local/cuda/bin:${PATH}"

# COPY deep_learning_container.py /usr/local/bin/deep_learning_container.py
# COPY bash_telemetry.sh /usr/local/bin/bash_telemetry.sh
# COPY dockerd_entrypoint.sh /usr/local/bin/dockerd_entrypoint.sh

# WORKDIR /

# RUN chmod +x /usr/local/bin/deep_learning_container.py && \
#     chmod +x /usr/local/bin/bash_telemetry.sh && \
#     chmod +x /usr/local/bin/dockerd_entrypoint.sh && \
#     echo 'source /usr/local/bin/bash_telemetry.sh' >> /etc/bash.bashrc 

# RUN apt-get update && \
#     apt-get upgrade -y && \
#     apt-get install -y --allow-change-held-packages --no-install-recommends unzip && \
#     apt-get clean && \
#     HOME_DIR=/root && \
#     curl -o ${HOME_DIR}/oss_compliance.zip https://aws-dlinfra-utilities.s3.amazonaws.com/oss_compliance.zip && \
#     unzip ${HOME_DIR}/oss_compliance.zip -d ${HOME_DIR}/ && \
#     cp ${HOME_DIR}/oss_compliance/test/testOSSCompliance /usr/local/bin/testOSSCompliance && \
#     chmod +x /usr/local/bin/testOSSCompliance && \
#     chmod +x ${HOME_DIR}/oss_compliance/generate_oss_compliance.sh && \
#     ${HOME_DIR}/oss_compliance/generate_oss_compliance.sh ${HOME_DIR} ${PYTHON} && \
#     # create symlink for python
#     ln -s /usr/bin/python3 /usr/bin/python && \
#     # clean up
#     rm -rf ${HOME_DIR}/oss_compliance* && \
#     rm -rf /tmp/tmp* && \
#     rm -rf /tmp/uv* && \
#     rm -rf /var/lib/apt/lists/* && \
#     rm -rf /root/.cache | true

# RUN mkdir -p /tmp/nvjpeg && \
#     cd /tmp/nvjpeg && \
#     wget https://developer.download.nvidia.com/compute/cuda/redist/libnvjpeg/linux-aarch64/libnvjpeg-linux-aarch64-12.4.0.76-archive.tar.xz && \
#     tar -xvf libnvjpeg-linux-aarch64-12.4.0.76-archive.tar.xz && \
#     rm -rf /usr/local/cuda/targets/sbsa-linux/lib/libnvjpeg* && \
#     rm -rf /usr/local/cuda/targets/sbsa-linux/include/nvjpeg.h && \
#     cp libnvjpeg-linux-aarch64-12.4.0.76-archive/lib/libnvjpeg* /usr/local/cuda/targets/sbsa-linux/lib/ && \
#     cp libnvjpeg-linux-aarch64-12.4.0.76-archive/include/* /usr/local/cuda/targets/sbsa-linux/include/ && \
#     rm -rf /tmp/nvjpeg && \
#     rm -rf /usr/local/cuda/bin/cuobjdump* && \
#     rm -rf /usr/local/cuda/bin/nvdisasm*

# ENTRYPOINT ["/usr/local/bin/dockerd_entrypoint.sh"]


# The vLLM Dockerfile is used to construct vLLM image that can be directly used
# to run the OpenAI compatible server.

# Example to build on 4xGH200 node.
# podman build --build-arg max_jobs=64 --build-arg nvcc_threads=8  --target vllm-base --tag vllm:v0.6.6.post1-$(git rev-parse --short HEAD)-arm64-cuda-gh200 .
# Example to build on 24 core node.
# docker build --build-arg max_jobs=14 --build-arg nvcc_threads=6  --target vllm-base --tag vllm:v0.6.6.post1-$(git rev-parse --short HEAD)-amd64-cuda-a100-h100 .


#################### BASE BUILD IMAGE ####################
# prepare basic build environment
FROM nvcr.io/nvidia/pytorch:25.02-py3 AS base

ENV DEBIAN_FRONTEND=noninteractive
RUN apt-get update -y \
    && apt-get install -y ccache software-properties-common git curl wget sudo vim libibverbs-dev ffmpeg libsm6 libxext6 libgl1

WORKDIR /workspace

ARG torch_cuda_arch_list='7.5'
ENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}
# Override the arch list for flash-attn to reduce the binary size
ARG vllm_fa_cmake_gpu_arches='75'
ENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}
# max jobs used by Ninja to build extensions
ARG max_jobs
ENV MAX_JOBS=${max_jobs}
# number of threads used by nvcc
ARG nvcc_threads
ENV NVCC_THREADS=$nvcc_threads


ARG VLLM_VERSION="v0.10.1.1"
RUN python3 -m pip install -r https://raw.githubusercontent.com/vllm-project/vllm/${VLLM_VERSION}/requirements/cuda.txt 
RUN python3 -m pip install -r https://raw.githubusercontent.com/vllm-project/vllm/${VLLM_VERSION}/requirements/common.txt 


#################### BASE BUILD IMAGE ####################

#################### Build IMAGE ####################
FROM base AS build

# build vLLM extensions

RUN mkdir wheels


# xFormers also installs its flash-attention inside not visible outside.
# https://github.com/facebookresearch/xformers/blob/d3948b5cb9a3711032a0ef0e036e809c7b08c1e0/.github/workflows/wheels_build.yml#L120
RUN git clone https://github.com/facebookresearch/xformers.git ; cd xformers ; git checkout v0.0.28.post3 ; git submodule update --init --recursive ; python setup.py bdist_wheel --dist-dir=/workspace/wheels

# Flashinfer.
# https://github.com/flashinfer-ai/flashinfer/blob/8f186cf0ea07717727079d0c92bbe9be3814a9cb/scripts/run-ci-build-wheel.sh#L47C1-L47C39
RUN git clone https://github.com/flashinfer-ai/flashinfer.git ; cd flashinfer ; git checkout  v0.2.0.post2 ; git submodule update --init --recursive ; cd python ; FLASHINFER_ENABLE_AOT=1 python setup.py bdist_wheel --dist-dir=/workspace/wheels

# Bitsandbytes.
RUN git clone https://github.com/bitsandbytes-foundation/bitsandbytes.git ; cd bitsandbytes ; git checkout 0.45 ; cmake -DCOMPUTE_BACKEND=cuda -S . ; make ; python setup.py bdist_wheel --dist-dir=/workspace/wheels

# Install them.
RUN pip install --no-deps /workspace/wheels/*.whl

WORKDIR /vllm-workspace

# files and directories related to build wheels
COPY . .

ENV CCACHE_DIR=/root/.cache/ccache
RUN --mount=type=cache,target=/root/.cache/ccache \
    --mount=type=bind,source=.git,target=.git \
    python setup.py bdist_wheel --dist-dir=/workspace/wheels

# Check the size of the wheel if RUN_WHEEL_CHECK is true
COPY .buildkite/check-wheel-size.py check-wheel-size.py
# sync the default value with .buildkite/check-wheel-size.py
ARG VLLM_MAX_SIZE_MB=400
ENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB
ARG RUN_WHEEL_CHECK=true
RUN if [ "$RUN_WHEEL_CHECK" = "true" ]; then \
        python check-wheel-size.py dist; \
    else \
        echo "Skipping wheel size check."; \
    fi
####################  Build IMAGE ####################


#################### vLLM installation IMAGE ####################
# image with vLLM installed
FROM base AS final

RUN --mount=type=bind,from=build,src=/workspace/wheels,target=/workspace/wheels \
    pip install --no-deps /workspace/wheels/*.whl

ARG PYTHON="python3"
LABEL maintainer="Amazon AI"
LABEL dlc_major_version="1"
ENV DEBIAN_FRONTEND=noninteractive \
    LANG=C.UTF-8 \
    LC_ALL=C.UTF-8 \
    DLC_CONTAINER_TYPE=base \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PYTHONIOENCODING=UTF-8 \
    LD_LIBRARY_PATH="/usr/local/lib:/opt/amazon/ofi-nccl/lib/aarch64-linux-gnu:/opt/amazon/openmpi/lib:/opt/amazon/efa/lib:/usr/local/cuda/lib64:${LD_LIBRARY_PATH}" \
    PATH="/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/usr/local/cuda/bin:${PATH}"

COPY deep_learning_container.py /usr/local/bin/deep_learning_container.py
COPY bash_telemetry.sh /usr/local/bin/bash_telemetry.sh
COPY dockerd_entrypoint.sh /usr/local/bin/dockerd_entrypoint.sh

WORKDIR /

RUN chmod +x /usr/local/bin/deep_learning_container.py && \
    chmod +x /usr/local/bin/bash_telemetry.sh && \
    chmod +x /usr/local/bin/dockerd_entrypoint.sh && \
    echo 'source /usr/local/bin/bash_telemetry.sh' >> /etc/bash.bashrc 

RUN apt-get update && \
    apt-get upgrade -y && \
    apt-get install -y --allow-change-held-packages --no-install-recommends unzip && \
    apt-get clean && \
    HOME_DIR=/root && \
    curl -o ${HOME_DIR}/oss_compliance.zip https://aws-dlinfra-utilities.s3.amazonaws.com/oss_compliance.zip && \
    unzip ${HOME_DIR}/oss_compliance.zip -d ${HOME_DIR}/ && \
    cp ${HOME_DIR}/oss_compliance/test/testOSSCompliance /usr/local/bin/testOSSCompliance && \
    chmod +x /usr/local/bin/testOSSCompliance && \
    chmod +x ${HOME_DIR}/oss_compliance/generate_oss_compliance.sh && \
    ${HOME_DIR}/oss_compliance/generate_oss_compliance.sh ${HOME_DIR} ${PYTHON} && \
    # create symlink for python
    ln -s /usr/bin/python3 /usr/bin/python && \
    # clean up
    rm -rf ${HOME_DIR}/oss_compliance* && \
    rm -rf /tmp/tmp* && \
    rm -rf /tmp/uv* && \
    rm -rf /var/lib/apt/lists/* && \
    rm -rf /root/.cache | true

RUN mkdir -p /tmp/nvjpeg && \
    cd /tmp/nvjpeg && \
    wget https://developer.download.nvidia.com/compute/cuda/redist/libnvjpeg/linux-aarch64/libnvjpeg-linux-aarch64-12.4.0.76-archive.tar.xz && \
    tar -xvf libnvjpeg-linux-aarch64-12.4.0.76-archive.tar.xz && \
    rm -rf /usr/local/cuda/targets/sbsa-linux/lib/libnvjpeg* && \
    rm -rf /usr/local/cuda/targets/sbsa-linux/include/nvjpeg.h && \
    cp libnvjpeg-linux-aarch64-12.4.0.76-archive/lib/libnvjpeg* /usr/local/cuda/targets/sbsa-linux/lib/ && \
    cp libnvjpeg-linux-aarch64-12.4.0.76-archive/include/* /usr/local/cuda/targets/sbsa-linux/include/ && \
    rm -rf /tmp/nvjpeg && \
    rm -rf /usr/local/cuda/bin/cuobjdump* && \
    rm -rf /usr/local/cuda/bin/nvdisasm*

#################### OPENAI API SERVER ####################
# openai api server alternative
# base openai image with additional requirements, for any subsequent openai-style images
FROM final AS vllm-openai-base

# install additional dependencies for openai api server
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install accelerate hf_transfer 'modelscope!=1.15.0' timm==0.9.10 boto3 runai-model-streamer 'runai-model-streamer[s3]'

ENV VLLM_USAGE_SOURCE production-docker-image

FROM vllm-openai-base AS vllm-openai

ENTRYPOINT ["python", "-m", "vllm.entrypoints.openai.api_server"]
#################### OPENAI API SERVER ####################