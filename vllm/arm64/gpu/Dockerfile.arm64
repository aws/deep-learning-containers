ARG CUDA_VERSION=12.8.1
ARG PYTHON_VERSION=3.12

ARG BUILD_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04
ARG FINAL_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04

ARG DEADSNAKES_MIRROR_URL
ARG DEADSNAKES_GPGKEY_URL
ARG GET_PIP_URL="https://bootstrap.pypa.io/get-pip.py"

ARG PIP_INDEX_URL
ARG PIP_EXTRA_INDEX_URL
ARG UV_INDEX_URL=${PIP_INDEX_URL}
ARG UV_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}

ARG PYTORCH_CUDA_INDEX_BASE_URL=https://download.pytorch.org/whl
ARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL=https://download.pytorch.org/whl/nightly

ARG PIP_KEYRING_PROVIDER=disabled
ARG UV_KEYRING_PROVIDER=${PIP_KEYRING_PROVIDER}

ARG INSTALL_KV_CONNECTORS=false

#################### PYTHON BASE IMAGE ####################
FROM ${BUILD_BASE_IMAGE} AS python-base
ARG TARGETPLATFORM
ARG CUDA_VERSION
ARG PYTHON_VERSION
ENV DEBIAN_FRONTEND=noninteractive

ARG DEADSNAKES_MIRROR_URL
ARG DEADSNAKES_GPGKEY_URL
ARG GET_PIP_URL

# Install Python and common dependencies
RUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \
    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \
    && apt-get update -y \
    && apt-get install -y ccache software-properties-common git curl sudo \
    && if [ ! -z ${DEADSNAKES_MIRROR_URL} ] ; then \
        if [ ! -z "${DEADSNAKES_GPGKEY_URL}" ] ; then \
            mkdir -p -m 0755 /etc/apt/keyrings \
            && curl -L ${DEADSNAKES_GPGKEY_URL} | gpg --dearmor > /etc/apt/keyrings/deadsnakes.gpg \
            && sudo chmod 644 /etc/apt/keyrings/deadsnakes.gpg \
            && echo "deb [signed-by=/etc/apt/keyrings/deadsnakes.gpg] ${DEADSNAKES_MIRROR_URL} $(lsb_release -cs) main" > /etc/apt/sources.list.d/deadsnakes.list; \
        fi ; \
    else \
        for i in 1 2 3; do \
            add-apt-repository -y ppa:deadsnakes/ppa && break || \
            { echo "Attempt $i failed, retrying in 5s..."; sleep 5; }; \
        done ; \
    fi \
    && apt-get update -y \
    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \
    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \
    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \
    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \
    && curl -sS ${GET_PIP_URL} | python${PYTHON_VERSION} \
    && ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/ \
    && python3 --version && python3 -m pip --version

#################### WHEEL BUILD IMAGE ####################
FROM python-base AS build
ARG TARGETPLATFORM
ARG PIP_INDEX_URL UV_INDEX_URL
ARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL
ARG PYTORCH_CUDA_INDEX_BASE_URL

WORKDIR /workspace/vllm

ARG VLLM_VERSION="v0.10.1.1"
ARG max_jobs=16
ENV MAX_JOBS=${max_jobs}

ARG nvcc_threads=8
ENV NVCC_THREADS=$nvcc_threads

# cuda arch list used by torch
ARG torch_cuda_arch_list='7.5'
ENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}

RUN python3 -m pip install -r https://raw.githubusercontent.com/vllm-project/vllm/${VLLM_VERSION}/requirements/build.txt \
    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \
    && mkdir -p /workspace/dist \
    && git clone https://github.com/vllm-project/vllm.git . \
    && git checkout ${VLLM_VERSION} \
    && python3 use_existing_torch.py 

ARG vllm_target_device="cuda"
ENV VLLM_TARGET_DEVICE=${vllm_target_device}
RUN rm -rf .deps \
    && mkdir -p .deps \
    && export VLLM_USE_PRECOMPILED=false \
    && export VLLM_DOCKER_BUILD_CONTEXT=1 \
    && python3 setup.py bdist_wheel --dist-dir=/workspace/dist --py-limited-api=cp38

#################### vLLM BASE IMAGE ####################
FROM ${FINAL_BASE_IMAGE} AS vllm-base
ARG TARGETPLATFORM
ARG CUDA_VERSION
ARG PYTHON_VERSION
ARG INSTALL_KV_CONNECTORS=false
ENV DEBIAN_FRONTEND=noninteractive

SHELL ["/bin/bash", "-c"]

WORKDIR /vllm-workspace

RUN apt-get update -y \
    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 python3-pip libibverbs-dev vim wget \
    && ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/

ARG PIP_INDEX_URL UV_INDEX_URL
ARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL
ARG PYTORCH_CUDA_INDEX_BASE_URL

# Copy Python installation from python-base
COPY --from=python-base /usr/local /usr/local
COPY --from=python-base /usr/bin /usr/bin
COPY --from=python-base /usr/lib /usr/lib

# Install vllm wheel and dependencies
COPY --from=build /workspace/dist /vllm-workspace/dist
RUN python -m pip install /vllm-workspace/dist/*.whl --verbose \
    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')

ARG FLASHINFER_GIT_REPO="https://github.com/flashinfer-ai/flashinfer.git"
ARG FLASHINFER_GIT_REF="v0.2.12"
ARG FLASHINFER_AOT_COMPILE="true"

RUN bash -c '. /etc/environment && \
    git clone --depth 1 --recursive --shallow-submodules \
        --branch ${FLASHINFER_GIT_REF} \
        ${FLASHINFER_GIT_REPO} flashinfer && \
    cd flashinfer && \
    if [ "${FLASHINFER_AOT_COMPILE}" = "true" ]; then \
        if [[ "${CUDA_VERSION}" == 11.* ]]; then \
            FI_TORCH_CUDA_ARCH_LIST="7.5 8.0 8.9"; \
        elif [[ "${CUDA_VERSION}" == 12.[0-7]* ]]; then \
            FI_TORCH_CUDA_ARCH_LIST="7.5 8.0 8.9 9.0a"; \
        else \
            FI_TORCH_CUDA_ARCH_LIST="7.5 8.0 8.9 9.0a 10.0a 12.0"; \
        fi && \
        TORCH_CUDA_ARCH_LIST="${FI_TORCH_CUDA_ARCH_LIST}" python3 -m flashinfer.aot && \
        TORCH_CUDA_ARCH_LIST="${FI_TORCH_CUDA_ARCH_LIST}" python3 -m pip install --no-build-isolation . \
            --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d ".") && \
        TORCH_CUDA_ARCH_LIST="${FI_TORCH_CUDA_ARCH_LIST}" python3 -m flashinfer --download-cubin || \
            echo "WARNING: Failed to download flashinfer cubins."; \
    else \
        python3 -m pip install . \
            --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d "."); \
    fi && \
    cd .. && \
    rm -rf flashinfer'

# Install DeepGEMM
ARG DEEPGEMM_GIT_REPO="https://github.com/deepseek-ai/DeepGEMM.git"
ARG DEEPGEMM_GIT_REF="7b6b5563b9d4c1ae07ffbce7f78ad3ac9204827c"
RUN bash -c ' \
    . /etc/environment \
    && CUDA_MAJOR="${CUDA_VERSION%%.*}" \
    && CUDA_MINOR="${CUDA_VERSION#${CUDA_MAJOR}.}" \
    && CUDA_MINOR="${CUDA_MINOR%%.*}" \
    && if [ "$CUDA_MAJOR" -ge 12 ] && [ "$CUDA_MINOR" -ge 8 ]; then \
        git clone --recursive --shallow-submodules ${DEEPGEMM_GIT_REPO} deepgemm \
        && cd deepgemm \
        && git checkout ${DEEPGEMM_GIT_REF} \
        && python3 setup.py bdist_wheel \
        && python3 -m pip install dist/*.whl \
        && cd .. \
        && rm -rf deepgemm; \
    else \
        echo "Skipping DeepGEMM installation (requires CUDA 12.8+ but got ${CUDA_VERSION})"; \
    fi'

#################### OPENAI API SERVER ####################
FROM vllm-base AS vllm-openai-base
ARG TARGETPLATFORM
ARG INSTALL_KV_CONNECTORS=true
ARG PIP_INDEX_URL UV_INDEX_URL
ARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL

# Install additional dependencies for openai api server
RUN if [ "$INSTALL_KV_CONNECTORS" = "true" ]; then \
        python -m pip install lmcache; \
    fi; \
    python -m pip install accelerate hf_transfer modelscope "bitsandbytes>=0.42.0" 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]

ENV VLLM_USAGE_SOURCE production-docker-image

#################### FINAL IMAGE ####################
FROM vllm-openai-base AS final
ARG PYTHON="python3"
LABEL maintainer="Amazon AI"
LABEL dlc_major_version="1"

ENV DEBIAN_FRONTEND=noninteractive \
    LANG=C.UTF-8 \
    LC_ALL=C.UTF-8 \
    DLC_CONTAINER_TYPE=base \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PYTHONIOENCODING=UTF-8 \
    LD_LIBRARY_PATH="/usr/local/lib:/opt/amazon/ofi-nccl/lib/x86_64-linux-gnu:/opt/amazon/openmpi/lib:/opt/amazon/efa/lib:/usr/local/cuda/lib64:${LD_LIBRARY_PATH}" \
    PATH="/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/usr/local/cuda/bin:${PATH}"

COPY deep_learning_container.py /usr/local/bin/deep_learning_container.py
COPY bash_telemetry.sh /usr/local/bin/bash_telemetry.sh
COPY dockerd_entrypoint.sh /usr/local/bin/dockerd_entrypoint.sh

WORKDIR /

RUN chmod +x /usr/local/bin/deep_learning_container.py \
    && chmod +x /usr/local/bin/bash_telemetry.sh \
    && chmod +x /usr/local/bin/dockerd_entrypoint.sh \
    && echo 'source /usr/local/bin/bash_telemetry.sh' >> /etc/bash.bashrc 

ENTRYPOINT ["/usr/local/bin/dockerd_entrypoint.sh"]