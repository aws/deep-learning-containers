# Base CUDA version and Python version
ARG CUDA_VERSION=12.8.1
ARG PYTHON_VERSION=3.12
ARG BUILD_BASE_IMAGE=nvidia/cuda:${CUDA_VERSION}-devel-ubuntu22.04

# AWS specific arguments
ARG PYTHON="python3"
ARG EFA_VERSION="1.43.1"
ARG VLLM_VERSION="v0.10.1" 

# Base build image
FROM ${BUILD_BASE_IMAGE} AS final
ARG CUDA_VERSION
ARG PYTHON_VERSION
ARG TARGETPLATFORM=linux/arm64
ARG VLLM_VERSION
ARG PYTHON="python3"

# AWS specific labels
LABEL maintainer="Amazon AI"
LABEL dlc_major_version="1"

ENV DEBIAN_FRONTEND=noninteractive \
    LANG=C.UTF-8 \
    LC_ALL=C.UTF-8 \
    DLC_CONTAINER_TYPE=base \
    # Python wonâ€™t try to write .pyc or .pyo files on the import of source modules
    # Force stdin, stdout and stderr to be totally unbuffered. Good for logging
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PYTHONIOENCODING=UTF-8 \
    LD_LIBRARY_PATH="/usr/local/lib:/opt/amazon/ofi-nccl/lib/aarch64-linux-gnu:/opt/amazon/openmpi/lib:/opt/amazon/efa/lib:/usr/local/cuda/lib64:${LD_LIBRARY_PATH}" \
    PATH="/opt/amazon/openmpi/bin:/opt/amazon/efa/bin:/usr/local/cuda/bin:${PATH}"

WORKDIR /

COPY install_efa.sh install_efa.sh
COPY deep_learning_container.py /usr/local/bin/deep_learning_container.py
COPY bash_telemetry.sh /usr/local/bin/bash_telemetry.sh
COPY dockerd_entrypoint.sh /usr/local/bin/dockerd_entrypoint.sh
RUN chmod +x /usr/local/bin/deep_learning_container.py && \
    chmod +x /usr/local/bin/bash_telemetry.sh && \
    chmod +x /usr/local/bin/dockerd_entrypoint.sh && \
    echo 'source /usr/local/bin/bash_telemetry.sh' >> /etc/bash.bashrc && \
    # Install EFA
    bash install_efa.sh ${EFA_VERSION} && \
    rm install_efa.sh && \
    # OSS compliance and software update
    apt-get update && \
    apt-get upgrade -y && \
    apt-get install -y --allow-change-held-packages --no-install-recommends unzip && \
    apt-get clean && \
    HOME_DIR=/root && \
    curl -o ${HOME_DIR}/oss_compliance.zip https://aws-dlinfra-utilities.s3.amazonaws.com/oss_compliance.zip && \
    unzip ${HOME_DIR}/oss_compliance.zip -d ${HOME_DIR}/ && \
    cp ${HOME_DIR}/oss_compliance/test/testOSSCompliance /usr/local/bin/testOSSCompliance && \
    chmod +x /usr/local/bin/testOSSCompliance && \
    chmod +x ${HOME_DIR}/oss_compliance/generate_oss_compliance.sh && \
    ${HOME_DIR}/oss_compliance/generate_oss_compliance.sh ${HOME_DIR} ${PYTHON} && \
    # create symlink for python
    ln -s /usr/bin/python3 /usr/bin/python && \
    # clean up
    rm -rf ${HOME_DIR}/oss_compliance* && \
    rm -rf /tmp/tmp* && \
    rm -rf /tmp/uv* && \
    rm -rf /var/lib/apt/lists/* && \
    rm -rf /root/.cache | true

# patch nvjpeg
RUN mkdir -p /tmp/nvjpeg \
    && cd /tmp/nvjpeg \
    && wget https://developer.download.nvidia.com/compute/cuda/redist/libnvjpeg/linux-aarch64/libnvjpeg-linux-aarch64-12.4.0.76-archive.tar.xz \
    && tar -xvf libnvjpeg-linux-aarch64-12.4.0.76-archive.tar.xz \
    && rm -rf /usr/local/cuda/targets/sbsa-linux/lib/libnvjpeg* \
    && rm -rf /usr/local/cuda/targets/sbsa-linux/include/nvjpeg.h \
    && cp libnvjpeg-linux-aarch64-12.4.0.76-archive/lib/libnvjpeg* /usr/local/cuda/targets/sbsa-linux/lib/ \
    && cp libnvjpeg-linux-aarch64-12.4.0.76-archive/include/* /usr/local/cuda/targets/sbsa-linux/include/ \
    && rm -rf /tmp/nvjpeg \
    # patch cuobjdump and nvdisasm
    && rm -rf /usr/local/cuda/bin/cuobjdump* \
    && rm -rf /usr/local/cuda/bin/nvdisasm*

RUN apt-get update && apt-get install -y \
    ccache \
    software-properties-common \
    git \
    curl \
    wget \
    sudo \
    vim \
    ffmpeg \
    libsm6 \
    libxext6 \
    libgl1 \
    libibverbs-dev \
    && rm -rf /var/lib/apt/lists/*

RUN add-apt-repository -y ppa:deadsnakes/ppa \
    && apt-get update \
    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \
    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \
    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \
    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \
    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \
    && ln -s /usr/bin/python3 /usr/bin/python

RUN python3 -m pip install uv

# Set UV configurations
ENV UV_HTTP_TIMEOUT=500
ENV UV_INDEX_STRATEGY="unsafe-best-match"
ENV UV_LINK_MODE=copy

# Install PyTorch for ARM64
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --system \
    --index-url https://download.pytorch.org/whl/nightly/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.') \
    "torch==2.8.0.dev20250318+cu128" \
    "torchvision==0.22.0.dev20250319" \
    --pre pytorch_triton==3.3.0+gitab727c40

RUN git clone https://github.com/vllm-project/vllm.git /vllm && \
    cd /vllm && \
    git checkout ${VLLM_VERSION}

WORKDIR /vllm

RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --system -r requirements/cuda.txt \
    --extra-index-url https://download.pytorch.org/whl/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')

# Build and install vLLM
RUN --mount=type=cache,target=/root/.cache/uv \
    TORCH_CUDA_ARCH_LIST="7.5 8.0 8.9 9.0a 10.0a 12.0" \
    python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \
    && uv pip install --system dist/*.whl

# Install FlashInfer from source
ARG FLASHINFER_GIT_REPO="https://github.com/flashinfer-ai/flashinfer.git"
ARG FLASHINFER_GIT_REF="v0.2.11"
RUN --mount=type=cache,target=/root/.cache/uv \
    git clone --depth 1 --recursive --shallow-submodules \
    --branch ${FLASHINFER_GIT_REF} \
    ${FLASHINFER_GIT_REPO} flashinfer \
    && cd flashinfer \
    && TORCH_CUDA_ARCH_LIST="7.5 8.0 8.9 9.0a 10.0a 12.0" python3 -m flashinfer.aot \
    && TORCH_CUDA_ARCH_LIST="7.5 8.0 8.9 9.0a 10.0a 12.0" uv pip install --system --no-build-isolation --force-reinstall --no-deps . \
    && cd .. && rm -rf flashinfer

# Install additional dependencies for API server
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --system \
    accelerate \
    hf_transfer \
    modelscope \
    "bitsandbytes>=0.42.0" \
    'timm==0.9.10' \
    boto3 \
    runai-model-streamer \
    runai-model-streamer[s3]

WORKDIR /

ENV VLLM_USAGE_SOURCE production-docker-image

ENTRYPOINT ["/usr/local/bin/dockerd_entrypoint.sh"]