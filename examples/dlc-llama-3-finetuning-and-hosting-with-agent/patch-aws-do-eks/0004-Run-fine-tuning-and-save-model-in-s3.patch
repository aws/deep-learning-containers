From 7152b0e449aeb7b3f5e89d5112291df9ada68563 Mon Sep 17 00:00:00 2001
From: Shantanu Tripathi <shantanutripathi237@gmail.com>
Date: Thu, 6 Mar 2025 23:43:50 -0800
Subject: [PATCH 4/5] Run fine tuning and save model in s3

---
 .../eks/deployment/csi/fsx/deploy.sh          |  2 +
 .../eks/deployment/csi/fsx/fsx-policy.json    |  7 ++++
 .../eks/deployment/csi/fsx/fsx.conf           | 39 +++++++++++++------
 .../pytorch/pytorchjob/fsdp/.env              | 13 +++++--
 .../pytorchjob/fsdp/fsdp.yaml-template        | 11 ++++--
 .../pytorch/pytorchjob/fsdp/generate.sh       |  5 +++
 .../pytorch/pytorchjob/fsdp/logs.sh           | 11 +++++-
 .../fsdp/model-processor.yaml-template        | 34 ++++++++++++++++
 .../pytorch/pytorchjob/fsdp/pvc.yaml          | 15 +++++++
 9 files changed, 116 insertions(+), 21 deletions(-)
 create mode 100644 Container-Root/eks/deployment/distributed-training/pytorch/pytorchjob/fsdp/model-processor.yaml-template
 create mode 100644 Container-Root/eks/deployment/distributed-training/pytorch/pytorchjob/fsdp/pvc.yaml

diff --git a/Container-Root/eks/deployment/csi/fsx/deploy.sh b/Container-Root/eks/deployment/csi/fsx/deploy.sh
index fdeca2b..2a6de46 100755
--- a/Container-Root/eks/deployment/csi/fsx/deploy.sh
+++ b/Container-Root/eks/deployment/csi/fsx/deploy.sh
@@ -97,6 +97,8 @@ provisioner: fsx.csi.aws.com
 parameters:
   subnetId: ${FSX_SUBNET_ID}
   securityGroupIds: ${SECURITY_GROUP_ID}
+  deploymentType: PERSISTENT_2
+  perUnitStorageThroughput: "250"
 EOF
 kubectl apply -f fsx-storage-class.yaml
 kubectl get sc
diff --git a/Container-Root/eks/deployment/csi/fsx/fsx-policy.json b/Container-Root/eks/deployment/csi/fsx/fsx-policy.json
index f944704..f3cdb11 100644
--- a/Container-Root/eks/deployment/csi/fsx/fsx-policy.json
+++ b/Container-Root/eks/deployment/csi/fsx/fsx-policy.json
@@ -16,5 +16,12 @@
         "fsx:*"
       ],
       "Resource": ["*"]
+    },
+    {
+      "Effect": "Allow",
+      "Action": [
+        "s3:*"
+      ],
+      "Resource": ["*"]
   }]
 }
diff --git a/Container-Root/eks/deployment/csi/fsx/fsx.conf b/Container-Root/eks/deployment/csi/fsx/fsx.conf
index b6d9941..fa7b311 100644
--- a/Container-Root/eks/deployment/csi/fsx/fsx.conf
+++ b/Container-Root/eks/deployment/csi/fsx/fsx.conf
@@ -1,23 +1,39 @@
 #!/bin/bash
-
-pushd ../../..
-CLUSTER_NAME=$(./eks-name.sh)
-popd
+# pushd ../../..
+# CLUSTER_NAME=$(./eks-name.sh)
+# popd
+# CONFIG - configuration type for EKS, CONFIG=conf(default)|yaml
+# Set CONFIG=yaml and edit eks.yaml to use advanced cluster configuration options
+# like efaEnabled.
+# When a yaml file is used to configure the cluster, please set CLUSTER_NAME in this file to match
+# the cluster name specified in the yaml.
+# Refer to yaml file schema here: https://eksctl.io/usage/schema/
+# or examples here: https://github.com/weaveworks/eksctl/tree/main/examples
+export CONFIG=yaml
+if [ -f ${HOME}/.aws/credentials ]; then
+        # Only set AWS_PROFILE when AWS CLI credentials are configured
+        export AWS_PROFILE=default
+fi
+# EKS_YAML - path to cluster yaml manifest to use in case CONFIG=yaml
+export EKS_YAML=/home/ubuntu/blog/eks.conf
+# EKS Cluster
+export CLUSTER_NAME=eks-blog-dev
+export CLUSTER_REGION=us-west-2
+export CLUSTER_ZONES=us-west-2a
 
 # FSX Configuration
 # All settings are required
 
 ## IAM Policy to provide access to FSx, if configured policy name does not exist, it will be created.
-export FSX_POLICY_NAME=fsx-csi
+export FSX_POLICY_NAME=fsx-s3-csi
 export FSX_POLICY_DOC=file://fsx-policy.json
 
 ## Derive
 asg1_name=$(eksctl get nodegroups --cluster $CLUSTER_NAME | grep -v NAME | head -n 1 | awk '{print $10}')
-launch_template_name=$(aws autoscaling describe-auto-scaling-groups --auto-scaling-group-name=$asg1_name | jq -r .AutoScalingGroups[].MixedInstancesPolicy.LaunchTemplate.LaunchTemplateSpecification.LaunchTemplateName)
-instance_profile_name=$(aws ec2 describe-launch-template-versions --versions '$Default' --launch-template-name=$launch_template_name | jq -r .LaunchTemplateVersions[].LaunchTemplateData.IamInstanceProfile.Name)
-instance1_id=$(aws autoscaling describe-auto-scaling-groups --auto-scaling-group-name=$asg1_name | jq -r .AutoScalingGroups[].Instances[0].InstanceId)
-subnet_id=$(aws ec2 describe-instances --instance-id=$instance1_id | jq -r .Reservations[0].Instances[0].SubnetId)
-
+launch_template_name=$(aws autoscaling describe-auto-scaling-groups --auto-scaling-group-name=$asg1_name | jq -r '.AutoScalingGroups[].MixedInstancesPolicy.LaunchTemplate.LaunchTemplateSpecification.LaunchTemplateName')
+instance_profile_name=$(aws ec2 describe-launch-template-versions --versions '$Default' --launch-template-name=$launch_template_name | jq -r '.LaunchTemplateVersions[].LaunchTemplateData.IamInstanceProfile.Name')
+instance1_id=$(aws autoscaling describe-auto-scaling-groups --auto-scaling-group-name=$asg1_name | jq -r '.AutoScalingGroups[].Instances[0].InstanceId')
+subnet_id=$(aws ec2 describe-instances --instance-id=$instance1_id | jq -r '.Reservations[0].Instances[0].SubnetId')
 ## Instance profiles of EKS node groups that will have access to FSx. Space separated string enclosed in ().
 export EKS_INSTANCE_PROFILE_NAMES=($instance_profile_name)
 
@@ -26,8 +42,7 @@ export EKS_INSTANCE_PROFILE_NAMES=($instance_profile_name)
 export FSX_SUBNET_ID=$subnet_id
 
 ## Security group name for access to FSx volumes. Will be created if it does not exist.
-export FSX_SECURITY_GROUP_NAME=eks-fsx-sg
-
+export FSX_SECURITY_GROUP_NAME=eks-cluster-sg-eks-blog-dev-365834070
 ## Name of FSX storage class to create or update
 export FSX_STORAGE_CLASS_NAME=fsx-sc
 
diff --git a/Container-Root/eks/deployment/distributed-training/pytorch/pytorchjob/fsdp/.env b/Container-Root/eks/deployment/distributed-training/pytorch/pytorchjob/fsdp/.env
index ef95b0e..c538b67 100644
--- a/Container-Root/eks/deployment/distributed-training/pytorch/pytorchjob/fsdp/.env
+++ b/Container-Root/eks/deployment/distributed-training/pytorch/pytorchjob/fsdp/.env
@@ -31,9 +31,11 @@ export FI_PROVIDER=efa
 ## Model
 ## Support is available for NanoGPT and Llama2. Only one of the sections below should be uncommented and should match the DOCKERFILE_EXT section above
 
-## Llama2
+## Llama
 ## Register at Huggingface and get a token by visiting: https://huggingface.co/docs/hub/security-tokens, then insert your token here
 export HF_TOKEN="<insert_your_huggingface_token_here>"
+export S3_LOCATION="<insert_s3_location_where_you_want_to_upload_fine_tuned_model>"
+
 ## Llama3.2 MODEL_NAME=meta-llama/Llama-3.2-11B-Vision-Instruct
 export MODEL_NAME=meta-llama/Llama-3.2-11B-Vision-Instruct
 ## Llama3.2 train command
@@ -41,8 +43,13 @@ export CMD="huggingface-cli login --token ${HF_TOKEN} && \
 torchrun --nnodes ${NUM_WORKERS} --nproc_per_node ${GPU_PER_WORKER}  \
 recipes/quickstart/finetuning/finetuning.py --enable_fsdp --lr 1e-5  --num_epochs 5 --batch_size_training 2 \
 --model_name ${MODEL_NAME} \
---dist_checkpoint_root_folder ./finetuned_model_mind2web \
+--dist_checkpoint_root_folder /data \
 --dist_checkpoint_folder fine-tuned  --use_fast_kernels \
 --dataset custom_dataset --custom_dataset.test_split test \
  --custom_dataset.file /workspace/llama-recipes/recipes/quickstart/finetuning/datasets/mind2web_dataset.py \  
- --run_validation True --batching_strategy padding "
+ --run_validation False --batching_strategy padding "
+
+export MODEL_PROCESSOR_CMD="huggingface-cli login --token ${HF_TOKEN} && \
+python -m llama_recipes.inference.checkpoint_converter_fsdp_hf  --fsdp_checkpoint_path /data/fine-tuned-meta-llama/Llama-3.2-11B-Vision-Instruct/  --consolidated_model_path /data/llama-finetuned --HF_model_path_or_name meta-llama/Llama-3.2-11B-Vision-Instruct && \
+aws s3 cp --recursive /data/llama-finetuned ${S3_LOCATION} && \
+echo Model Uploaded"
\ No newline at end of file
diff --git a/Container-Root/eks/deployment/distributed-training/pytorch/pytorchjob/fsdp/fsdp.yaml-template b/Container-Root/eks/deployment/distributed-training/pytorch/pytorchjob/fsdp/fsdp.yaml-template
index a1c0a8d..f59f65c 100644
--- a/Container-Root/eks/deployment/distributed-training/pytorch/pytorchjob/fsdp/fsdp.yaml-template
+++ b/Container-Root/eks/deployment/distributed-training/pytorch/pytorchjob/fsdp/fsdp.yaml-template
@@ -28,15 +28,16 @@ spec:
         spec:
           volumes:
             - name: shmem
-              #emptyDir:
-              #  medium: Memory
               hostPath: 
                 path: /dev/shm
             - name: local
               hostPath:
                 path: /mnt/k8s-disks/0
-          #nodeSelector:
-          #  node.kubernetes.io/instance-type: "${INSTANCE_TYPE}"
+            - name: persistent-storage
+              persistentVolumeClaim:
+                claimName: fsx-claim
+          nodeSelector:
+            node.kubernetes.io/instance-type: "${INSTANCE_TYPE}"
           containers:
             - name: pytorch
               image: ${REGISTRY}${IMAGE}${TAG}
@@ -95,3 +96,5 @@ spec:
                   mountPath: /dev/shm
                 - name: local
                   mountPath: /local
+                - name: persistent-storage
+                  mountPath: /data
\ No newline at end of file
diff --git a/Container-Root/eks/deployment/distributed-training/pytorch/pytorchjob/fsdp/generate.sh b/Container-Root/eks/deployment/distributed-training/pytorch/pytorchjob/fsdp/generate.sh
index faad718..2d80802 100755
--- a/Container-Root/eks/deployment/distributed-training/pytorch/pytorchjob/fsdp/generate.sh
+++ b/Container-Root/eks/deployment/distributed-training/pytorch/pytorchjob/fsdp/generate.sh
@@ -16,6 +16,11 @@ cat fsdp.yaml-template | envsubst > fsdp.yaml
 
 #cat fsdp.yaml
 
+echo ""
+echo "Generating Model Processor manifest ..."
+
+cat model-processor.yaml-template | envsubst > model-processor.yaml
+
 echo "Done."
 echo ""
 
diff --git a/Container-Root/eks/deployment/distributed-training/pytorch/pytorchjob/fsdp/logs.sh b/Container-Root/eks/deployment/distributed-training/pytorch/pytorchjob/fsdp/logs.sh
index aac1090..2591da1 100755
--- a/Container-Root/eks/deployment/distributed-training/pytorch/pytorchjob/fsdp/logs.sh
+++ b/Container-Root/eks/deployment/distributed-training/pytorch/pytorchjob/fsdp/logs.sh
@@ -1,6 +1,13 @@
 #!/bin/bash
 
-. .env
+echo ""
+echo "<<<<<< Fine Tuning Logs >>>>>>"
+echo ""
 
-kubectl logs -f $JOB_NAME 
+kubectl logs -l app=fsdp
 
+echo ""
+echo "<<<<<< Model Processor Logs [Its fine to see 'No resources found in default namespace' if you are still running the fine tuning] >>>>>>]"
+echo ""
+
+kubectl logs -l app=model-processor
\ No newline at end of file
diff --git a/Container-Root/eks/deployment/distributed-training/pytorch/pytorchjob/fsdp/model-processor.yaml-template b/Container-Root/eks/deployment/distributed-training/pytorch/pytorchjob/fsdp/model-processor.yaml-template
new file mode 100644
index 0000000..8bcdcfc
--- /dev/null
+++ b/Container-Root/eks/deployment/distributed-training/pytorch/pytorchjob/fsdp/model-processor.yaml-template
@@ -0,0 +1,34 @@
+---
+apiVersion: v1
+kind: Pod
+metadata:
+  name: model-processor
+  labels:
+    app: model-processor
+spec:
+  restartPolicy: Never
+  volumes:
+    - name: shmem
+      hostPath: 
+        path: /dev/shm
+    - name: local
+      hostPath:
+        path: /mnt/k8s-disks/0
+    - name: persistent-storage
+      persistentVolumeClaim:
+        claimName: fsx-claim
+  containers:
+    - name: model-processor
+      image: ${REGISTRY}${IMAGE}${TAG}
+      imagePullPolicy: Always
+      command:
+        - bash
+        - -c
+        - "${MODEL_PROCESSOR_CMD}"
+      volumeMounts:
+        - name: shmem
+          mountPath: /dev/shm
+        - name: local
+          mountPath: /local
+        - name: persistent-storage
+          mountPath: /data
diff --git a/Container-Root/eks/deployment/distributed-training/pytorch/pytorchjob/fsdp/pvc.yaml b/Container-Root/eks/deployment/distributed-training/pytorch/pytorchjob/fsdp/pvc.yaml
new file mode 100644
index 0000000..8b4b8ef
--- /dev/null
+++ b/Container-Root/eks/deployment/distributed-training/pytorch/pytorchjob/fsdp/pvc.yaml
@@ -0,0 +1,15 @@
+apiVersion: v1
+kind: PersistentVolumeClaim
+metadata:
+  name: fsx-claim
+spec:
+  accessModes:
+    - ReadWriteMany
+  storageClassName: fsx-sc
+  resources:
+    requests:
+      storage: 1200Gi
+
+## After Creating this, wait for a couple of minutes for PVC to be created.
+## Check with command `kubectl get pvc`
+## This command takes roughly 8-10 minutes
\ No newline at end of file
-- 
2.39.5 (Apple Git-154)

