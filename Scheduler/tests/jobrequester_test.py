import concurrent.futures
import json
import logging
import os
import sys

import xml.etree.ElementTree as ET

import boto3

import log_return

from job_requester import JobRequester


LOGGER = logging.getLogger(__name__)
LOGGER.setLevel(logging.DEBUG)
LOGGER.addHandler(logging.StreamHandler(sys.stdout))

TEST_IMAGE = "754106851545.dkr.ecr.us-west-2.amazonaws.com/pr-tensorflow-training:2.2.0-gpu-py37-cu101-ubuntu18.04-example-pr-269-2020-06-11-22-13-27"
XML_REPORT_MESSAGE = '<testsuites><testsuite errors="0" failures="0" hostname="875d5ee2efe3" name="pytest" skipped="5" tests="11" time="2324.963" timestamp="2020-07-02T20:51:47.429207"><testcase classname="integration.sagemaker.test_experiments" file="integration/sagemaker/test_experiments.py" line="34" name="test_training" time="344.795"><system-out>2020-07-02 20:51:51 Starting - Starting the training job...\n2020-07-02 20:51:53 Starting - Launching requested ML instances.........\n2020-07-02 20:53:32 Starting - Preparing the instances for training...\n2020-07-02 20:54:22 Downloading - Downloading input data\n2020-07-02 20:54:22 Training - Downloading the training image.................2020-07-02 20:57:00,009 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\n2020-07-02 20:57:00,514 sagemaker-training-toolkit INFO     Invoking user script\n\nTraining Env:\n\n{\n    "additional_framework_parameters": {},\n    "channel_input_dirs": {\n        "training": "/opt/ml/input/data/training"\n    },\n    "current_host": "algo-1",\n    "framework_module": "sagemaker_tensorflow_container.training:main",\n    "hosts": [\n        "algo-1"\n    ],\n    "hyperparameters": {\n        "model_dir": "s3://sagemaker-us-west-2-754106851545/test-tf-experiments-mnist-1593723109-04bb/model"\n    },\n    "input_config_dir": "/opt/ml/input/config",\n    "input_data_config": {\n        "training": {\n            "TrainingInputMode": "File",\n            "S3DistributionType": "FullyReplicated",\n            "RecordWrapperType": "None"\n        }\n    },\n    "input_dir": "/opt/ml/input",\n    "is_master": true,\n    "job_name": "test-tf-experiments-mnist-1593723109-04bb",\n    "log_level": 20,\n    "master_hostname": "algo-1",\n    "model_dir": "/opt/ml/model",\n    "module_dir": "s3://sagemaker-us-west-2-754106851545/test-tf-experiments-mnist-1593723109-04bb/source/sourcedir.tar.gz",\n    "module_name": "mnist",\n    "network_interface_name": "eth0",\n    "num_cpus": 32,\n    "num_gpus": 4,\n    "output_data_dir": "/opt/ml/output/data",\n    "output_dir": "/opt/ml/output",\n    "output_intermediate_dir": "/opt/ml/output/intermediate",\n    "resource_config": {\n        "current_host": "algo-1",\n        "hosts": [\n            "algo-1"\n        ],\n        "network_interface_name": "eth0"\n    },\n    "user_entry_point": "mnist.py"\n}\n\nEnvironment variables:\n\nSM_HOSTS=["algo-1"]\nSM_NETWORK_INTERFACE_NAME=eth0\nSM_HPS={"model_dir":"s3://sagemaker-us-west-2-754106851545/test-tf-experiments-mnist-1593723109-04bb/model"}\nSM_USER_ENTRY_POINT=mnist.py\nSM_FRAMEWORK_PARAMS={}\nSM_RESOURCE_CONFIG={"current_host":"algo-1","hosts":["algo-1"],"network_interface_name":"eth0"}\nSM_INPUT_DATA_CONFIG={"training":{"RecordWrapperType":"None","S3DistributionType":"FullyReplicated","TrainingInputMode":"File"}}\nSM_OUTPUT_DATA_DIR=/opt/ml/output/data\nSM_CHANNELS=["training"]\nSM_CURRENT_HOST=algo-1\nSM_MODULE_NAME=mnist\nSM_LOG_LEVEL=20\nSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\nSM_INPUT_DIR=/opt/ml/input\nSM_INPUT_CONFIG_DIR=/opt/ml/input/config\nSM_OUTPUT_DIR=/opt/ml/output\nSM_NUM_CPUS=32\nSM_NUM_GPUS=4\nSM_MODEL_DIR=/opt/ml/model\nSM_MODULE_DIR=s3://sagemaker-us-west-2-754106851545/test-tf-experiments-mnist-1593723109-04bb/source/sourcedir.tar.gz\nSM_TRAINING_ENV={"additional_framework_parameters":{},"channel_input_dirs":{"training":"/opt/ml/input/data/training"},"current_host":"algo-1","framework_module":"sagemaker_tensorflow_container.training:main","hosts":["algo-1"],"hyperparameters":{"model_dir":"s3://sagemaker-us-west-2-754106851545/test-tf-experiments-mnist-1593723109-04bb/model"},"input_config_dir":"/opt/ml/input/config","input_data_config":{"training":{"RecordWrapperType":"None","S3DistributionType":"FullyReplicated","TrainingInputMode":"File"}},"input_dir":"/opt/ml/input","is_master":true,"job_name":"test-tf-experiments-mnist-1593723109-04bb","log_level":20,"master_hostname":"algo-1","model_dir":"/opt/ml/model","module_dir":"s3://sagemaker-us-west-2-754106851545/test-tf-experiments-mnist-1593723109-04bb/source/sourcedir.tar.gz","module_name":"mnist","network_interface_name":"eth0","num_cpus":32,"num_gpus":4,"output_data_dir":"/opt/ml/output/data","output_dir":"/opt/ml/output","output_intermediate_dir":"/opt/ml/output/intermediate","resource_config":{"current_host":"algo-1","hosts":["algo-1"],"network_interface_name":"eth0"},"user_entry_point":"mnist.py"}\nSM_USER_ARGS=["--model_dir","s3://sagemaker-us-west-2-754106851545/test-tf-experiments-mnist-1593723109-04bb/model"]\nSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\nSM_CHANNEL_TRAINING=/opt/ml/input/data/training\nSM_HP_MODEL_DIR=s3://sagemaker-us-west-2-754106851545/test-tf-experiments-mnist-1593723109-04bb/model\nPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\n\nInvoking script with the following command:\n\n/usr/local/bin/python3.7 mnist.py --model_dir s3://sagemaker-us-west-2-754106851545/test-tf-experiments-mnist-1593723109-04bb/model\n\n\n[2020-07-02 20:57:05.906 ip-10-0-143-139.us-west-2.compute.internal:49 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n[2020-07-02 20:57:05.906 ip-10-0-143-139.us-west-2.compute.internal:49 INFO hook.py:191] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n[2020-07-02 20:57:05.906 ip-10-0-143-139.us-west-2.compute.internal:49 INFO hook.py:236] Saving to /opt/ml/output/tensors\n[2020-07-02 20:57:05.906 ip-10-0-143-139.us-west-2.compute.internal:49 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n[2020-07-02 20:57:06.014 ip-10-0-143-139.us-west-2.compute.internal:49 INFO keras.py:68] Executing in TF2.x eager mode.SageMaker Debugger will not be saving gradients\n[2020-07-02 20:57:06.022 ip-10-0-143-139.us-west-2.compute.internal:49 INFO hook.py:376] Monitoring the collections: losses, sm_metrics, metrics\nERROR:root:\'NoneType\' object has no attribute \'write\'\n#015 1/32 [..............................] - ETA: 0s - loss: 2.3051 - accuracy: 0.0625#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01524/32 [=====================&gt;........] - ETA: 0s - loss: 1.2948 - accuracy: 0.6237#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01532/32 [==============================] - 0s 2ms/step - loss: 1.1554 - accuracy: 0.6660 - batch: 0.0000e+00\n#015 1/32 [..............................] - ETA: 0s - loss: 0.6012 - accuracy: 0.8125#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01527/32 [========================&gt;.....] - ETA: 0s - loss: 0.6999 - accuracy: 0.7859#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01532/32 [==============================] - 0s 2ms/step - loss: 0.7098 - accuracy: 0.7840\n[2020-07-02 20:57:07.524 ip-10-0-143-139.us-west-2.compute.internal:49 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\n2020-07-02 20:57:08,247 sagemaker_tensorflow_container.training WARNING  Your model will NOT be servable with SageMaker TensorFlow Serving container. The model artifact was not saved in the TensorFlow SavedModel directory structure:\nhttps://www.tensorflow.org/guide/saved_model#structure_of_a_savedmodel_directory\n2020-07-02 20:57:08,247 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n\n2020-07-02 20:57:19 Uploading - Uploading generated training model\n2020-07-02 20:57:19 Completed - Training job completed\nTraining seconds: 184\nBillable seconds: 184\n</system-out><system-err>WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\nWARNING:sagemaker:\'s3_input\' class will be renamed to \'TrainingInput\' in SageMaker Python SDK v2.\nINFO:sagemaker:Creating training-job with name: test-tf-experiments-mnist-1593723109-04bb\n</system-err></testcase><testcase classname="integration.sagemaker.test_horovod" file="integration/sagemaker/test_horovod.py" line="24" name="test_distributed_training_horovod[cpu-3]" time="434.444"><system-out>2020-07-02 20:57:34 Starting - Starting the training job...\n2020-07-02 20:57:41 Starting - Launching requested ML instances.........\n2020-07-02 20:59:18 Starting - Preparing the instances for training......\n2020-07-02 21:00:28 Downloading - Downloading input data...\n2020-07-02 21:00:36 Training - Downloading the training image..............2020-07-02 21:03:07,389 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\n2020-07-02 21:03:14,028 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\n2020-07-02 21:03:14,028 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\n2020-07-02 21:03:14,035 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\n2020-07-02 21:03:14,112 paramiko.transport INFO     Authentication (publickey) successful!\n2020-07-02 21:03:14,112 sagemaker-training-toolkit INFO     Can connect to host algo-1\n2020-07-02 21:03:14,112 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\n2020-07-02 21:03:14,118 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\n2020-07-02 21:03:11,657 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\n2020-07-02 21:03:12,110 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\n2020-07-02 21:03:12,110 sagemaker-training-toolkit INFO     Creating SSH daemon.\n2020-07-02 21:03:12,117 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\n2020-07-02 21:03:12,119 sagemaker-training-toolkit INFO     Cannot connect to host algo-2\n2020-07-02 21:03:12,119 sagemaker-training-toolkit INFO     Connection failed with exception: \n [Errno None] Unable to connect to port 22 on 10.0.138.71\n2020-07-02 21:03:13,120 sagemaker-training-toolkit INFO     Cannot connect to host algo-2\n2020-07-02 21:03:13,121 sagemaker-training-toolkit INFO     Connection failed with exception: \n [Errno None] Unable to connect to port 22 on 10.0.138.71\n2020-07-02 21:03:14,122 sagemaker-training-toolkit INFO     Cannot connect to host algo-2\n2020-07-02 21:03:14,122 sagemaker-training-toolkit INFO     Connection failed with exception: \n [Errno None] Unable to connect to port 22 on 10.0.138.71\n2020-07-02 21:03:15,129 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\n2020-07-02 21:03:15,235 paramiko.transport INFO     Authentication (publickey) successful!\n2020-07-02 21:03:15,236 sagemaker-training-toolkit INFO     Can connect to host algo-2\n2020-07-02 21:03:15,236 sagemaker-training-toolkit INFO     Worker algo-2 available for communication\n2020-07-02 21:03:15,236 sagemaker-training-toolkit INFO     Env Hosts: [\'algo-1\', \'algo-2\'] Hosts: [\'algo-1\', \'algo-2\'] process_per_hosts: 1 num_processes: 2\n2020-07-02 21:03:15,237 sagemaker-training-toolkit INFO     Network interface name: eth0\n2020-07-02 21:03:15,286 sagemaker-training-toolkit INFO     Invoking user script\n\nTraining Env:\n\n{\n    "additional_framework_parameters": {\n        "sagemaker_mpi_num_of_processes_per_host": 1,\n        "sagemaker_mpi_custom_mpi_options": "-verbose -x orte_base_help_aggregate=0",\n        "sagemaker_mpi_enabled": true\n    },\n    "channel_input_dirs": {},\n    "current_host": "algo-1",\n    "framework_module": "sagemaker_tensorflow_container.training:main",\n    "hosts": [\n        "algo-1",\n        "algo-2"\n    ],\n    "hyperparameters": {\n        "model_dir": "s3://sagemaker-us-west-2-754106851545/test-tf-horovod-1593723454-a6bd/model"\n    },\n    "input_config_dir": "/opt/ml/input/config",\n    "input_data_config": {},\n    "input_dir": "/opt/ml/input",\n    "is_master": true,\n    "job_name": "test-tf-horovod-1593723454-a6bd",\n    "log_level": 20,\n    "master_hostname": "algo-1",\n    "model_dir": "/opt/ml/model",\n    "module_dir": "s3://sagemaker-us-west-2-754106851545/test-tf-horovod-1593723454-a6bd/source/sourcedir.tar.gz",\n    "module_name": "horovod_mnist",\n    "network_interface_name": "eth0",\n    "num_cpus": 32,\n    "num_gpus": 4,\n    "output_data_dir": "/opt/ml/output/data",\n    "output_dir": "/opt/ml/output",\n    "output_intermediate_dir": "/opt/ml/output/intermediate",\n    "resource_config": {\n        "current_host": "algo-1",\n        "hosts": [\n            "algo-1",\n            "algo-2"\n        ],\n        "network_interface_name": "eth0"\n    },\n    "user_entry_point": "horovod_mnist.py"\n}\n\nEnvironment variables:\n\nSM_HOSTS=["algo-1","algo-2"]\nSM_NETWORK_INTERFACE_NAME=eth0\nSM_HPS={"model_dir":"s3://sagemaker-us-west-2-754106851545/test-tf-horovod-1593723454-a6bd/model"}\nSM_USER_ENTRY_POINT=horovod_mnist.py\nSM_FRAMEWORK_PARAMS={"sagemaker_mpi_custom_mpi_options":"-verbose -x orte_base_help_aggregate=0","sagemaker_mpi_enabled":true,"sagemaker_mpi_num_of_processes_per_host":1}\nSM_RESOURCE_CONFIG={"current_host":"algo-1","hosts":["algo-1","algo-2"],"network_interface_name":"eth0"}\nSM_INPUT_DATA_CONFIG={}\nSM_OUTPUT_DATA_DIR=/opt/ml/output/data\nSM_CHANNELS=[]\nSM_CURRENT_HOST=algo-1\nSM_MODULE_NAME=horovod_mnist\nSM_LOG_LEVEL=20\nSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\nSM_INPUT_DIR=/opt/ml/input\nSM_INPUT_CONFIG_DIR=/opt/ml/input/config\nSM_OUTPUT_DIR=/opt/ml/output\nSM_NUM_CPUS=32\nSM_NUM_GPUS=4\nSM_MODEL_DIR=/opt/ml/model\nSM_MODULE_DIR=s3://sagemaker-us-west-2-754106851545/test-tf-horovod-1593723454-a6bd/source/sourcedir.tar.gz\nSM_TRAINING_ENV={"additional_framework_parameters":{"sagemaker_mpi_custom_mpi_options":"-verbose -x orte_base_help_aggregate=0","sagemaker_mpi_enabled":true,"sagemaker_mpi_num_of_processes_per_host":1},"channel_input_dirs":{},"current_host":"algo-1","framework_module":"sagemaker_tensorflow_container.training:main","hosts":["algo-1","algo-2"],"hyperparameters":{"model_dir":"s3://sagemaker-us-west-2-754106851545/test-tf-horovod-1593723454-a6bd/model"},"input_config_dir":"/opt/ml/input/config","input_data_config":{},"input_dir":"/opt/ml/input","is_master":true,"job_name":"test-tf-horovod-1593723454-a6bd","log_level":20,"master_hostname":"algo-1","model_dir":"/opt/ml/model","module_dir":"s3://sagemaker-us-west-2-754106851545/test-tf-horovod-1593723454-a6bd/source/sourcedir.tar.gz","module_name":"horovod_mnist","network_interface_name":"eth0","num_cpus":32,"num_gpus":4,"output_data_dir":"/opt/ml/output/data","output_dir":"/opt/ml/output","output_intermediate_dir":"/opt/ml/output/intermediate","resource_config":{"current_host":"algo-1","hosts":["algo-1","algo-2"],"network_interface_name":"eth0"},"user_entry_point":"horovod_mnist.py"}\nSM_USER_ARGS=["--model_dir","s3://sagemaker-us-west-2-754106851545/test-tf-horovod-1593723454-a6bd/model"]\nSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\nSM_HP_MODEL_DIR=s3://sagemaker-us-west-2-754106851545/test-tf-horovod-1593723454-a6bd/model\nPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\n\nInvoking script with the following command:\n\nmpirun --host algo-1,algo-2 -np 2 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to socket -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/usr/local/lib/python3.7/site-packages/gethostname.cpython-37m-x86_64-linux-gnu.so -verbose -x orte_base_help_aggregate=0 -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_HP_MODEL_DIR -x PYTHONPATH /usr/local/bin/python3.7 -m mpi4py horovod_mnist.py --model_dir s3://sagemaker-us-west-2-754106851545/test-tf-horovod-1593723454-a6bd/model\n\n\nWarning: Permanently added \'algo-2,10.0.138.71\' (ECDSA) to the list of known hosts.#015\n Data for JOB [17545,1] offset 0 Total slots allocated 2\n\n ========================   JOB MAP   ========================\n\n Data for node: ip-10-0-136-63#011Num slots: 1#011Max slots: 0#011Num procs: 1\n #011Process OMPI jobid: [17545,1] App: 0 Process rank: 0 Bound: UNBOUND\n\n Data for node: algo-2#011Num slots: 1#011Max slots: 0#011Num procs: 1\n #011Process OMPI jobid: [17545,1] App: 0 Process rank: 1 Bound: N/A\n\n =============================================================\n Data for JOB [17545,1] offset 0 Total slots allocated 2\n\n ========================   JOB MAP   ========================\n\n Data for node: ip-10-0-136-63#011Num slots: 1#011Max slots: 0#011Num procs: 1\n #011Process OMPI jobid: [17545,1] App: 0 Process rank: 0 Bound: N/A\n\n Data for node: algo-2#011Num slots: 1#011Max slots: 0#011Num procs: 1\n #011Process OMPI jobid: [17545,1] App: 0 Process rank: 1 Bound: UNBOUND\n\n =============================================================\n--------------------------------------------------------------------------\nWARNING: Open MPI tried to bind a process but failed.  This is a\nwarning only; your job will continue, though performance may\nbe degraded.\n\n  Local host:        ip-10-0-136-63\n  Application name:  /usr/local/bin/python3.7\n  Error message:     failed to bind memory\n  Location:          rtc_hwloc.c:447\n\n--------------------------------------------------------------------------\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:18.469768: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:18.766084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:18.767344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n[1,1]&lt;stderr&gt;:pciBusID: 0000:00:1b.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n[1,1]&lt;stderr&gt;:coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:18.767432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:18.768635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: \n[1,1]&lt;stderr&gt;:pciBusID: 0000:00:1c.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n[1,1]&lt;stderr&gt;:coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:18.768709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:18.769909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 2 with properties: \n[1,1]&lt;stderr&gt;:pciBusID: 0000:00:1d.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n[1,1]&lt;stderr&gt;:coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:18.769981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:18.771213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 3 with properties: \n[1,1]&lt;stderr&gt;:pciBusID: 0000:00:1e.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n[1,1]&lt;stderr&gt;:coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:18.771266: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:18.840822: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:18.865406: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:18.870773: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:18.914096: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:18.919116: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:19.039793: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:19.039936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:19.041285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:19.042582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:19.043838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:19.045082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:19.046340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:19.047578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:19.048813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:19.050037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1, 2, 3\n[1,1]&lt;stdout&gt;:Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n[1,0]&lt;stdout&gt;:Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n[1,1]&lt;stdout&gt;:#015[1,1]&lt;stdout&gt;:    8192/11490434 [..............................] - ETA: 0s[1,1]&lt;stdout&gt;:#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015[1,1]&lt;stdout&gt;: 1245184/11490434 [==&gt;...........................] - ETA: 0s[1,1]&lt;stdout&gt;:#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015[1,1]&lt;stdout&gt;: 6299648/11490434 [===============&gt;..............] - ETA: 0s[1,0]&lt;stdout&gt;:#015[1,0]&lt;stdout&gt;:    8192/11490434 [..............................] - ETA: 0s[1,1]&lt;stdout&gt;:#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015[1,1]&lt;stdout&gt;: 9011200/11490434 [======================&gt;.......] - ETA: 0s[1,1]&lt;stdout&gt;:#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015[1,1]&lt;stdout&gt;:11493376/11490434 [==============================] - 0s 0us/step\n[1,0]&lt;stdout&gt;:#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015[1,0]&lt;stdout&gt;:  925696/11490434 [=&gt;............................] - ETA: 0s[1,0]&lt;stdout&gt;:#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015[1,0]&lt;stdout&gt;: 6873088/11490434 [================&gt;.............] - ETA: 0s[1,0]&lt;stdout&gt;:#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015[1,0]&lt;stdout&gt;:11493376/11490434 [==============================] - 0s 0us/step\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:19.857882: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:19.894151: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2300045000 Hz\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:19.895796: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fcb48000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:19.895823: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:20.253715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:20.255208: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c94c50fa20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:20.255238: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:20.256123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:20.257340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n[1,1]&lt;stderr&gt;:pciBusID: 0000:00:1b.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n[1,1]&lt;stderr&gt;:coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:20.257393: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:20.257433: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:20.257453: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:20.257471: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:20.257489: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:20.257513: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:20.257532: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:20.257599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:20.258863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:20.260038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:20.260709: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:20.353623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:20.353666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n[1,1]&lt;stderr&gt;:2020-07-02 21:03:20.353675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n[1,1]&lt;stderr&gt;:2020-07-02 21:03:20.354455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:20.355737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:20.356960: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14878 MB memory) -&gt; physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1b.0, compute capability: 7.0)\n[ip-10-0-136-63.us-west-2.compute.internal:00064] 1 more process has sent help message help-orte-odls-default.txt / memory not bound\n[ip-10-0-136-63.us-west-2.compute.internal:00064] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages\n[1,1]&lt;stdout&gt;:[2020-07-02 21:03:22.877 algo-2:82 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n[1,1]&lt;stdout&gt;:[2020-07-02 21:03:22.877 algo-2:82 INFO hook.py:191] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n[1,1]&lt;stdout&gt;:[2020-07-02 21:03:22.877 algo-2:82 INFO hook.py:236] Saving to /opt/ml/output/tensors\n[1,1]&lt;stdout&gt;:[2020-07-02 21:03:22.877 algo-2:82 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n[1,1]&lt;stdout&gt;:[2020-07-02 21:03:22.878 algo-2:82 INFO hook.py:376] Monitoring the collections: sm_metrics, losses, metrics\n[1,0]&lt;stdout&gt;:[2020-07-02 21:03:22.910 algo-1:70 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n[1,0]&lt;stdout&gt;:[2020-07-02 21:03:22.910 algo-1:70 INFO hook.py:191] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n[1,0]&lt;stdout&gt;:[2020-07-02 21:03:22.911 algo-1:70 INFO hook.py:236] Saving to /opt/ml/output/tensors\n[1,0]&lt;stdout&gt;:[2020-07-02 21:03:22.911 algo-1:70 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n[1,0]&lt;stdout&gt;:[2020-07-02 21:03:22.911 algo-1:70 INFO hook.py:376] Monitoring the collections: losses, metrics, sm_metrics\n\n2020-07-02 21:03:07 Training - Training image download completed. Training in progress.[1,1]&lt;stderr&gt;:2020-07-02 21:03:25.503976: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n[1,1]&lt;stderr&gt;:2020-07-02 21:03:26.116973: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n[1,0]&lt;stdout&gt;:algo-1:70:107 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.136.63&lt;0&gt;\n[1,0]&lt;stdout&gt;:algo-1:70:107 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\n[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:algo-1:70:107 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\n[1,0]&lt;stdout&gt;:NCCL version 2.4.7+cuda10.1\n[1,1]&lt;stdout&gt;:algo-2:82:119 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.138.71&lt;0&gt;\n[1,1]&lt;stdout&gt;:algo-2:82:119 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\n[1,1]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:algo-2:82:119 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\n[1,1]&lt;stdout&gt;:algo-2:82:119 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff\n[1,0]&lt;stdout&gt;:algo-1:70:107 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff\n[1,0]&lt;stdout&gt;:algo-1:70:107 [0] NCCL INFO CUDA Dev 0[0], Socket NIC distance :  PHB\n[1,1]&lt;stdout&gt;:algo-2:82:119 [0] NCCL INFO CUDA Dev 0[0], Socket NIC distance :  PHB\n[1,0]&lt;stdout&gt;:algo-1:70:107 [0] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\n[1,0]&lt;stdout&gt;:algo-1:70:107 [0] NCCL INFO Duplicating rings to 4 per user request.\n[1,0]&lt;stdout&gt;:algo-1:70:107 [0] NCCL INFO Channel 00 :    0   1\n[1,0]&lt;stdout&gt;:algo-1:70:107 [0] NCCL INFO Channel 01 :    0   1\n[1,0]&lt;stdout&gt;:algo-1:70:107 [0] NCCL INFO Channel 02 :    0   1\n[1,0]&lt;stdout&gt;:algo-1:70:107 [0] NCCL INFO Channel 03 :    0   1\n[1,1]&lt;stdout&gt;:algo-2:82:119 [0] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\n[1,0]&lt;stdout&gt;:algo-1:70:107 [0] NCCL INFO Ring 00 : 1 -&gt; 0 [receive] via NET/Socket/0\n[1,1]&lt;stdout&gt;:algo-2:82:119 [0] NCCL INFO Ring 00 : 0 -&gt; 1 [receive] via NET/Socket/0\n[1,0]&lt;stdout&gt;:algo-1:70:107 [0] NCCL INFO Ring 00 : 0 -&gt; 1 [send] via NET/Socket/0\n[1,1]&lt;stdout&gt;:algo-2:82:119 [0] NCCL INFO Ring 00 : 1 -&gt; 0 [send] via NET/Socket/0\n[1,0]&lt;stdout&gt;:algo-1:70:107 [0] NCCL INFO Ring 01 : 1 -&gt; 0 [receive] via NET/Socket/0\n[1,1]&lt;stdout&gt;:algo-2:82:119 [0] NCCL INFO Ring 01 : 0 -&gt; 1 [receive] via NET/Socket/0\n[1,0]&lt;stdout&gt;:algo-1:70:107 [0] NCCL INFO Ring 01 : 0 -&gt; 1 [send] via NET/Socket/0\n[1,1]&lt;stdout&gt;:algo-2:82:119 [0] NCCL INFO Ring 01 : 1 -&gt; 0 [send] via NET/Socket/0\n[1,0]&lt;stdout&gt;:algo-1:70:107 [0] NCCL INFO Ring 02 : 1 -&gt; 0 [receive] via NET/Socket/0\n[1,1]&lt;stdout&gt;:algo-2:82:119 [0] NCCL INFO Ring 02 : 0 -&gt; 1 [receive] via NET/Socket/0\n[1,0]&lt;stdout&gt;:algo-1:70:107 [0] NCCL INFO Ring 02 : 0 -&gt; 1 [send] via NET/Socket/0\n[1,1]&lt;stdout&gt;:algo-2:82:119 [0] NCCL INFO Ring 02 : 1 -&gt; 0 [send] via NET/Socket/0\n[1,0]&lt;stdout&gt;:algo-1:70:107 [0] NCCL INFO Ring 03 : 1 -&gt; 0 [receive] via NET/Socket/0\n[1,1]&lt;stdout&gt;:algo-2:82:119 [0] NCCL INFO Ring 03 : 0 -&gt; 1 [receive] via NET/Socket/0\n[1,0]&lt;stdout&gt;:algo-1:70:107 [0] NCCL INFO Ring 03 : 0 -&gt; 1 [send] via NET/Socket/0\n[1,1]&lt;stdout&gt;:algo-2:82:119 [0] NCCL INFO Ring 03 : 1 -&gt; 0 [send] via NET/Socket/0\n[1,0]&lt;stdout&gt;:algo-1:70:107 [0] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees disabled\n[1,0]&lt;stdout&gt;:algo-1:70:107 [0] NCCL INFO comm 0x7f9694314690 rank 0 nranks 2 cudaDev 0 nvmlDev 0 - Init COMPLETE\n[1,1]&lt;stdout&gt;:algo-2:82:119 [0] NCCL INFO comm 0x7fcc883140c0 rank 1 nranks 2 cudaDev 0 nvmlDev 0 - Init COMPLETE\n[1,0]&lt;stdout&gt;:algo-1:70:107 [0] NCCL INFO Launch mode Parallel\n[1,0]&lt;stdout&gt;:Step #0#011Loss: 2.307808\n[1,1]&lt;stdout&gt;:Step #0#011Loss: 2.327379\n[1,0]&lt;stdout&gt;:Step #10#011Loss: 0.742810\n[1,1]&lt;stdout&gt;:Step #10#011Loss: 0.781632\n[1,0]&lt;stdout&gt;:Step #20#011Loss: 0.641009\n[1,1]&lt;stdout&gt;:Step #20#011Loss: 0.598567\n[1,0]&lt;stdout&gt;:Step #30#011Loss: 0.335672\n[1,1]&lt;stdout&gt;:Step #30#011Loss: 0.420820\n[1,0]&lt;stdout&gt;:Step #40#011Loss: 0.377620\n[1,1]&lt;stdout&gt;:Step #40#011Loss: 0.285831\n[1,0]&lt;stdout&gt;:Step #50#011Loss: 0.202327\n[1,1]&lt;stdout&gt;:Step #50#011Loss: 0.233630\n[1,0]&lt;stdout&gt;:Step #60#011Loss: 0.256843\n[1,1]&lt;stdout&gt;:Step #60#011Loss: 0.279651\n[1,0]&lt;stdout&gt;:Step #70#011Loss: 0.095820\n[1,1]&lt;stdout&gt;:Step #70#011Loss: 0.227738\n[1,0]&lt;stdout&gt;:Step #80#011Loss: 0.214378\n[1,1]&lt;stdout&gt;:Step #80#011Loss: 0.154876\n[1,0]&lt;stdout&gt;:Step #90#011Loss: 0.146009\n[1,1]&lt;stdout&gt;:Step #90#011Loss: 0.238503\n[1,0]&lt;stdout&gt;:Step #100#011Loss: 0.185604\n[1,1]&lt;stdout&gt;:Step #100#011Loss: 0.172140\n[1,0]&lt;stdout&gt;:Step #110#011Loss: 0.119801\n[1,1]&lt;stdout&gt;:Step #110#011Loss: 0.195974\n[1,0]&lt;stdout&gt;:Step #120#011Loss: 0.100198\n[1,1]&lt;stdout&gt;:Step #120#011Loss: 0.100828\n[1,0]&lt;stdout&gt;:Step #130#011Loss: 0.098939\n[1,1]&lt;stdout&gt;:Step #130#011Loss: 0.158118\n[1,0]&lt;stdout&gt;:Step #140#011Loss: 0.079507\n[1,1]&lt;stdout&gt;:Step #140#011Loss: 0.167455\n[1,0]&lt;stdout&gt;:Step #150#011Loss: 0.093469\n[1,1]&lt;stdout&gt;:Step #150#011Loss: 0.107673\n[1,0]&lt;stdout&gt;:Step #160#011Loss: 0.055061\n[1,1]&lt;stdout&gt;:Step #160#011Loss: 0.081923\n[1,0]&lt;stdout&gt;:Step #170#011Loss: 0.364181\n[1,1]&lt;stdout&gt;:Step #170#011Loss: 0.143270\n[1,0]&lt;stdout&gt;:Step #180#011Loss: 0.077359\n[1,1]&lt;stdout&gt;:Step #180#011Loss: 0.087907\n[1,0]&lt;stdout&gt;:Step #190#011Loss: 0.104382\n[1,1]&lt;stdout&gt;:Step #190#011Loss: 0.068644\n[1,0]&lt;stdout&gt;:Step #200#011Loss: 0.049583\n[1,1]&lt;stdout&gt;:Step #200#011Loss: 0.108937\n[1,0]&lt;stdout&gt;:Step #210#011Loss: 0.039036\n[1,1]&lt;stdout&gt;:Step #210#011Loss: 0.050398\n[1,0]&lt;stdout&gt;:Step #220#011Loss: 0.108539\n[1,1]&lt;stdout&gt;:Step #220#011Loss: 0.059005\n[1,0]&lt;stdout&gt;:Step #230#011Loss: 0.089835\n[1,1]&lt;stdout&gt;:Step #230#011Loss: 0.068340\n[1,0]&lt;stdout&gt;:Step #240#011Loss: 0.148662\n[1,1]&lt;stdout&gt;:Step #240#011Loss: 0.089224\n[1,0]&lt;stdout&gt;:Step #250#011Loss: 0.071542\n[1,1]&lt;stdout&gt;:Step #250#011Loss: 0.149340\n[1,0]&lt;stdout&gt;:Step #260#011Loss: 0.171439\n[1,1]&lt;stdout&gt;:Step #260#011Loss: 0.108011\n[1,0]&lt;stdout&gt;:Step #270#011Loss: 0.163448\n[1,1]&lt;stdout&gt;:Step #270#011Loss: 0.054769\n[1,0]&lt;stdout&gt;:Step #280#011Loss: 0.035286\n[1,1]&lt;stdout&gt;:Step #280#011Loss: 0.070977\n[1,0]&lt;stdout&gt;:Step #290#011Loss: 0.061684\n[1,1]&lt;stdout&gt;:Step #290#011Loss: 0.050713\n[1,0]&lt;stderr&gt;:2020-07-02 21:03:32.613565: W tensorflow/python/util/util.cc:329] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n[1,0]&lt;stderr&gt;:WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n[1,0]&lt;stderr&gt;:Instructions for updating:\n[1,0]&lt;stderr&gt;:If using Keras pass *_constraint arguments to layers.\n[1,0]&lt;stderr&gt;:WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n[1,0]&lt;stderr&gt;:Instructions for updating:\n[1,0]&lt;stderr&gt;:If using Keras pass *_constraint arguments to layers.\n[1,0]&lt;stderr&gt;:INFO:tensorflow:Assets written to: /opt/ml/model/mnist/1/assets\n[1,0]&lt;stderr&gt;:INFO:tensorflow:Assets written to: /opt/ml/model/mnist/1/assets\n[1,0]&lt;stdout&gt;:[2020-07-02 21:03:33.200 algo-1:70 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\n2020-07-02 21:03:34,234 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n2020-07-02 21:04:04,303 sagemaker-training-toolkit INFO     MPI process finished.\n2020-07-02 21:04:04,303 sagemaker_tensorflow_container.training WARNING  No model artifact is saved under path /opt/ml/model. Your training job will not save any model files to S3.\nFor details of how to construct your training script see:\nhttps://sagemaker.readthedocs.io/en/stable/using_tf.html#adapting-your-local-tensorflow-script\n2020-07-02 21:04:04,303 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n\n2020-07-02 21:04:17 Uploading - Uploading generated training model\n2020-07-02 21:04:17 Completed - Training job completed\nTraining seconds: 458\nBillable seconds: 458\n</system-out><system-err>WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\nINFO:sagemaker:Creating training-job with name: test-tf-horovod-1593723454-a6bd\n</system-err></testcase><testcase classname="integration.sagemaker.test_mnist" file="integration/sagemaker/test_mnist.py" line="27" name="test_mnist[cpu-3]" time="0.000"><skipped message="Skipping test in PR context to speed up iteration time. Test will be run in nightly/release pipeline." type="pytest.skip">integration/sagemaker/test_mnist.py:28: Skipping test in PR context to speed up iteration time. Test will be run in nightly/release pipeline.</skipped></testcase><testcase classname="integration.sagemaker.test_mnist" file="integration/sagemaker/test_mnist.py" line="47" name="test_distributed_mnist_no_ps[cpu-3]" time="0.000"><skipped message="Skipping test in PR context to speed up iteration time. Test will be run in nightly/release pipeline." type="pytest.skip">integration/sagemaker/test_mnist.py:48: Skipping test in PR context to speed up iteration time. Test will be run in nightly/release pipeline.</skipped></testcase><testcase classname="integration.sagemaker.test_mnist" file="integration/sagemaker/test_mnist.py" line="66" name="test_distributed_mnist_ps[cpu-3]" time="466.018"><system-out>2020-07-02 21:04:49 Starting - Starting the training job...\n2020-07-02 21:04:52 Starting - Launching requested ML instances......\n2020-07-02 21:06:02 Starting - Preparing the instances for training.........\n2020-07-02 21:07:46 Downloading - Downloading input data...\n2020-07-02 21:07:54 Training - Downloading the training image............\n2020-07-02 21:10:08 Training - Training image download completed. Training in progress.2020-07-02 21:10:08,227 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\n2020-07-02 21:10:08,579 sagemaker_tensorflow_container.training INFO     Running distributed training job with parameter servers\n2020-07-02 21:10:08,579 sagemaker_tensorflow_container.training INFO     Launching parameter server process\n2020-07-02 21:10:08,579 sagemaker_tensorflow_container.training INFO     Running distributed training job with parameter servers\n2020-07-02 21:10:09,194 sagemaker_tensorflow_container.training INFO     Launching worker process\n2020-07-02 21:10:10,861 sagemaker-training-toolkit INFO     Invoking user script\n\nTraining Env:\n\n{\n    "additional_framework_parameters": {\n        "sagemaker_parameter_server_enabled": true\n    },\n    "channel_input_dirs": {\n        "training": "/opt/ml/input/data/training"\n    },\n    "current_host": "algo-2",\n    "framework_module": "sagemaker_tensorflow_container.training:main",\n    "hosts": [\n        "algo-1",\n        "algo-2"\n    ],\n    "hyperparameters": {\n        "model_dir": "s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/model"\n    },\n    "input_config_dir": "/opt/ml/input/config",\n    "input_data_config": {\n        "training": {\n            "TrainingInputMode": "File",\n            "S3DistributionType": "FullyReplicated",\n            "RecordWrapperType": "None"\n        }\n    },\n    "input_dir": "/opt/ml/input",\n    "is_master": false,\n    "job_name": "test-tf-sm-distributed-mnist-1593723889-2166",\n    "log_level": 20,\n    "master_hostname": "algo-1",\n    "model_dir": "/opt/ml/model",\n    "module_dir": "s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/source/sourcedir.tar.gz",\n    "module_name": "mnist_estimator",\n    "network_interface_name": "eth0",\n    "num_cpus": 32,\n    "num_gpus": 4,\n    "output_data_dir": "/opt/ml/output/data",\n    "output_dir": "/opt/ml/output",\n    "output_intermediate_dir": "/opt/ml/output/intermediate",\n    "resource_config": {\n        "current_host": "algo-2",\n        "hosts": [\n            "algo-1",\n            "algo-2"\n        ],\n        "network_interface_name": "eth0"\n    },\n    "user_entry_point": "mnist_estimator.py"\n}\n\nEnvironment variables:\n\nSM_HOSTS=["algo-1","algo-2"]\nSM_NETWORK_INTERFACE_NAME=eth0\nSM_HPS={"model_dir":"s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/model"}\nSM_USER_ENTRY_POINT=mnist_estimator.py\nSM_FRAMEWORK_PARAMS={"sagemaker_parameter_server_enabled":true}\nSM_RESOURCE_CONFIG={"current_host":"algo-2","hosts":["algo-1","algo-2"],"network_interface_name":"eth0"}\nSM_INPUT_DATA_CONFIG={"training":{"RecordWrapperType":"None","S3DistributionType":"FullyReplicated","TrainingInputMode":"File"}}\nSM_OUTPUT_DATA_DIR=/opt/ml/output/data\nSM_CHANNELS=["training"]\nSM_CURRENT_HOST=algo-2\nSM_MODULE_NAME=mnist_estimator\nSM_LOG_LEVEL=20\nSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\nSM_INPUT_DIR=/opt/ml/input\nSM_INPUT_CONFIG_DIR=/opt/ml/input/config\nSM_OUTPUT_DIR=/opt/ml/output\nSM_NUM_CPUS=32\nSM_NUM_GPUS=4\nSM_MODEL_DIR=/opt/ml/model\nSM_MODULE_DIR=s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/source/sourcedir.tar.gz\nSM_TRAINING_ENV={"additional_framework_parameters":{"sagemaker_parameter_server_enabled":true},"channel_input_dirs":{"training":"/opt/ml/input/data/training"},"current_host":"algo-2","framework_module":"sagemaker_tensorflow_container.training:main","hosts":["algo-1","algo-2"],"hyperparameters":{"model_dir":"s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/model"},"input_config_dir":"/opt/ml/input/config","input_data_config":{"training":{"RecordWrapperType":"None","S3DistributionType":"FullyReplicated","TrainingInputMode":"File"}},"input_dir":"/opt/ml/input","is_master":false,"job_name":"test-tf-sm-distributed-mnist-1593723889-2166","log_level":20,"master_hostname":"algo-1","model_dir":"/opt/ml/model","module_dir":"s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/source/sourcedir.tar.gz","module_name":"mnist_estimator","network_interface_name":"eth0","num_cpus":32,"num_gpus":4,"output_data_dir":"/opt/ml/output/data","output_dir":"/opt/ml/output","output_intermediate_dir":"/opt/ml/output/intermediate","resource_config":{"current_host":"algo-2","hosts":["algo-1","algo-2"],"network_interface_name":"eth0"},"user_entry_point":"mnist_estimator.py"}\nSM_USER_ARGS=["--model_dir","s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/model"]\nSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\nSM_CHANNEL_TRAINING=/opt/ml/input/data/training\nSM_HP_MODEL_DIR=s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/model\nTF_CONFIG={"cluster": {"master": ["algo-1:2222"], "ps": ["algo-1:2223", "algo-2:2223"], "worker": ["algo-2:2222"]}, "environment": "cloud", "task": {"index": 0, "type": "worker"}}\nPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\n\nInvoking script with the following command:\n\n/usr/local/bin/python3.7 mnist_estimator.py --model_dir s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/model\n\n\ntrain /opt/ml/input/data/training\nmodel_dir s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/model\nmax_steps 200\nsave_checkpoint_steps 200\nthrottle_secs 60\nhosts [\'algo-1\', \'algo-2\']\ncurrent_host algo-2\nbatch_size 100\nexport_model_during_training False\nINFO:tensorflow:TF_CONFIG environment variable: {\'cluster\': {\'master\': [\'algo-1:2222\'], \'ps\': [\'algo-1:2223\', \'algo-2:2223\'], \'worker\': [\'algo-2:2222\']}, \'environment\': \'cloud\', \'task\': {\'index\': 0, \'type\': \'worker\'}}\nINFO:tensorflow:Using config: {\'_model_dir\': \'s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/model\', \'_tf_random_seed\': None, \'_save_summary_steps\': 100, \'_save_checkpoints_steps\': 200, \'_save_checkpoints_secs\': None, \'_session_config\': device_filters: "/job:ps"\ndevice_filters: "/job:worker/task:0"\nallow_soft_placement: true\ngraph_options {\n  rewrite_options {\n    meta_optimizer_iterations: ONE\n  }\n}\n, \'_keep_checkpoint_max\': 5, \'_keep_checkpoint_every_n_hours\': 10000, \'_log_step_count_steps\': 100, \'_train_distribute\': None, \'_device_fn\': None, \'_protocol\': None, \'_eval_distribute\': None, \'_experimental_distribute\': None, \'_experimental_max_worker_delay_secs\': None, \'_session_creation_timeout_secs\': 7200, \'_service\': None, \'_cluster_spec\': ClusterSpec({\'master\': [\'algo-1:2222\'], \'ps\': [\'algo-1:2223\', \'algo-2:2223\'], \'worker\': [\'algo-2:2222\']}), \'_task_type\': \'worker\', \'_task_id\': 0, \'_global_id_in_cluster\': 1, \'_master\': \'grpc://algo-2:2222\', \'_evaluation_master\': \'\', \'_num_ps_replicas\': 2, \'_num_worker_replicas\': 2, \'_is_chief\': False}\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/util/lazy_loader.py:63: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n\nWARNING:tensorflow:From mnist_estimator.py:157: The name tf.estimator.inputs.numpy_input_fn is deprecated. Please use tf.compat.v1.estimator.inputs.numpy_input_fn instead.\n\nINFO:tensorflow:Not using Distribute Coordinator.\nINFO:tensorflow:Start Tensorflow server.\nINFO:tensorflow:Waiting 5 secs before starting training.\n2020-07-02 21:10:12,195 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\n2020-07-02 21:10:12,445 sagemaker_tensorflow_container.training INFO     Running distributed training job with parameter servers\n2020-07-02 21:10:12,445 sagemaker_tensorflow_container.training INFO     Launching parameter server process\n2020-07-02 21:10:12,446 sagemaker_tensorflow_container.training INFO     Running distributed training job with parameter servers\n2020-07-02 21:10:12,996 sagemaker_tensorflow_container.training INFO     Launching worker process\n2020-07-02 21:10:13,222 sagemaker-training-toolkit INFO     Invoking user script\n\nTraining Env:\n\n{\n    "additional_framework_parameters": {\n        "sagemaker_parameter_server_enabled": true\n    },\n    "channel_input_dirs": {\n        "training": "/opt/ml/input/data/training"\n    },\n    "current_host": "algo-1",\n    "framework_module": "sagemaker_tensorflow_container.training:main",\n    "hosts": [\n        "algo-1",\n        "algo-2"\n    ],\n    "hyperparameters": {\n        "model_dir": "s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/model"\n    },\n    "input_config_dir": "/opt/ml/input/config",\n    "input_data_config": {\n        "training": {\n            "TrainingInputMode": "File",\n            "S3DistributionType": "FullyReplicated",\n            "RecordWrapperType": "None"\n        }\n    },\n    "input_dir": "/opt/ml/input",\n    "is_master": true,\n    "job_name": "test-tf-sm-distributed-mnist-1593723889-2166",\n    "log_level": 20,\n    "master_hostname": "algo-1",\n    "model_dir": "/opt/ml/model",\n    "module_dir": "s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/source/sourcedir.tar.gz",\n    "module_name": "mnist_estimator",\n    "network_interface_name": "eth0",\n    "num_cpus": 32,\n    "num_gpus": 4,\n    "output_data_dir": "/opt/ml/output/data",\n    "output_dir": "/opt/ml/output",\n    "output_intermediate_dir": "/opt/ml/output/intermediate",\n    "resource_config": {\n        "current_host": "algo-1",\n        "hosts": [\n            "algo-1",\n            "algo-2"\n        ],\n        "network_interface_name": "eth0"\n    },\n    "user_entry_point": "mnist_estimator.py"\n}\n\nEnvironment variables:\n\nSM_HOSTS=["algo-1","algo-2"]\nSM_NETWORK_INTERFACE_NAME=eth0\nSM_HPS={"model_dir":"s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/model"}\nSM_USER_ENTRY_POINT=mnist_estimator.py\nSM_FRAMEWORK_PARAMS={"sagemaker_parameter_server_enabled":true}\nSM_RESOURCE_CONFIG={"current_host":"algo-1","hosts":["algo-1","algo-2"],"network_interface_name":"eth0"}\nSM_INPUT_DATA_CONFIG={"training":{"RecordWrapperType":"None","S3DistributionType":"FullyReplicated","TrainingInputMode":"File"}}\nSM_OUTPUT_DATA_DIR=/opt/ml/output/data\nSM_CHANNELS=["training"]\nSM_CURRENT_HOST=algo-1\nSM_MODULE_NAME=mnist_estimator\nSM_LOG_LEVEL=20\nSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\nSM_INPUT_DIR=/opt/ml/input\nSM_INPUT_CONFIG_DIR=/opt/ml/input/config\nSM_OUTPUT_DIR=/opt/ml/output\nSM_NUM_CPUS=32\nSM_NUM_GPUS=4\nSM_MODEL_DIR=/opt/ml/model\nSM_MODULE_DIR=s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/source/sourcedir.tar.gz\nSM_TRAINING_ENV={"additional_framework_parameters":{"sagemaker_parameter_server_enabled":true},"channel_input_dirs":{"training":"/opt/ml/input/data/training"},"current_host":"algo-1","framework_module":"sagemaker_tensorflow_container.training:main","hosts":["algo-1","algo-2"],"hyperparameters":{"model_dir":"s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/model"},"input_config_dir":"/opt/ml/input/config","input_data_config":{"training":{"RecordWrapperType":"None","S3DistributionType":"FullyReplicated","TrainingInputMode":"File"}},"input_dir":"/opt/ml/input","is_master":true,"job_name":"test-tf-sm-distributed-mnist-1593723889-2166","log_level":20,"master_hostname":"algo-1","model_dir":"/opt/ml/model","module_dir":"s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/source/sourcedir.tar.gz","module_name":"mnist_estimator","network_interface_name":"eth0","num_cpus":32,"num_gpus":4,"output_data_dir":"/opt/ml/output/data","output_dir":"/opt/ml/output","output_intermediate_dir":"/opt/ml/output/intermediate","resource_config":{"current_host":"algo-1","hosts":["algo-1","algo-2"],"network_interface_name":"eth0"},"user_entry_point":"mnist_estimator.py"}\nSM_USER_ARGS=["--model_dir","s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/model"]\nSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\nSM_CHANNEL_TRAINING=/opt/ml/input/data/training\nSM_HP_MODEL_DIR=s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/model\nTF_CONFIG={"cluster": {"master": ["algo-1:2222"], "ps": ["algo-1:2223", "algo-2:2223"], "worker": ["algo-2:2222"]}, "environment": "cloud", "task": {"index": 0, "type": "master"}}\nPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\n\nInvoking script with the following command:\n\n/usr/local/bin/python3.7 mnist_estimator.py --model_dir s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/model\n\n\ntrain /opt/ml/input/data/training\nmodel_dir s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/model\nmax_steps 200\nsave_checkpoint_steps 200\nthrottle_secs 60\nhosts [\'algo-1\', \'algo-2\']\ncurrent_host algo-1\nbatch_size 100\nexport_model_during_training False\nINFO:tensorflow:TF_CONFIG environment variable: {\'cluster\': {\'master\': [\'algo-1:2222\'], \'ps\': [\'algo-1:2223\', \'algo-2:2223\'], \'worker\': [\'algo-2:2222\']}, \'environment\': \'cloud\', \'task\': {\'index\': 0, \'type\': \'master\'}}\nINFO:tensorflow:Using config: {\'_model_dir\': \'s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/model\', \'_tf_random_seed\': None, \'_save_summary_steps\': 100, \'_save_checkpoints_steps\': 200, \'_save_checkpoints_secs\': None, \'_session_config\': device_filters: "/job:ps"\ndevice_filters: "/job:master"\nallow_soft_placement: true\ngraph_options {\n  rewrite_options {\n    meta_optimizer_iterations: ONE\n  }\n}\n, \'_keep_checkpoint_max\': 5, \'_keep_checkpoint_every_n_hours\': 10000, \'_log_step_count_steps\': 100, \'_train_distribute\': None, \'_device_fn\': None, \'_protocol\': None, \'_eval_distribute\': None, \'_experimental_distribute\': None, \'_experimental_max_worker_delay_secs\': None, \'_session_creation_timeout_secs\': 7200, \'_service\': None, \'_cluster_spec\': ClusterSpec({\'master\': [\'algo-1:2222\'], \'ps\': [\'algo-1:2223\', \'algo-2:2223\'], \'worker\': [\'algo-2:2222\']}), \'_task_type\': \'master\', \'_task_id\': 0, \'_global_id_in_cluster\': 0, \'_master\': \'grpc://algo-1:2222\', \'_evaluation_master\': \'\', \'_num_ps_replicas\': 2, \'_num_worker_replicas\': 2, \'_is_chief\': True}\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/util/lazy_loader.py:63: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n\nWARNING:tensorflow:From mnist_estimator.py:157: The name tf.estimator.inputs.numpy_input_fn is deprecated. Please use tf.compat.v1.estimator.inputs.numpy_input_fn instead.\n\nINFO:tensorflow:Not using Distribute Coordinator.\nINFO:tensorflow:Start Tensorflow server.\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:65: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nTo construct input pipelines, use the `tf.data` module.\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:491: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nTo construct input pipelines, use the `tf.data` module.\nINFO:tensorflow:Calling model_fn.\nWARNING:tensorflow:From mnist_estimator.py:32: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.keras.layers.Conv2D` instead.\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `layer.__call__` method instead.\nWARNING:tensorflow:From mnist_estimator.py:38: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.MaxPooling2D instead.\nWARNING:tensorflow:From mnist_estimator.py:67: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.Dense instead.\nWARNING:tensorflow:From mnist_estimator.py:71: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.dropout instead.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Create CheckpointSaverHook.\n[2020-07-02 21:10:16.954 ip-10-0-206-88.us-west-2.compute.internal:263 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n[2020-07-02 21:10:16.954 ip-10-0-206-88.us-west-2.compute.internal:263 INFO hook.py:191] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n[2020-07-02 21:10:16.954 ip-10-0-206-88.us-west-2.compute.internal:263 INFO hook.py:236] Saving to /opt/ml/output/tensors\n[2020-07-02 21:10:16.955 ip-10-0-206-88.us-west-2.compute.internal:263 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n[2020-07-02 21:10:17.279 ip-10-0-206-88.us-west-2.compute.internal:263 INFO hook.py:376] Monitoring the collections: sm_metrics, losses, metrics\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:912: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nTo construct input pipelines, use the `tf.data` module.\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:912: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nTo construct input pipelines, use the `tf.data` module.\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\nINFO:tensorflow:Saving checkpoints for 0 into s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/model/model.ckpt.\nINFO:tensorflow:Saving checkpoints for 0 into s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/model/model.ckpt.\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:65: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nTo construct input pipelines, use the `tf.data` module.\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:491: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nTo construct input pipelines, use the `tf.data` module.\nINFO:tensorflow:Calling model_fn.\nWARNING:tensorflow:From mnist_estimator.py:32: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.keras.layers.Conv2D` instead.\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `layer.__call__` method instead.\nWARNING:tensorflow:From mnist_estimator.py:38: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.MaxPooling2D instead.\nWARNING:tensorflow:From mnist_estimator.py:67: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.Dense instead.\nWARNING:tensorflow:From mnist_estimator.py:71: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.dropout instead.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Create CheckpointSaverHook.\n[2020-07-02 21:10:19.482 ip-10-0-240-109.us-west-2.compute.internal:263 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n[2020-07-02 21:10:19.483 ip-10-0-240-109.us-west-2.compute.internal:263 INFO hook.py:191] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n[2020-07-02 21:10:19.483 ip-10-0-240-109.us-west-2.compute.internal:263 INFO hook.py:236] Saving to /opt/ml/output/tensors\n[2020-07-02 21:10:19.483 ip-10-0-240-109.us-west-2.compute.internal:263 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n[2020-07-02 21:10:19.693 ip-10-0-240-109.us-west-2.compute.internal:263 INFO hook.py:376] Monitoring the collections: metrics, sm_metrics, losses\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/session.py:304: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.compat.v1.graph_util.extract_sub_graph`\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/session.py:304: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.compat.v1.graph_util.extract_sub_graph`\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:912: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nTo construct input pipelines, use the `tf.data` module.\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py:912: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nTo construct input pipelines, use the `tf.data` module.\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/session.py:304: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.compat.v1.graph_util.extract_sub_graph`\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/session.py:304: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.compat.v1.graph_util.extract_sub_graph`\nINFO:tensorflow:loss = 2.3171945, step = 0\nINFO:tensorflow:loss = 2.3171945, step = 0\nERROR:root:\'NoneType\' object has no attribute \'write\'\nINFO:tensorflow:loss = 2.313506, step = 0\nINFO:tensorflow:loss = 2.313506, step = 0\nERROR:root:\'NoneType\' object has no attribute \'write\'\nINFO:tensorflow:global_step/sec: 68.7084\nINFO:tensorflow:global_step/sec: 68.7084\nINFO:tensorflow:loss = 2.2808747, step = 167 (2.395 sec)\nINFO:tensorflow:loss = 2.2808747, step = 167 (2.395 sec)\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 202...\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 202...\nINFO:tensorflow:Saving checkpoints for 202 into s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/model/model.ckpt.\nINFO:tensorflow:Saving checkpoints for 202 into s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/model/model.ckpt.\nINFO:tensorflow:Loss for final step: 2.282173.\nINFO:tensorflow:Loss for final step: 2.282173.\n[2020-07-02 21:10:28.263 ip-10-0-240-109.us-west-2.compute.internal:263 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\n2020-07-02 21:10:29,383 sagemaker_tensorflow_container.training INFO     master algo-1 is still up, waiting for it to exit\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 202...\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 202...\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Starting evaluation at 2020-07-02T21:10:29Z\nINFO:tensorflow:Starting evaluation at 2020-07-02T21:10:29Z\n[2020-07-02 21:10:29.750 ip-10-0-206-88.us-west-2.compute.internal:263 INFO hook.py:376] Monitoring the collections: sm_metrics, losses, metrics\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Restoring parameters from s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/model/model.ckpt-202\nINFO:tensorflow:Restoring parameters from s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/model/model.ckpt-202\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Inference Time : 0.95984s\nINFO:tensorflow:Inference Time : 0.95984s\nINFO:tensorflow:Finished evaluation at 2020-07-02-21:10:30\nINFO:tensorflow:Finished evaluation at 2020-07-02-21:10:30\nINFO:tensorflow:Saving dict for global step 202: accuracy = 0.22, global_step = 202, loss = 2.2832696\nINFO:tensorflow:Saving dict for global step 202: accuracy = 0.22, global_step = 202, loss = 2.2832696\nINFO:tensorflow:Saving \'checkpoint_path\' summary for global step 202: s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/model/model.ckpt-202\nINFO:tensorflow:Saving \'checkpoint_path\' summary for global step 202: s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/model/model.ckpt-202\nDEBUG:tensorflow:Calling exporter with the `is_the_final_export=True`.\nDEBUG:tensorflow:Calling exporter with the `is_the_final_export=True`.\nINFO:tensorflow:Loss for final step: 2.2609262.\nINFO:tensorflow:Loss for final step: 2.2609262.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\nINFO:tensorflow:Signatures INCLUDED in export for Classify: None\nINFO:tensorflow:Signatures INCLUDED in export for Classify: None\nINFO:tensorflow:Signatures INCLUDED in export for Regress: None\nINFO:tensorflow:Signatures INCLUDED in export for Regress: None\nINFO:tensorflow:Signatures INCLUDED in export for Predict: [\'serving_default\']\nINFO:tensorflow:Signatures INCLUDED in export for Predict: [\'serving_default\']\nINFO:tensorflow:Signatures INCLUDED in export for Train: None\nINFO:tensorflow:Signatures INCLUDED in export for Train: None\nINFO:tensorflow:Signatures INCLUDED in export for Eval: None\nINFO:tensorflow:Signatures INCLUDED in export for Eval: None\nINFO:tensorflow:Restoring parameters from s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/model/model.ckpt-202\nINFO:tensorflow:Restoring parameters from s3://sagemaker-us-west-2-754106851545/test-tf-sm-distributed-mnist-1593723889-2166/model/model.ckpt-202\nINFO:tensorflow:Assets added to graph.\nINFO:tensorflow:Assets added to graph.\nINFO:tensorflow:No assets to write.\nINFO:tensorflow:No assets to write.\nINFO:tensorflow:SavedModel written to: /opt/ml/model/temp-1593724231/saved_model.pb\nINFO:tensorflow:SavedModel written to: /opt/ml/model/temp-1593724231/saved_model.pb\n[2020-07-02 21:10:32.106 ip-10-0-206-88.us-west-2.compute.internal:263 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\n2020-07-02 21:10:33,274 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n\n2020-07-02 21:11:50 Uploading - Uploading generated training model2020-07-02 21:11:47,815 sagemaker_tensorflow_container.training INFO     master algo-1 is down, stopping parameter server\n2020-07-02 21:11:47,816 sagemaker_tensorflow_container.training WARNING  No model artifact is saved under path /opt/ml/model. Your training job will not save any model files to S3.\nFor details of how to construct your training script see:\nhttps://sagemaker.readthedocs.io/en/stable/using_tf.html#adapting-your-local-tensorflow-script\n2020-07-02 21:11:47,816 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n\n2020-07-02 21:11:59 Completed - Training job completed\nTraining seconds: 506\nBillable seconds: 506\n</system-out><system-err>WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\nWARNING:sagemaker:\'s3_input\' class will be renamed to \'TrainingInput\' in SageMaker Python SDK v2.\nINFO:sagemaker:Creating training-job with name: test-tf-sm-distributed-mnist-1593723889-2166\n</system-err></testcase><testcase classname="integration.sagemaker.test_mnist" file="integration/sagemaker/test_mnist.py" line="86" name="test_s3_plugin[cpu-3]" time="0.000"><skipped message="Skipping test in PR context to speed up iteration time. Test will be run in nightly/release pipeline." type="pytest.skip">integration/sagemaker/test_mnist.py:87: Skipping test in PR context to speed up iteration time. Test will be run in nightly/release pipeline.</skipped></testcase><testcase classname="integration.sagemaker.test_mnist" file="integration/sagemaker/test_mnist.py" line="118" name="test_tuning[cpu-3]" time="0.000"><skipped message="Skipping test in PR context to speed up iteration time. Test will be run in nightly/release pipeline." type="pytest.skip">integration/sagemaker/test_mnist.py:119: Skipping test in PR context to speed up iteration time. Test will be run in nightly/release pipeline.</skipped></testcase><testcase classname="integration.sagemaker.test_mnist" file="integration/sagemaker/test_mnist.py" line="153" name="test_smdebug[cpu-3]" time="0.000"><skipped message="skip the test temporarily due to timeout issue" type="pytest.skip">integration/sagemaker/test_mnist.py:154: skip the test temporarily due to timeout issue</skipped></testcase><testcase classname="integration.sagemaker.test_pipemode" file="integration/sagemaker/test_pipemode.py" line="97" name="test_single_record[cpu-3]" time="375.788"><system-out>2020-07-02 21:12:38 Starting - Starting the training job...\n2020-07-02 21:12:43 Starting - Launching requested ML instances.........\n2020-07-02 21:14:18 Starting - Preparing the instances for training......\n2020-07-02 21:15:35 Downloading - Downloading input data\n2020-07-02 21:15:35 Training - Downloading the training image...............\n2020-07-02 21:18:07 Uploading - Uploading generated training model2020-07-02 21:17:55,751 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\n2020-07-02 21:17:56,291 sagemaker-training-toolkit INFO     Invoking user script\n\nTraining Env:\n\n{\n    "additional_framework_parameters": {},\n    "channel_input_dirs": {\n        "elizabeth": "/opt/ml/input/data/elizabeth"\n    },\n    "current_host": "algo-1",\n    "framework_module": "sagemaker_tensorflow_container.training:main",\n    "hosts": [\n        "algo-1"\n    ],\n    "hyperparameters": {\n        "model_dir": "s3://sagemaker-us-west-2-754106851545/test-sagemaker-pipemode-1593724357-cfcc/model",\n        "dimension": 5\n    },\n    "input_config_dir": "/opt/ml/input/config",\n    "input_data_config": {\n        "elizabeth": {\n            "TrainingInputMode": "Pipe",\n            "S3DistributionType": "FullyReplicated",\n            "RecordWrapperType": "RecordIO"\n        }\n    },\n    "input_dir": "/opt/ml/input",\n    "is_master": true,\n    "job_name": "test-sagemaker-pipemode-1593724357-cfcc",\n    "log_level": 20,\n    "master_hostname": "algo-1",\n    "model_dir": "/opt/ml/model",\n    "module_dir": "s3://sagemaker-us-west-2-754106851545/test-sagemaker-pipemode-1593724357-cfcc/source/sourcedir.tar.gz",\n    "module_name": "pipemode",\n    "network_interface_name": "eth0",\n    "num_cpus": 32,\n    "num_gpus": 4,\n    "output_data_dir": "/opt/ml/output/data",\n    "output_dir": "/opt/ml/output",\n    "output_intermediate_dir": "/opt/ml/output/intermediate",\n    "resource_config": {\n        "current_host": "algo-1",\n        "hosts": [\n            "algo-1"\n        ],\n        "network_interface_name": "eth0"\n    },\n    "user_entry_point": "pipemode.py"\n}\n\nEnvironment variables:\n\nSM_HOSTS=["algo-1"]\nSM_NETWORK_INTERFACE_NAME=eth0\nSM_HPS={"dimension":5,"model_dir":"s3://sagemaker-us-west-2-754106851545/test-sagemaker-pipemode-1593724357-cfcc/model"}\nSM_USER_ENTRY_POINT=pipemode.py\nSM_FRAMEWORK_PARAMS={}\nSM_RESOURCE_CONFIG={"current_host":"algo-1","hosts":["algo-1"],"network_interface_name":"eth0"}\nSM_INPUT_DATA_CONFIG={"elizabeth":{"RecordWrapperType":"RecordIO","S3DistributionType":"FullyReplicated","TrainingInputMode":"Pipe"}}\nSM_OUTPUT_DATA_DIR=/opt/ml/output/data\nSM_CHANNELS=["elizabeth"]\nSM_CURRENT_HOST=algo-1\nSM_MODULE_NAME=pipemode\nSM_LOG_LEVEL=20\nSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\nSM_INPUT_DIR=/opt/ml/input\nSM_INPUT_CONFIG_DIR=/opt/ml/input/config\nSM_OUTPUT_DIR=/opt/ml/output\nSM_NUM_CPUS=32\nSM_NUM_GPUS=4\nSM_MODEL_DIR=/opt/ml/model\nSM_MODULE_DIR=s3://sagemaker-us-west-2-754106851545/test-sagemaker-pipemode-1593724357-cfcc/source/sourcedir.tar.gz\nSM_TRAINING_ENV={"additional_framework_parameters":{},"channel_input_dirs":{"elizabeth":"/opt/ml/input/data/elizabeth"},"current_host":"algo-1","framework_module":"sagemaker_tensorflow_container.training:main","hosts":["algo-1"],"hyperparameters":{"dimension":5,"model_dir":"s3://sagemaker-us-west-2-754106851545/test-sagemaker-pipemode-1593724357-cfcc/model"},"input_config_dir":"/opt/ml/input/config","input_data_config":{"elizabeth":{"RecordWrapperType":"RecordIO","S3DistributionType":"FullyReplicated","TrainingInputMode":"Pipe"}},"input_dir":"/opt/ml/input","is_master":true,"job_name":"test-sagemaker-pipemode-1593724357-cfcc","log_level":20,"master_hostname":"algo-1","model_dir":"/opt/ml/model","module_dir":"s3://sagemaker-us-west-2-754106851545/test-sagemaker-pipemode-1593724357-cfcc/source/sourcedir.tar.gz","module_name":"pipemode","network_interface_name":"eth0","num_cpus":32,"num_gpus":4,"output_data_dir":"/opt/ml/output/data","output_dir":"/opt/ml/output","output_intermediate_dir":"/opt/ml/output/intermediate","resource_config":{"current_host":"algo-1","hosts":["algo-1"],"network_interface_name":"eth0"},"user_entry_point":"pipemode.py"}\nSM_USER_ARGS=["--dimension","5","--model_dir","s3://sagemaker-us-west-2-754106851545/test-sagemaker-pipemode-1593724357-cfcc/model"]\nSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\nSM_CHANNEL_ELIZABETH=/opt/ml/input/data/elizabeth\nSM_HP_MODEL_DIR=s3://sagemaker-us-west-2-754106851545/test-sagemaker-pipemode-1593724357-cfcc/model\nSM_HP_DIMENSION=5\nPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\n\nInvoking script with the following command:\n\n/usr/local/bin/python3.7 pipemode.py --dimension 5 --model_dir s3://sagemaker-us-west-2-754106851545/test-sagemaker-pipemode-1593724357-cfcc/model\n\n\nStarting estimator script\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmprispps38\nAbout to call train\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\nWARNING:tensorflow:Layer linear/linear_model is casting an input tensor from dtype float64 to the layer\'s dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it\'s dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx(\'float64\')`. To change just this layer, pass dtype=\'float64\' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:540: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `layer.add_weight` method instead.\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/ftrl.py:144: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/ftrl.py:144: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Create CheckpointSaverHook.\n[2020-07-02 21:18:00.247 ip-10-0-114-206.us-west-2.compute.internal:49 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n[2020-07-02 21:18:00.247 ip-10-0-114-206.us-west-2.compute.internal:49 INFO hook.py:191] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n[2020-07-02 21:18:00.247 ip-10-0-114-206.us-west-2.compute.internal:49 INFO hook.py:236] Saving to /opt/ml/output/tensors\n[2020-07-02 21:18:00.247 ip-10-0-114-206.us-west-2.compute.internal:49 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n[2020-07-02 21:18:00.303 ip-10-0-114-206.us-west-2.compute.internal:49 INFO hook.py:376] Monitoring the collections: sm_metrics, losses, metrics\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\nINFO:tensorflow:Saving checkpoints for 0 into /tmp/tmprispps38/model.ckpt.\nINFO:tensorflow:Saving checkpoints for 0 into /tmp/tmprispps38/model.ckpt.\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n2020-07-02 21:18:01.447712: W tensorflow/core/framework/dataset.cc:445] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/session.py:304: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.compat.v1.graph_util.extract_sub_graph`\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/session.py:304: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.compat.v1.graph_util.extract_sub_graph`\nINFO:tensorflow:loss = 0.6931472, step = 0\nINFO:tensorflow:loss = 0.6931472, step = 0\nERROR:root:\'NoneType\' object has no attribute \'write\'\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 60...\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 60...\nINFO:tensorflow:Saving checkpoints for 60 into /tmp/tmprispps38/model.ckpt.\nINFO:tensorflow:Saving checkpoints for 60 into /tmp/tmprispps38/model.ckpt.\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 60...\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 60...\nINFO:tensorflow:Loss for final step: 0.2542204.\nINFO:tensorflow:Loss for final step: 0.2542204.\nAbout to call evaluate\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Calling model_fn.\nWARNING:tensorflow:Layer linear/linear_model is casting an input tensor from dtype float64 to the layer\'s dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it\'s dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx(\'float64\')`. To change just this layer, pass dtype=\'float64\' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\nWARNING:tensorflow:Layer linear/linear_model is casting an input tensor from dtype float64 to the layer\'s dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it\'s dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx(\'float64\')`. To change just this layer, pass dtype=\'float64\' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Starting evaluation at 2020-07-02T21:18:04Z\nINFO:tensorflow:Starting evaluation at 2020-07-02T21:18:04Z\n[2020-07-02 21:18:04.691 ip-10-0-114-206.us-west-2.compute.internal:49 INFO hook.py:376] Monitoring the collections: sm_metrics, losses, metrics\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Restoring parameters from /tmp/tmprispps38/model.ckpt-60\nINFO:tensorflow:Restoring parameters from /tmp/tmprispps38/model.ckpt-60\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Inference Time : 0.83708s\nINFO:tensorflow:Inference Time : 0.83708s\nINFO:tensorflow:Finished evaluation at 2020-07-02-21:18:05\nINFO:tensorflow:Finished evaluation at 2020-07-02-21:18:05\nINFO:tensorflow:Saving dict for global step 60: accuracy = 0.83, accuracy_baseline = 0.52, auc = 0.92147434, auc_precision_recall = 0.9101388, average_loss = 0.38564423, global_step = 60, label/mean = 0.52, loss = 0.3856441, precision = 0.8888889, prediction/mean = 0.4644718, recall = 0.7692308\nINFO:tensorflow:Saving dict for global step 60: accuracy = 0.83, accuracy_baseline = 0.52, auc = 0.92147434, auc_precision_recall = 0.9101388, average_loss = 0.38564423, global_step = 60, label/mean = 0.52, loss = 0.3856441, precision = 0.8888889, prediction/mean = 0.4644718, recall = 0.7692308\nINFO:tensorflow:Saving \'checkpoint_path\' summary for global step 60: /tmp/tmprispps38/model.ckpt-60\nINFO:tensorflow:Saving \'checkpoint_path\' summary for global step 60: /tmp/tmprispps38/model.ckpt-60\naccuracy: 0.83\naccuracy_baseline: 0.52\nauc: 0.92147434\nauc_precision_recall: 0.9101388\naverage_loss: 0.38564423\nglobal_step: 60\nlabel/mean: 0.52\nloss: 0.3856441\nprecision: 0.8888889\nprediction/mean: 0.4644718\nrecall: 0.7692308\nValidate that new PipeModeDataset on existing channel can be created\nPipeModeDatasetOp::Dataset::Iterator total read_time_ms: 0\nPipeModeDatasetOp::Dataset::Iterator total read_bytes: 73\nPipeModeDatasetOp::Dataset::Iterator total read_GB/s: inf\nValidate create, read, discard, recreate\nPipeModeDatasetOp::Dataset::Iterator total read_time_ms: 0\nPipeModeDatasetOp::Dataset::Iterator total read_bytes: 73\nPipeModeDatasetOp::Dataset::Iterator total read_GB/s: inf\nValidate recreate\n[2020-07-02 21:18:05.803 ip-10-0-114-206.us-west-2.compute.internal:49 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\n2020-07-02 21:18:06,619 sagemaker_tensorflow_container.training WARNING  No model artifact is saved under path /opt/ml/model. Your training job will not save any model files to S3.\nFor details of how to construct your training script see:\nhttps://sagemaker.readthedocs.io/en/stable/using_tf.html#adapting-your-local-tensorflow-script\n2020-07-02 21:18:06,620 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n\n2020-07-02 21:18:15 Completed - Training job completed\nTraining seconds: 168\nBillable seconds: 168\n</system-out><system-err>WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\nWARNING:sagemaker:\'s3_input\' class will be renamed to \'TrainingInput\' in SageMaker Python SDK v2.\nINFO:sagemaker:Creating training-job with name: test-sagemaker-pipemode-1593724357-cfcc\n</system-err></testcase><testcase classname="integration.sagemaker.test_pipemode" file="integration/sagemaker/test_pipemode.py" line="107" name="test_multi_records[cpu-3]" time="343.099"><system-out>2020-07-02 21:18:51 Starting - Starting the training job...\n2020-07-02 21:18:53 Starting - Launching requested ML instances......\n2020-07-02 21:20:03 Starting - Preparing the instances for training......\n2020-07-02 21:21:18 Downloading - Downloading input data\n2020-07-02 21:21:18 Training - Downloading the training image...............\n2020-07-02 21:23:37 Training - Training image download completed. Training in progress.2020-07-02 21:23:41,324 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\n2020-07-02 21:23:41,799 sagemaker-training-toolkit INFO     Invoking user script\n\nTraining Env:\n\n{\n    "additional_framework_parameters": {},\n    "channel_input_dirs": {\n        "elizabeth": "/opt/ml/input/data/elizabeth"\n    },\n    "current_host": "algo-1",\n    "framework_module": "sagemaker_tensorflow_container.training:main",\n    "hosts": [\n        "algo-1"\n    ],\n    "hyperparameters": {\n        "model_dir": "s3://sagemaker-us-west-2-754106851545/test-sagemaker-pipemode-1593724730-818e/model",\n        "dimension": 5\n    },\n    "input_config_dir": "/opt/ml/input/config",\n    "input_data_config": {\n        "elizabeth": {\n            "TrainingInputMode": "Pipe",\n            "S3DistributionType": "FullyReplicated",\n            "RecordWrapperType": "None"\n        }\n    },\n    "input_dir": "/opt/ml/input",\n    "is_master": true,\n    "job_name": "test-sagemaker-pipemode-1593724730-818e",\n    "log_level": 20,\n    "master_hostname": "algo-1",\n    "model_dir": "/opt/ml/model",\n    "module_dir": "s3://sagemaker-us-west-2-754106851545/test-sagemaker-pipemode-1593724730-818e/source/sourcedir.tar.gz",\n    "module_name": "pipemode",\n    "network_interface_name": "eth0",\n    "num_cpus": 32,\n    "num_gpus": 4,\n    "output_data_dir": "/opt/ml/output/data",\n    "output_dir": "/opt/ml/output",\n    "output_intermediate_dir": "/opt/ml/output/intermediate",\n    "resource_config": {\n        "current_host": "algo-1",\n        "hosts": [\n            "algo-1"\n        ],\n        "network_interface_name": "eth0"\n    },\n    "user_entry_point": "pipemode.py"\n}\n\nEnvironment variables:\n\nSM_HOSTS=["algo-1"]\nSM_NETWORK_INTERFACE_NAME=eth0\nSM_HPS={"dimension":5,"model_dir":"s3://sagemaker-us-west-2-754106851545/test-sagemaker-pipemode-1593724730-818e/model"}\nSM_USER_ENTRY_POINT=pipemode.py\nSM_FRAMEWORK_PARAMS={}\nSM_RESOURCE_CONFIG={"current_host":"algo-1","hosts":["algo-1"],"network_interface_name":"eth0"}\nSM_INPUT_DATA_CONFIG={"elizabeth":{"RecordWrapperType":"None","S3DistributionType":"FullyReplicated","TrainingInputMode":"Pipe"}}\nSM_OUTPUT_DATA_DIR=/opt/ml/output/data\nSM_CHANNELS=["elizabeth"]\nSM_CURRENT_HOST=algo-1\nSM_MODULE_NAME=pipemode\nSM_LOG_LEVEL=20\nSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\nSM_INPUT_DIR=/opt/ml/input\nSM_INPUT_CONFIG_DIR=/opt/ml/input/config\nSM_OUTPUT_DIR=/opt/ml/output\nSM_NUM_CPUS=32\nSM_NUM_GPUS=4\nSM_MODEL_DIR=/opt/ml/model\nSM_MODULE_DIR=s3://sagemaker-us-west-2-754106851545/test-sagemaker-pipemode-1593724730-818e/source/sourcedir.tar.gz\nSM_TRAINING_ENV={"additional_framework_parameters":{},"channel_input_dirs":{"elizabeth":"/opt/ml/input/data/elizabeth"},"current_host":"algo-1","framework_module":"sagemaker_tensorflow_container.training:main","hosts":["algo-1"],"hyperparameters":{"dimension":5,"model_dir":"s3://sagemaker-us-west-2-754106851545/test-sagemaker-pipemode-1593724730-818e/model"},"input_config_dir":"/opt/ml/input/config","input_data_config":{"elizabeth":{"RecordWrapperType":"None","S3DistributionType":"FullyReplicated","TrainingInputMode":"Pipe"}},"input_dir":"/opt/ml/input","is_master":true,"job_name":"test-sagemaker-pipemode-1593724730-818e","log_level":20,"master_hostname":"algo-1","model_dir":"/opt/ml/model","module_dir":"s3://sagemaker-us-west-2-754106851545/test-sagemaker-pipemode-1593724730-818e/source/sourcedir.tar.gz","module_name":"pipemode","network_interface_name":"eth0","num_cpus":32,"num_gpus":4,"output_data_dir":"/opt/ml/output/data","output_dir":"/opt/ml/output","output_intermediate_dir":"/opt/ml/output/intermediate","resource_config":{"current_host":"algo-1","hosts":["algo-1"],"network_interface_name":"eth0"},"user_entry_point":"pipemode.py"}\nSM_USER_ARGS=["--dimension","5","--model_dir","s3://sagemaker-us-west-2-754106851545/test-sagemaker-pipemode-1593724730-818e/model"]\nSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\nSM_CHANNEL_ELIZABETH=/opt/ml/input/data/elizabeth\nSM_HP_MODEL_DIR=s3://sagemaker-us-west-2-754106851545/test-sagemaker-pipemode-1593724730-818e/model\nSM_HP_DIMENSION=5\nPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\n\nInvoking script with the following command:\n\n/usr/local/bin/python3.7 pipemode.py --dimension 5 --model_dir s3://sagemaker-us-west-2-754106851545/test-sagemaker-pipemode-1593724730-818e/model\n\n\nStarting estimator script\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp4ayz_mqd\nAbout to call train\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\nWARNING:tensorflow:Layer linear/linear_model is casting an input tensor from dtype float64 to the layer\'s dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it\'s dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx(\'float64\')`. To change just this layer, pass dtype=\'float64\' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/feature_column/feature_column_v2.py:540: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `layer.add_weight` method instead.\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/ftrl.py:144: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/ftrl.py:144: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Create CheckpointSaverHook.\n[2020-07-02 21:23:45.762 ip-10-0-88-9.us-west-2.compute.internal:49 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n[2020-07-02 21:23:45.762 ip-10-0-88-9.us-west-2.compute.internal:49 INFO hook.py:191] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n[2020-07-02 21:23:45.763 ip-10-0-88-9.us-west-2.compute.internal:49 INFO hook.py:236] Saving to /opt/ml/output/tensors\n[2020-07-02 21:23:45.763 ip-10-0-88-9.us-west-2.compute.internal:49 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n[2020-07-02 21:23:45.817 ip-10-0-88-9.us-west-2.compute.internal:49 INFO hook.py:376] Monitoring the collections: sm_metrics, losses, metrics\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\nINFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp4ayz_mqd/model.ckpt.\nINFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp4ayz_mqd/model.ckpt.\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n2020-07-02 21:23:46.963636: W tensorflow/core/framework/dataset.cc:445] Input of PipeModeDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/session.py:304: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.compat.v1.graph_util.extract_sub_graph`\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/smdebug/tensorflow/session.py:304: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.compat.v1.graph_util.extract_sub_graph`\nINFO:tensorflow:loss = 0.6931472, step = 0\nINFO:tensorflow:loss = 0.6931472, step = 0\nERROR:root:\'NoneType\' object has no attribute \'write\'\nINFO:tensorflow:global_step/sec: 462.302\nINFO:tensorflow:global_step/sec: 462.302\nINFO:tensorflow:loss = 0.3324805, step = 100 (0.217 sec)\nINFO:tensorflow:loss = 0.3324805, step = 100 (0.217 sec)\nINFO:tensorflow:global_step/sec: 662.216\nINFO:tensorflow:global_step/sec: 662.216\nINFO:tensorflow:loss = 0.50898033, step = 200 (0.151 sec)\nINFO:tensorflow:loss = 0.50898033, step = 200 (0.151 sec)\nINFO:tensorflow:global_step/sec: 659.125\nINFO:tensorflow:global_step/sec: 659.125\nINFO:tensorflow:loss = 0.34530896, step = 300 (0.152 sec)\nINFO:tensorflow:loss = 0.34530896, step = 300 (0.152 sec)\nINFO:tensorflow:global_step/sec: 669.871\nINFO:tensorflow:global_step/sec: 669.871\nINFO:tensorflow:loss = 0.5266059, step = 400 (0.149 sec)\nINFO:tensorflow:loss = 0.5266059, step = 400 (0.149 sec)\nINFO:tensorflow:global_step/sec: 659.527\nINFO:tensorflow:global_step/sec: 659.527\nINFO:tensorflow:loss = 0.35104048, step = 500 (0.151 sec)\nINFO:tensorflow:loss = 0.35104048, step = 500 (0.151 sec)\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 600...\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 600...\nINFO:tensorflow:Saving checkpoints for 600 into /tmp/tmp4ayz_mqd/model.ckpt.\nINFO:tensorflow:Saving checkpoints for 600 into /tmp/tmp4ayz_mqd/model.ckpt.\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 600...\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 600...\nINFO:tensorflow:Loss for final step: 0.20178577.\nINFO:tensorflow:Loss for final step: 0.20178577.\nAbout to call evaluate\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Calling model_fn.\nWARNING:tensorflow:Layer linear/linear_model is casting an input tensor from dtype float64 to the layer\'s dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it\'s dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx(\'float64\')`. To change just this layer, pass dtype=\'float64\' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\nWARNING:tensorflow:Layer linear/linear_model is casting an input tensor from dtype float64 to the layer\'s dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it\'s dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx(\'float64\')`. To change just this layer, pass dtype=\'float64\' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Starting evaluation at 2020-07-02T21:23:51Z\nINFO:tensorflow:Starting evaluation at 2020-07-02T21:23:51Z\n[2020-07-02 21:23:51.064 ip-10-0-88-9.us-west-2.compute.internal:49 INFO hook.py:376] Monitoring the collections: sm_metrics, losses, metrics\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Restoring parameters from /tmp/tmp4ayz_mqd/model.ckpt-600\nINFO:tensorflow:Restoring parameters from /tmp/tmp4ayz_mqd/model.ckpt-600\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Inference Time : 2.45047s\nINFO:tensorflow:Inference Time : 2.45047s\nINFO:tensorflow:Finished evaluation at 2020-07-02-21:23:53\nINFO:tensorflow:Finished evaluation at 2020-07-02-21:23:53\nINFO:tensorflow:Saving dict for global step 600: accuracy = 0.874, accuracy_baseline = 0.5, auc = 0.948958, auc_precision_recall = 0.94833684, average_loss = 0.2892462, global_step = 600, label/mean = 0.5, loss = 0.28924605, precision = 0.8847737, prediction/mean = 0.48811188, recall = 0.86\nINFO:tensorflow:Saving dict for global step 600: accuracy = 0.874, accuracy_baseline = 0.5, auc = 0.948958, auc_precision_recall = 0.94833684, average_loss = 0.2892462, global_step = 600, label/mean = 0.5, loss = 0.28924605, precision = 0.8847737, prediction/mean = 0.48811188, recall = 0.86\nINFO:tensorflow:Saving \'checkpoint_path\' summary for global step 600: /tmp/tmp4ayz_mqd/model.ckpt-600\nINFO:tensorflow:Saving \'checkpoint_path\' summary for global step 600: /tmp/tmp4ayz_mqd/model.ckpt-600\naccuracy: 0.874\naccuracy_baseline: 0.5\nauc: 0.948958\nauc_precision_recall: 0.94833684\naverage_loss: 0.2892462\nglobal_step: 600\nlabel/mean: 0.5\nloss: 0.28924605\nprecision: 0.8847737\nprediction/mean: 0.48811188\nrecall: 0.86\nValidate that new PipeModeDataset on existing channel can be created\nPipeModeDatasetOp::Dataset::Iterator total read_time_ms: 0\nPipeModeDatasetOp::Dataset::Iterator total read_bytes: 73\nPipeModeDatasetOp::Dataset::Iterator total read_GB/s: inf\nValidate create, read, discard, recreate\nPipeModeDatasetOp::Dataset::Iterator total read_time_ms: 0\nPipeModeDatasetOp::Dataset::Iterator total read_bytes: 73\nPipeModeDatasetOp::Dataset::Iterator total read_GB/s: inf\nValidate recreate\n[2020-07-02 21:23:53.792 ip-10-0-88-9.us-west-2.compute.internal:49 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\n2020-07-02 21:23:54,596 sagemaker_tensorflow_container.training WARNING  No model artifact is saved under path /opt/ml/model. Your training job will not save any model files to S3.\nFor details of how to construct your training script see:\nhttps://sagemaker.readthedocs.io/en/stable/using_tf.html#adapting-your-local-tensorflow-script\n2020-07-02 21:23:54,597 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n\n2020-07-02 21:24:05 Uploading - Uploading generated training model\n2020-07-02 21:24:05 Completed - Training job completed\nTraining seconds: 174\nBillable seconds: 174\n</system-out><system-err>WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\nWARNING:sagemaker:\'s3_input\' class will be renamed to \'TrainingInput\' in SageMaker Python SDK v2.\nINFO:sagemaker:Creating training-job with name: test-sagemaker-pipemode-1593724730-818e\n</system-err></testcase><testcase classname="integration.sagemaker.test_tuning_model_dir" file="integration/sagemaker/test_tuning_model_dir.py" line="22" name="test_model_dir_with_training_job_name[cpu-3]" time="358.881"><system-out>.......................................................................!\n</system-out><system-err>WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\nINFO:root:_TuningJob.start_new!!!\nINFO:sagemaker:Creating hyperparameter tuning job with name: test-tf-model-di-1593725073-a26d\n</system-err></testcase></testsuite></testsuites>'
SAMPLE_CB_ARN = "arn:aws:codebuild:us-west-2:754106851545:build/DLCTestJobExecutor:894c9690-f6dc-4a15-b4b8-b9f2ddc51ea9"


def test_requester():
    """
    Tests the send_request and receive_logs functions of the Job Requester package.
    How tests are executed:
    - create one Job Requester object, and multiple threads. Perform send_request with the Job Requester object in
      each of these threads.
    - send messages to the SQS queue that the Job Requester object created, to imitate the response logs received back
      from the Job Executor.
    - In each of the threads, perform receive_logs to receive the log correspond to the send_request earlier.
    """
    threads = 10
    request_object = JobRequester()
    identifiers_list = []
    input_list = []

    # creating unique image names and build_context strings
    for _ in range(threads):
        input_list.append((TEST_IMAGE, "PR", 3))

    # sending requests
    with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:
        futures = [executor.submit(request_object.send_request, x, y, z) for (x, y, z) in input_list]

    print("Created tickets......")
    for future in futures:
        res = future.result()
        print(res)
        identifiers_list.append(res)
    print("\n")

    # create sample xml report files
    image_tag = TEST_IMAGE.split(":")[-1]
    report_path = os.path.join(os.getcwd(), f"{image_tag}.xml")
    with open(report_path, "w") as report:
        report.write(XML_REPORT_MESSAGE)

    os.environ["CODEBUILD_BUILD_ARN"] = SAMPLE_CB_ARN
    for identifier in identifiers_list:
        os.environ["TICKET_KEY"] = f"folder/{identifier.ticket_name}"
        log_return.update_pool("completed", identifier.instance_type, 3, identifier.job_type, report_path)

    # receiving logs
    with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:
        logs = [executor.submit(request_object.receive_logs, identifier) for identifier in identifiers_list]

    LOGGER.info("Receiving logs...")
    for log in logs:
        assert "XML_REPORT" in log.result(), f"XML Report not found as part of the returned log message."

    # clean up test artifacts
    S3 = boto3.client("s3")
    ticket_names = [item.ticket_name for item in identifiers_list]
    for name in ticket_names:
        S3.delete_object(Bucket=request_object.s3_ticket_bucket, Key=name)

    LOGGER.info("Tests passed.")


if __name__ == "__main__":
    test_requester()
