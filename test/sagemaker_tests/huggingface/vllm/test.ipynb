{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a944ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "iam = boto3.client('iam')\n",
    "role = iam.get_role(RoleName='sagemaker-dlc-demo')['Role']['Arn']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86a84f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_image = \"754289655784.dkr.ecr.us-east-1.amazonaws.com/test-tgi:vllm-0.12.0-gpu-py312-cu129-ubuntu22.04-sagemaker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c961547",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "health_check_timeout = 900\n",
    "\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'SM_VLLM_MODEL': \"Qwen/Qwen3-4B-Thinking-2507\", # model_id from hf.co/models\n",
    "  'SM_VLLM_MAX_MODEL_LEN': \"2048\",  # Max length of input text\n",
    "  'SM_VLLM_HOST': \"0.0.0.0\",  # Required for SageMaker networking\n",
    "  #'HUGGING_FACE_HUB_TOKEN': \"<REPLACE WITH YOUR TOKEN>\"\n",
    "}\n",
    "\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  env=config,\n",
    "  name=\"qwen3-4b-thinking-2507-demo-endpoint\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "002412ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: qwen3-4b-thinking-2507-demo-endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!"
     ]
    }
   ],
   "source": [
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout,\n",
    "  inference_ami_version=\"al2-ami-sagemaker-inference-gpu-3-1\", \n",
    "  wait=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfff1dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt to generate\n",
    "messages=[\n",
    "    { \"role\": \"user\", \"content\": \"Give me a short introduction to large language model\" }\n",
    "  ]\n",
    "\n",
    "# Generation arguments\n",
    "parameters = {\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_tokens\": 128,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6041a998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, the user asked for a short introduction to large language models. Let me start by recalling what I know. LLMs are a big topic in AI, so I need to keep it concise but informative. \n",
      "\n",
      "First, I should define what an LLM is. They're AI models trained on massive amounts of text data. The key points are their size—billions or trillions of parameters—and their ability to generate human-like text. \n",
      "\n",
      "Wait, the user might not know terms like \"parameters.\" I should explain it simply. Maybe say they're huge neural networks that learn patterns from text. Also, mention common tasks:\n"
     ]
    }
   ],
   "source": [
    "chat = llm.predict({\"messages\" :messages, **parameters})\n",
    "\n",
    "print(chat[\"choices\"][0][\"message\"][\"content\"].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "293363c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mllm\u001b[49m.delete_model()\n\u001b[32m      2\u001b[39m llm.delete_endpoint()\n",
      "\u001b[31mNameError\u001b[39m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8144ef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e7302d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
