From 92a8cc55a315457133941a34dfb727da75e04e00 Mon Sep 17 00:00:00 2001
From: Your Name <you@example.com>
Date: Wed, 29 Mar 2023 21:20:04 +0000
Subject: [PATCH] test: adapt BERT_pytorch training with gloo and mpi backend

---
 requirements.txt                              |  1 +
 run.py                                        | 95 +++++++++++--------
 .../models/BERT_pytorch/__init__.py           | 12 ++-
 .../bert_pytorch/trainer/pretrain.py          | 11 ++-
 4 files changed, 76 insertions(+), 43 deletions(-)

diff --git a/requirements.txt b/requirements.txt
index 367be90c..f5b120a7 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -21,3 +21,4 @@ git+https://github.com/kornia/kornia.git@b7050c3
 scipy # for lazy_bench.py
 submitit
 pynvml
+numba
\ No newline at end of file
diff --git a/run.py b/run.py
index 1549689c..f587ff9d 100644
--- a/run.py
+++ b/run.py
@@ -10,12 +10,14 @@ Wall time provided for sanity but is not a sane benchmark measurement.
 """
 import argparse
 import time
+import os
 import numpy as np
 import torch.profiler as profiler
 
 from torchbenchmark import load_model_by_name, load_canary_model_by_name
 from torchbenchmark.util.experiment.metrics import get_peak_memory
 import torch
+import torch.distributed as dist
 
 WARMUP_ROUNDS = 3
 SUPPORT_DEVICE_LIST = ["cpu", "cuda"]
@@ -58,7 +60,7 @@ def run_one_step_with_cudastreams(func, streamcount):
         print('{:<20} {:>20}'.format("GPU Time:", "%.3f milliseconds" % start_event.elapsed_time(end_event)), sep='')
 
 
-def printResultSummaryTime(result_summary, metrics_needed=[], model=None, flops_model_analyzer=None, cpu_peak_mem=None, mem_device_id=None, gpu_peak_mem=None):
+def printResultSummaryTime(result_summary, metrics_needed=[], model=None, flops_model_analyzer=None, cpu_peak_mem=None, mem_device_id=None, gpu_peak_mem=None, args=None):
     if args.device == "cuda":
         gpu_time = np.median(list(map(lambda x: x[0], result_summary)))
         cpu_walltime = np.median(list(map(lambda x: x[1], result_summary)))
@@ -86,12 +88,19 @@ def printResultSummaryTime(result_summary, metrics_needed=[], model=None, flops_
     if cpu_peak_mem is not None:
         print('{:<20} {:>20}'.format("CPU Peak Memory:", "%.4f GB" % cpu_peak_mem, sep=''))
 
+    proc_rank = torch.multiprocessing.current_process()._identity[0] - 1
+    with open(os.environ["OUT_PATH"] + '_' + str(proc_rank), 'w') as f:
+        f.write(f"[1]cpu_walltime, [2]cpu_peak_mem, [3]gpu_time, [4]gpu_peak_time\n")
+        f.write(f"{cpu_walltime}\n")
+        f.write(f"{cpu_peak_mem}\n")
+        if args.device == "cuda":
+            f.write(f"{gpu_time}\n")
+            f.write(f"{gpu_peak_mem}\n")
 
-def run_one_step(func, nwarmup=WARMUP_ROUNDS, num_iter=10, model=None, export_metrics_file=None, stress=0, metrics_needed=[], metrics_gpu_backend=None):
+def run_one_step(func, nwarmup=WARMUP_ROUNDS, num_iter=10, model=None, export_metrics_file=None, stress=0, metrics_needed=[], metrics_gpu_backend=None, args=None):
     # Warm-up `nwarmup` rounds
     for _i in range(nwarmup):
         func()
-
     result_summary = []
     flops_model_analyzer = None
     if 'flops' in metrics_needed:
@@ -131,10 +140,12 @@ def run_one_step(func, nwarmup=WARMUP_ROUNDS, num_iter=10, model=None, export_me
             # TODO: modify this to add GPU time as well
             result_summary.append([(t1 - t0) / 1_000_000])
         else:
+            print(f"before run one step before rank {dist.get_rank()}")
             t0 = time.time_ns()
             func()
             t1 = time.time_ns()
             result_summary.append([(t1 - t0) / 1_000_000])
+            print(f"after run one step before rank {dist.get_rank()}")
         if stress:
             cur_time = time.time_ns()
             # print out the status every 10s.
@@ -158,7 +169,7 @@ def run_one_step(func, nwarmup=WARMUP_ROUNDS, num_iter=10, model=None, export_me
     if 'cpu_peak_mem' or 'gpu_peak_mem' in metrics_needed:
         cpu_peak_mem, mem_device_id, gpu_peak_mem = get_peak_memory(func, model.device, export_metrics_file=export_metrics_file, metrics_needed=metrics_needed, metrics_gpu_backend=metrics_gpu_backend)
 
-    printResultSummaryTime(result_summary, metrics_needed, model, flops_model_analyzer, cpu_peak_mem, mem_device_id, gpu_peak_mem)
+    printResultSummaryTime(result_summary, metrics_needed, model, flops_model_analyzer, cpu_peak_mem, mem_device_id, gpu_peak_mem, args=args)
 
 
 
@@ -253,38 +264,7 @@ def _validate_profile_options(profile_options: str):
     return profile_options_list
 
 
-if __name__ == "__main__":
-    parser = argparse.ArgumentParser(__doc__)
-    parser.add_argument("model", help="Full or partial name of a model to run.  If partial, picks the first match.")
-    parser.add_argument("-d", "--device", choices=SUPPORT_DEVICE_LIST, default="cpu", help="Which device to use.")
-    parser.add_argument("-m", "--mode", choices=["eager", "jit"], default="eager", help="Which mode to run.")
-    parser.add_argument("-t", "--test", choices=["eval", "train"], default="eval", help="Which test to run.")
-    parser.add_argument("--profile", action="store_true", help="Run the profiler around the function")
-    parser.add_argument("--profile-options", type=_validate_profile_options, help=f"Select which profile options to enable. Valid options: {SUPPORT_PROFILE_LIST}.")
-    parser.add_argument("--amp", action="store_true", help="enable torch.autocast()")
-    parser.add_argument("--profile-folder", default="./logs", help="Save profiling model traces to this directory.")
-    parser.add_argument("--profile-detailed", action="store_true",
-                        help=f"Enable all profile options, including {SUPPORT_PROFILE_LIST}. Overrides --profile-options.")
-    parser.add_argument("--profile-devices", type=_validate_devices,
-                        help="Profile comma separated list of activities such as cpu,cuda.")
-    parser.add_argument("--profile-eg", action="store_true", help="Collect execution graph by PARAM")
-    parser.add_argument("--profile-eg-folder", default="./eg_logs",
-                        help="Save execution graph traces to this directory.")
-    parser.add_argument("--cudastreams", action="store_true",
-                        help="Utilization test using increasing number of cuda streams.")
-    parser.add_argument("--bs", type=int, help="Specify batch size to the test.")
-    parser.add_argument("--export-metrics", action="store_true",
-                        help="Export all specified metrics records to a csv file. The default csv file name is [model_name]_all_metrics.csv.")
-    parser.add_argument("--stress", type=float, default=0, help="Specify execution time (seconds) to stress devices.")
-    parser.add_argument("--metrics", type=str,
-                        help="Specify metrics [cpu_peak_mem,gpu_peak_mem,flops]to be collected. The metrics are separated by comma such as cpu_peak_mem,gpu_peak_mem.")
-    parser.add_argument("--metrics-gpu-backend", choices=["dcgm", "default"], default="default", help="""Specify the backend [dcgm, default] to collect metrics. \nIn default mode, the latency(execution time) is collected by time.time_ns() and it is always enabled. Optionally,
-    \n  - you can specify cpu peak memory usage by --metrics cpu_peak_mem, and it is collected by psutil.Process().  \n  - you can specify gpu peak memory usage by --metrics gpu_peak_mem, and it is collected by nvml library.\n  - you can specify flops by --metrics flops, and it is collected by fvcore.\nIn dcgm mode, the latency(execution time) is collected by time.time_ns() and it is always enabled. Optionally,\n  - you can specify cpu peak memory usage by --metrics cpu_peak_mem, and it is collected by psutil.Process().\n  - you can specify cpu and gpu peak memory usage by --metrics cpu_peak_mem,gpu_peak_mem, and they are collected by dcgm library.""")
-    parser.add_argument("--channels-last", action="store_true", help="enable torch.channels_last()")
-    args, extra_args = parser.parse_known_args()
-    if args.cudastreams and not args.device == "cuda":
-        print("cuda device required to use --cudastreams option!")
-        exit(-1)
+def main(rank, args, extra_args):
 
     found = False
     Model = load_model_by_name(args.model)
@@ -295,6 +275,10 @@ if __name__ == "__main__":
             print(f"Unable to find model matching {args.model}.")
             exit(-1)
 
+    os.environ["MASTER_ADDR"] = "127.0.0.1"
+    os.environ["MASTER_PORT"] = "12345"
+    dist.init_process_group(backend=os.environ["PT_BACKEND"].lower(), rank=rank, world_size=int(os.environ["WORLD_SIZE"]))
+    print(f"running on rank {dist.get_rank()}")
     m = Model(device=args.device, test=args.test, jit=(args.mode == "jit"), batch_size=args.bs, extra_args=extra_args)
     if m.dynamo:
         mode = f"dynamo {m.opt_args.torchdynamo}"
@@ -341,6 +325,43 @@ if __name__ == "__main__":
         run_one_step_with_cudastreams(test, 10)
     else:
         run_one_step(test, model=m, export_metrics_file=export_metrics_file,
-                     stress=args.stress, metrics_needed=metrics_needed, metrics_gpu_backend=args.metrics_gpu_backend)
+                     stress=args.stress, metrics_needed=metrics_needed, metrics_gpu_backend=args.metrics_gpu_backend, args=args)
     if hasattr(m, 'correctness'):
         print('{:<20} {:>20}'.format("Correctness: ", str(m.correctness)), sep='')
+
+    dist.destroy_process_group()
+
+if __name__ == "__main__":
+    parser = argparse.ArgumentParser(__doc__)
+    parser.add_argument("model", help="Full or partial name of a model to run.  If partial, picks the first match.")
+    parser.add_argument("-d", "--device", choices=SUPPORT_DEVICE_LIST, default="cpu", help="Which device to use.")
+    parser.add_argument("-m", "--mode", choices=["eager", "jit"], default="eager", help="Which mode to run.")
+    parser.add_argument("-t", "--test", choices=["eval", "train"], default="eval", help="Which test to run.")
+    parser.add_argument("--profile", action="store_true", help="Run the profiler around the function")
+    parser.add_argument("--profile-options", type=_validate_profile_options, help=f"Select which profile options to enable. Valid options: {SUPPORT_PROFILE_LIST}.")
+    parser.add_argument("--amp", action="store_true", help="enable torch.autocast()")
+    parser.add_argument("--profile-folder", default="./logs", help="Save profiling model traces to this directory.")
+    parser.add_argument("--profile-detailed", action="store_true",
+                        help=f"Enable all profile options, including {SUPPORT_PROFILE_LIST}. Overrides --profile-options.")
+    parser.add_argument("--profile-devices", type=_validate_devices,
+                        help="Profile comma separated list of activities such as cpu,cuda.")
+    parser.add_argument("--profile-eg", action="store_true", help="Collect execution graph by PARAM")
+    parser.add_argument("--profile-eg-folder", default="./eg_logs",
+                        help="Save execution graph traces to this directory.")
+    parser.add_argument("--cudastreams", action="store_true",
+                        help="Utilization test using increasing number of cuda streams.")
+    parser.add_argument("--bs", type=int, help="Specify batch size to the test.")
+    parser.add_argument("--export-metrics", action="store_true",
+                        help="Export all specified metrics records to a csv file. The default csv file name is [model_name]_all_metrics.csv.")
+    parser.add_argument("--stress", type=float, default=0, help="Specify execution time (seconds) to stress devices.")
+    parser.add_argument("--metrics", type=str,
+                        help="Specify metrics [cpu_peak_mem,gpu_peak_mem,flops]to be collected. The metrics are separated by comma such as cpu_peak_mem,gpu_peak_mem.")
+    parser.add_argument("--metrics-gpu-backend", choices=["dcgm", "default"], default="default", help="""Specify the backend [dcgm, default] to collect metrics. \nIn default mode, the latency(execution time) is collected by time.time_ns() and it is always enabled. Optionally,
+    \n  - you can specify cpu peak memory usage by --metrics cpu_peak_mem, and it is collected by psutil.Process().  \n  - you can specify gpu peak memory usage by --metrics gpu_peak_mem, and it is collected by nvml library.\n  - you can specify flops by --metrics flops, and it is collected by fvcore.\nIn dcgm mode, the latency(execution time) is collected by time.time_ns() and it is always enabled. Optionally,\n  - you can specify cpu peak memory usage by --metrics cpu_peak_mem, and it is collected by psutil.Process().\n  - you can specify cpu and gpu peak memory usage by --metrics cpu_peak_mem,gpu_peak_mem, and they are collected by dcgm library.""")
+    parser.add_argument("--channels-last", action="store_true", help="enable torch.channels_last()")
+    args, extra_args = parser.parse_known_args()
+    if args.cudastreams and not args.device == "cuda":
+        print("cuda device required to use --cudastreams option!")
+        exit(-1)
+
+    torch.multiprocessing.spawn(main, args=(args, extra_args), nprocs=int(os.environ["WORLD_SIZE"]), join=True)
\ No newline at end of file
diff --git a/torchbenchmark/models/BERT_pytorch/__init__.py b/torchbenchmark/models/BERT_pytorch/__init__.py
index f79893fb..9fab0875 100644
--- a/torchbenchmark/models/BERT_pytorch/__init__.py
+++ b/torchbenchmark/models/BERT_pytorch/__init__.py
@@ -1,6 +1,7 @@
 import argparse
 import random
 import torch
+import os
 
 import numpy as np
 
@@ -11,6 +12,7 @@ from .bert_pytorch.trainer import BERTTrainer
 from .bert_pytorch.dataset import BERTDataset, WordVocab
 from .bert_pytorch.model import BERT
 from torch.utils.data import DataLoader
+import torch.distributed as dist
 import typing
 
 torch.backends.cudnn.deterministic = False
@@ -138,9 +140,10 @@ class Model(BenchmarkModel):
             if args.test_dataset is not None else None
 
         set_random_seed()
-
-        train_data_loader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=args.num_workers)
-        test_data_loader = DataLoader(test_dataset, batch_size=args.batch_size, num_workers=args.num_workers) \
+        from torch.utils.data.distributed import DistributedSampler
+        train_sampler, test_sampler = DistributedSampler(train_dataset), DistributedSampler(test_dataset)
+        train_data_loader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=args.num_workers,sampler=train_sampler)
+        test_data_loader = DataLoader(test_dataset, batch_size=args.batch_size, num_workers=args.num_workers, sampler=test_sampler) \
             if test_dataset is not None else None
 
         bert = BERT(len(vocab), hidden=args.hidden, n_layers=args.layers, attn_heads=args.attn_heads)
@@ -192,4 +195,7 @@ class Model(BenchmarkModel):
         # 3. backward and optimization only in train
         trainer.optim_schedule.zero_grad()
         loss.backward()
+        for param in trainer.model.parameters():
+            dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)
+        param.grad.data /= dist.get_world_size()
         trainer.optim_schedule.step_and_update_lr()
diff --git a/torchbenchmark/models/BERT_pytorch/bert_pytorch/trainer/pretrain.py b/torchbenchmark/models/BERT_pytorch/bert_pytorch/trainer/pretrain.py
index d1a4673c..83a7c056 100644
--- a/torchbenchmark/models/BERT_pytorch/bert_pytorch/trainer/pretrain.py
+++ b/torchbenchmark/models/BERT_pytorch/bert_pytorch/trainer/pretrain.py
@@ -1,11 +1,13 @@
 import torch
 import torch.nn as nn
+
+from torch.nn.parallel import DistributedDataParallel
 from torch.optim import Adam
 from torch.utils.data import DataLoader
 
 from ..model import BERTLM, BERT
 from .optim_schedule import ScheduledOptim
-
+import os
 
 class BERTTrainer:
     """
@@ -45,8 +47,11 @@ class BERTTrainer:
 
         # Distributed GPU training if CUDA can detect more than 1 GPU
         if with_cuda and torch.cuda.device_count() > 1:
-            self.model = nn.DataParallel(self.model, device_ids=cuda_devices)
-
+            self.model = DistributedDataParallel(self.model, device_ids=cuda_devices)
+        else:
+            self.model = DistributedDataParallel(self.model)
+        if torch.cuda.is_available() and os.environ["USE_INDUCTOR"] == '1':
+            self.model = torch.compile(self.model, backend="inductor", mode='default')
         # Setting the train and test data loader
         self.train_data = train_dataloader
         self.test_data = test_dataloader
-- 
2.17.1

