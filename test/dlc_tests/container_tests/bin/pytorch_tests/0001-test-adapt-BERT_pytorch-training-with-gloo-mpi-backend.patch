From f97dbc04afbaccd14bf27f23c3a3b066e44e456d Mon Sep 17 00:00:00 2001
From: Your Name <you@example.com>
Date: Wed, 19 Apr 2023 18:35:36 +0000
Subject: [PATCH] refactor: add DDP with gloo backend to bert script

---
 requirements.txt                              |   1 +
 run.py                                        | 109 +++++++++++-------
 .../models/BERT_pytorch/__init__.py           |  19 ++-
 .../bert_pytorch/trainer/pretrain.py          |  15 ++-
 4 files changed, 96 insertions(+), 48 deletions(-)

diff --git a/requirements.txt b/requirements.txt
index b80143de..46e881b7 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -21,3 +21,4 @@ git+https://github.com/kornia/kornia.git@b7050c3
 scipy # for lazy_bench.py
 submitit
 pynvml
+numba
\ No newline at end of file
diff --git a/run.py b/run.py
index 827cf167..0d0ec675 100644
--- a/run.py
+++ b/run.py
@@ -11,10 +11,12 @@ Wall time provided for sanity but is not a sane benchmark measurement.
 import argparse
 import logging
 import time
-
+import os
 import numpy as np
 import torch
 import torch.profiler as profiler
+import torch.distributed as dist
+import torch.multiprocessing as mp
 
 from torchbenchmark import load_canary_model_by_name, load_model_by_name
 from torchbenchmark.util.experiment.metrics import get_peak_memory
@@ -66,7 +68,7 @@ def run_one_step_with_cudastreams(func, streamcount):
         print('{:<20} {:>20}'.format("GPU Time:", "%.3f milliseconds" % start_event.elapsed_time(end_event)), sep='')
 
 
-def printResultSummaryTime(result_summary, metrics_needed=[], model=None, flops_model_analyzer=None, cpu_peak_mem=None, mem_device_id=None, gpu_peak_mem=None):
+def printResultSummaryTime(result_summary, metrics_needed=[], model=None, flops_model_analyzer=None, cpu_peak_mem=None, mem_device_id=None, gpu_peak_mem=None, args=None):
     if args.device == "cuda":
         gpu_time = np.median(list(map(lambda x: x[0], result_summary)))
         cpu_walltime = np.median(list(map(lambda x: x[1], result_summary)))
@@ -94,12 +96,19 @@ def printResultSummaryTime(result_summary, metrics_needed=[], model=None, flops_
     if cpu_peak_mem is not None:
         print('{:<20} {:>20}'.format("CPU Peak Memory:", "%.4f GB" % cpu_peak_mem, sep=''))
 
+    proc_rank = torch.multiprocessing.current_process()._identity[0] - 1
+    with open(os.environ["OUT_PATH"] + '_' + str(proc_rank), 'w') as f:
+        f.write(f"[1]cpu_walltime, [2]cpu_peak_mem, [3]gpu_time, [4]gpu_peak_time\n")
+        f.write(f"{cpu_walltime}\n")
+        f.write(f"{cpu_peak_mem}\n")
+        if args.device == "cuda":
+            f.write(f"{gpu_time}\n")
+            f.write(f"{gpu_peak_mem}\n")
 
-def run_one_step(func, nwarmup=WARMUP_ROUNDS, num_iter=10, model=None, export_metrics_file=None, stress=0, metrics_needed=[], metrics_gpu_backend=None):
+def run_one_step(func, nwarmup=WARMUP_ROUNDS, num_iter=10, model=None, export_metrics_file=None, stress=0, metrics_needed=[], metrics_gpu_backend=None, args=None):
     # Warm-up `nwarmup` rounds
     for _i in range(nwarmup):
         func()
-
     result_summary = []
     flops_model_analyzer = None
     if 'flops' in metrics_needed:
@@ -117,6 +126,8 @@ def run_one_step(func, nwarmup=WARMUP_ROUNDS, num_iter=10, model=None, export_me
     last_it = 0
     first_print_out = True
     while (not stress and _i < num_iter) or (stress and cur_time < target_time) :
+
+        dist.barrier()
         if args.device == "cuda":
             torch.cuda.synchronize()
             start_event = torch.cuda.Event(enable_timing=True)
@@ -166,7 +177,7 @@ def run_one_step(func, nwarmup=WARMUP_ROUNDS, num_iter=10, model=None, export_me
     if 'cpu_peak_mem' in metrics_needed or 'gpu_peak_mem' in metrics_needed:
         cpu_peak_mem, mem_device_id, gpu_peak_mem = get_peak_memory(func, model.device, export_metrics_file=export_metrics_file, metrics_needed=metrics_needed, metrics_gpu_backend=metrics_gpu_backend)
 
-    printResultSummaryTime(result_summary, metrics_needed, model, flops_model_analyzer, cpu_peak_mem, mem_device_id, gpu_peak_mem)
+    printResultSummaryTime(result_summary, metrics_needed, model, flops_model_analyzer, cpu_peak_mem, mem_device_id, gpu_peak_mem, args=args)
 
 
 
@@ -261,38 +272,7 @@ def _validate_profile_options(profile_options: str):
     return profile_options_list
 
 
-if __name__ == "__main__":
-    parser = argparse.ArgumentParser(__doc__)
-    parser.add_argument("model", help="Full or partial name of a model to run.  If partial, picks the first match.")
-    parser.add_argument("-d", "--device", choices=SUPPORT_DEVICE_LIST, default="cpu", help="Which device to use.")
-    parser.add_argument("-m", "--mode", choices=["eager", "jit"], default="eager", help="Which mode to run.")
-    parser.add_argument("-t", "--test", choices=["eval", "train"], default="eval", help="Which test to run.")
-    parser.add_argument("--profile", action="store_true", help="Run the profiler around the function")
-    parser.add_argument("--profile-options", type=_validate_profile_options, help=f"Select which profile options to enable. Valid options: {SUPPORT_PROFILE_LIST}.")
-    parser.add_argument("--amp", action="store_true", help="enable torch.autocast()")
-    parser.add_argument("--profile-folder", default="./logs", help="Save profiling model traces to this directory.")
-    parser.add_argument("--profile-detailed", action="store_true",
-                        help=f"Enable all profile options, including {SUPPORT_PROFILE_LIST}. Overrides --profile-options.")
-    parser.add_argument("--profile-devices", type=_validate_devices,
-                        help="Profile comma separated list of activities such as cpu,cuda.")
-    parser.add_argument("--profile-eg", action="store_true", help="Collect execution graph by PARAM")
-    parser.add_argument("--profile-eg-folder", default="./eg_logs",
-                        help="Save execution graph traces to this directory.")
-    parser.add_argument("--cudastreams", action="store_true",
-                        help="Utilization test using increasing number of cuda streams.")
-    parser.add_argument("--bs", type=int, help="Specify batch size to the test.")
-    parser.add_argument("--export-metrics", action="store_true",
-                        help="Export all specified metrics records to a csv file. The default csv file name is [model_name]_all_metrics.csv.")
-    parser.add_argument("--stress", type=float, default=0, help="Specify execution time (seconds) to stress devices.")
-    parser.add_argument("--metrics", type=str,
-                        help="Specify metrics [cpu_peak_mem,gpu_peak_mem,flops]to be collected. The metrics are separated by comma such as cpu_peak_mem,gpu_peak_mem.")
-    parser.add_argument("--metrics-gpu-backend", choices=["dcgm", "default"], default="default", help="""Specify the backend [dcgm, default] to collect metrics. \nIn default mode, the latency(execution time) is collected by time.time_ns() and it is always enabled. Optionally,
-    \n  - you can specify cpu peak memory usage by --metrics cpu_peak_mem, and it is collected by psutil.Process().  \n  - you can specify gpu peak memory usage by --metrics gpu_peak_mem, and it is collected by nvml library.\n  - you can specify flops by --metrics flops, and it is collected by fvcore.\nIn dcgm mode, the latency(execution time) is collected by time.time_ns() and it is always enabled. Optionally,\n  - you can specify cpu peak memory usage by --metrics cpu_peak_mem, and it is collected by psutil.Process().\n  - you can specify cpu and gpu peak memory usage by --metrics cpu_peak_mem,gpu_peak_mem, and they are collected by dcgm library.""")
-    parser.add_argument("--channels-last", action="store_true", help="enable torch.channels_last()")
-    args, extra_args = parser.parse_known_args()
-    if args.cudastreams and not args.device == "cuda":
-        print("cuda device required to use --cudastreams option!")
-        exit(-1)
+def main(rank, args, extra_args):
 
     found = False
     Model = load_model_by_name(args.model)
@@ -303,7 +283,15 @@ if __name__ == "__main__":
             print(f"Unable to find model matching {args.model}.")
             exit(-1)
 
-    m = Model(device=args.device, test=args.test, jit=(args.mode == "jit"), batch_size=args.bs, extra_args=extra_args)
+    os.environ["MASTER_ADDR"] = "127.0.0.1"
+    os.environ["MASTER_PORT"] = "12345"
+    dist.init_process_group(backend=os.environ["PT_BACKEND"].lower(), rank=rank, world_size=int(os.environ["WORLD_SIZE"]))
+    print(f"running on rank {dist.get_rank()}")
+    if args.device == "cuda":
+        device = torch.device(f"cuda:{dist.get_rank()}")
+    else:
+        device = args.device
+    m = Model(device=device, test=args.test, jit=(args.mode == "jit"), batch_size=args.bs, extra_args=extra_args)
     if m.dynamo:
         mode = f"dynamo {m.opt_args.torchdynamo}"
     elif m.opt_args.backend:
@@ -349,6 +337,49 @@ if __name__ == "__main__":
         run_one_step_with_cudastreams(test, 10)
     else:
         run_one_step(test, model=m, export_metrics_file=export_metrics_file,
-                     stress=args.stress, metrics_needed=metrics_needed, metrics_gpu_backend=args.metrics_gpu_backend)
+                     stress=args.stress, metrics_needed=metrics_needed, metrics_gpu_backend=args.metrics_gpu_backend, args=args)
     if hasattr(m, 'correctness'):
         print('{:<20} {:>20}'.format("Correctness: ", str(m.correctness)), sep='')
+
+    dist.destroy_process_group()
+
+if __name__ == "__main__":
+    parser = argparse.ArgumentParser(__doc__)
+    parser.add_argument("model", help="Full or partial name of a model to run.  If partial, picks the first match.")
+    parser.add_argument("-d", "--device", choices=SUPPORT_DEVICE_LIST, default="cpu", help="Which device to use.")
+    parser.add_argument("-m", "--mode", choices=["eager", "jit"], default="eager", help="Which mode to run.")
+    parser.add_argument("-t", "--test", choices=["eval", "train"], default="eval", help="Which test to run.")
+    parser.add_argument("--profile", action="store_true", help="Run the profiler around the function")
+    parser.add_argument("--profile-options", type=_validate_profile_options, help=f"Select which profile options to enable. Valid options: {SUPPORT_PROFILE_LIST}.")
+    parser.add_argument("--amp", action="store_true", help="enable torch.autocast()")
+    parser.add_argument("--profile-folder", default="./logs", help="Save profiling model traces to this directory.")
+    parser.add_argument("--profile-detailed", action="store_true",
+                        help=f"Enable all profile options, including {SUPPORT_PROFILE_LIST}. Overrides --profile-options.")
+    parser.add_argument("--profile-devices", type=_validate_devices,
+                        help="Profile comma separated list of activities such as cpu,cuda.")
+    parser.add_argument("--profile-eg", action="store_true", help="Collect execution graph by PARAM")
+    parser.add_argument("--profile-eg-folder", default="./eg_logs",
+                        help="Save execution graph traces to this directory.")
+    parser.add_argument("--cudastreams", action="store_true",
+                        help="Utilization test using increasing number of cuda streams.")
+    parser.add_argument("--bs", type=int, help="Specify batch size to the test.")
+    parser.add_argument("--export-metrics", action="store_true",
+                        help="Export all specified metrics records to a csv file. The default csv file name is [model_name]_all_metrics.csv.")
+    parser.add_argument("--stress", type=float, default=0, help="Specify execution time (seconds) to stress devices.")
+    parser.add_argument("--metrics", type=str,
+                        help="Specify metrics [cpu_peak_mem,gpu_peak_mem,flops]to be collected. The metrics are separated by comma such as cpu_peak_mem,gpu_peak_mem.")
+    parser.add_argument("--metrics-gpu-backend", choices=["dcgm", "default"], default="default", help="""Specify the backend [dcgm, default] to collect metrics. \nIn default mode, the latency(execution time) is collected by time.time_ns() and it is always enabled. Optionally,
+    \n  - you can specify cpu peak memory usage by --metrics cpu_peak_mem, and it is collected by psutil.Process().  \n  - you can specify gpu peak memory usage by --metrics gpu_peak_mem, and it is collected by nvml library.\n  - you can specify flops by --metrics flops, and it is collected by fvcore.\nIn dcgm mode, the latency(execution time) is collected by time.time_ns() and it is always enabled. Optionally,\n  - you can specify cpu peak memory usage by --metrics cpu_peak_mem, and it is collected by psutil.Process().\n  - you can specify cpu and gpu peak memory usage by --metrics cpu_peak_mem,gpu_peak_mem, and they are collected by dcgm library.""")
+    parser.add_argument("--channels-last", action="store_true", help="enable torch.channels_last()")
+    args, extra_args = parser.parse_known_args()
+    if args.cudastreams and not args.device == "cuda":
+        print("cuda device required to use --cudastreams option!")
+        exit(-1)
+
+    mp.set_start_method('spawn', force=True)
+    # torch._dynamo.config.suppress_errors = True
+    procs = [mp.Process(target=main, args=(rank, args, extra_args)) for rank in range(int(os.environ["WORLD_SIZE"]))]
+    for p in procs:
+        p.start()
+    for p in procs:
+        p.join()
\ No newline at end of file
diff --git a/torchbenchmark/models/BERT_pytorch/__init__.py b/torchbenchmark/models/BERT_pytorch/__init__.py
index 030b03d2..3e341b18 100644
--- a/torchbenchmark/models/BERT_pytorch/__init__.py
+++ b/torchbenchmark/models/BERT_pytorch/__init__.py
@@ -1,6 +1,7 @@
 import argparse
 import random
 import torch
+import os
 
 import numpy as np
 
@@ -11,6 +12,7 @@ from .bert_pytorch.trainer import BERTTrainer
 from .bert_pytorch.dataset import BERTDataset, WordVocab
 from .bert_pytorch.model import BERT
 from torch.utils.data import DataLoader
+import torch.distributed as dist
 import typing
 
 torch.backends.cudnn.deterministic = False
@@ -86,7 +88,10 @@ class Model(BenchmarkModel):
             '--vocab_path', f'{root}/data/vocab.small',
             '--output_path', 'bert.model',
         ]) # Avoid reading sys.argv here
-        args.with_cuda = self.device == 'cuda'
+        if self.device == "cpu":
+            args.with_cuda = False
+        else:
+            args.with_cuda = True
         args.script = self.jit
         args.on_memory = True
 
@@ -138,12 +143,13 @@ class Model(BenchmarkModel):
             if args.test_dataset is not None else None
 
         set_random_seed()
-
-        train_data_loader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=args.num_workers)
-        test_data_loader = DataLoader(test_dataset, batch_size=args.batch_size, num_workers=args.num_workers) \
+        from torch.utils.data.distributed import DistributedSampler
+        train_sampler, test_sampler = DistributedSampler(train_dataset), DistributedSampler(test_dataset)
+        train_data_loader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=args.num_workers,sampler=train_sampler)
+        test_data_loader = DataLoader(test_dataset, batch_size=args.batch_size, num_workers=args.num_workers, sampler=test_sampler) \
             if test_dataset is not None else None
 
-        bert = BERT(len(vocab), hidden=args.hidden, n_layers=args.layers, attn_heads=args.attn_heads)
+        bert = BERT(len(vocab), hidden=args.hidden, n_layers=args.layers, attn_heads=args.attn_heads).to(self.device)
 
         trainer = BERTTrainer(bert, len(vocab), train_dataloader=train_data_loader, test_dataloader=test_data_loader,
                                    lr=args.lr, betas=(args.adam_beta1, args.adam_beta2), weight_decay=args.adam_weight_decay,
@@ -192,6 +198,9 @@ class Model(BenchmarkModel):
         # 3. backward and optimization only in train
         trainer.optim_schedule.zero_grad()
         loss.backward()
+        # for param in trainer.model.parameters():
+        #     dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)
+        # param.grad.data /= dist.get_world_size()
         trainer.optim_schedule.step_and_update_lr()
 
     # self.model is a Trainer that has an inner optimizer wrapped by a scheduled optimizer. Return the inner,
diff --git a/torchbenchmark/models/BERT_pytorch/bert_pytorch/trainer/pretrain.py b/torchbenchmark/models/BERT_pytorch/bert_pytorch/trainer/pretrain.py
index 67840ef5..153df03d 100644
--- a/torchbenchmark/models/BERT_pytorch/bert_pytorch/trainer/pretrain.py
+++ b/torchbenchmark/models/BERT_pytorch/bert_pytorch/trainer/pretrain.py
@@ -1,11 +1,14 @@
 import torch
 import torch.nn as nn
+
+from torch.nn.parallel import DistributedDataParallel
 from torch.optim import Adam
 from torch.utils.data import DataLoader
+from torch.distributed import get_rank
 
 from ..model import BERTLM, BERT
 from .optim_schedule import ScheduledOptim
-
+import os
 
 class BERTTrainer:
     """
@@ -36,7 +39,7 @@ class BERTTrainer:
 
         # Setup cuda device for BERT training, argument -c, --cuda should be true
         cuda_condition = torch.cuda.is_available() and with_cuda
-        self.device = torch.device("cuda:0" if cuda_condition else "cpu")
+        self.device = torch.device(f"cuda:{get_rank()}" if cuda_condition else "cpu")
 
         # This BERT model will be saved every epoch
         self.bert = bert
@@ -45,8 +48,12 @@ class BERTTrainer:
 
         # Distributed GPU training if CUDA can detect more than 1 GPU
         if with_cuda and torch.cuda.device_count() > 1:
-            self.model = nn.DataParallel(self.model, device_ids=cuda_devices)
-
+            self.model = DistributedDataParallel(self.model, device_ids=cuda_devices)
+        else:
+            self.model = DistributedDataParallel(self.model)
+        if torch.cuda.is_available() and os.environ["USE_INDUCTOR"] == '1':
+            self.model = torch.compile(self.model, backend="inductor", mode='default')
+            print("model compiled", self.model)
         # Setting the train and test data loader
         self.train_data = train_dataloader
         self.test_data = test_dataloader
-- 
2.25.1

