# AWS Deep Learning Containers for vLLM with EFA Support on SageMaker

[AWS Deep Learning Containers (DLCs)](https://aws.amazon.com/machine-learning/containers/) now support vLLM images optimized for large language model serving on Amazon SageMaker. The vLLM DLC provides a production-ready environment for deploying and serving LLMs with advanced features like PagedAttention for efficient memory management and continuous batching.

## Changelog

To learn about latest changes in vLLM DLC, checkout the [changelog](https://github.com/aws/deep-learning-containers/blob/master/vllm/CHANGELOG.md).

A list of available containers can be found on [GitHub](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#vllm-containers).

vLLM v0.11.1 and above utilizes CUDA 12.9 which is only compatible with Nvidia Driver 535 and above (550 preferred). To deploy the container on SageMaker platform, please specify `al2-ami-sagemaker-inference-gpu-3-1` as the [ProductionVariant](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ProductionVariant.html).

## Security Advisory

AWS recommends that customers monitor critical security updates in the [AWS Security Bulletin](https://aws.amazon.com/security/security-bulletins/).

## Python Support

Python 3.12 is supported.

## Instance Type Support

The containers support x86_64 instance types.

## Build and Test

- Built on: c5.18xlarge
- Tested on: p4d.24xlarge, p5.48xlarge
- Tested with deepseek-ai/DeepSeek-R1-Distill-Qwen-32B model, single-node and multi-node serving configurations

## Known Issues

No known issues so far.
